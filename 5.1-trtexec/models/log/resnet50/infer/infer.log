&&&& RUNNING TensorRT.trtexec [TensorRT v8501] # trtexec --loadEngine=resnet50.engine --verbose --dumpOutput --dumpProfile --dumpLayerInfo --exportOutput=log/resnet50/infer/infer_output.log --exportProfile=log/resnet50/infer/infer_profile.log --exportLayerInfo=log/resnet50/infer/infer_layer_info.log --warmUp=200 --iterations=50
[05/26/2023-16:10:43] [I] === Model Options ===
[05/26/2023-16:10:43] [I] Format: *
[05/26/2023-16:10:43] [I] Model: 
[05/26/2023-16:10:43] [I] Output:
[05/26/2023-16:10:43] [I] === Build Options ===
[05/26/2023-16:10:43] [I] Max batch: 1
[05/26/2023-16:10:43] [I] Memory Pools: workspace: default, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default
[05/26/2023-16:10:43] [I] minTiming: 1
[05/26/2023-16:10:43] [I] avgTiming: 8
[05/26/2023-16:10:43] [I] Precision: FP32
[05/26/2023-16:10:43] [I] LayerPrecisions: 
[05/26/2023-16:10:43] [I] Calibration: 
[05/26/2023-16:10:43] [I] Refit: Disabled
[05/26/2023-16:10:43] [I] Sparsity: Disabled
[05/26/2023-16:10:43] [I] Safe mode: Disabled
[05/26/2023-16:10:43] [I] DirectIO mode: Disabled
[05/26/2023-16:10:43] [I] Restricted mode: Disabled
[05/26/2023-16:10:43] [I] Build only: Disabled
[05/26/2023-16:10:43] [I] Save engine: 
[05/26/2023-16:10:43] [I] Load engine: resnet50.engine
[05/26/2023-16:10:43] [I] Profiling verbosity: 0
[05/26/2023-16:10:43] [I] Tactic sources: Using default tactic sources
[05/26/2023-16:10:43] [I] timingCacheMode: local
[05/26/2023-16:10:43] [I] timingCacheFile: 
[05/26/2023-16:10:43] [I] Heuristic: Disabled
[05/26/2023-16:10:43] [I] Preview Features: Use default preview flags.
[05/26/2023-16:10:43] [I] Input(s)s format: fp32:CHW
[05/26/2023-16:10:43] [I] Output(s)s format: fp32:CHW
[05/26/2023-16:10:43] [I] Input build shapes: model
[05/26/2023-16:10:43] [I] Input calibration shapes: model
[05/26/2023-16:10:43] [I] === System Options ===
[05/26/2023-16:10:43] [I] Device: 0
[05/26/2023-16:10:43] [I] DLACore: 
[05/26/2023-16:10:43] [I] Plugins:
[05/26/2023-16:10:43] [I] === Inference Options ===
[05/26/2023-16:10:43] [I] Batch: 1
[05/26/2023-16:10:43] [I] Input inference shapes: model
[05/26/2023-16:10:43] [I] Iterations: 50
[05/26/2023-16:10:43] [I] Duration: 3s (+ 200ms warm up)
[05/26/2023-16:10:43] [I] Sleep time: 0ms
[05/26/2023-16:10:43] [I] Idle time: 0ms
[05/26/2023-16:10:43] [I] Streams: 1
[05/26/2023-16:10:43] [I] ExposeDMA: Disabled
[05/26/2023-16:10:43] [I] Data transfers: Enabled
[05/26/2023-16:10:43] [I] Spin-wait: Disabled
[05/26/2023-16:10:43] [I] Multithreading: Disabled
[05/26/2023-16:10:43] [I] CUDA Graph: Disabled
[05/26/2023-16:10:43] [I] Separate profiling: Disabled
[05/26/2023-16:10:43] [I] Time Deserialize: Disabled
[05/26/2023-16:10:43] [I] Time Refit: Disabled
[05/26/2023-16:10:43] [I] NVTX verbosity: 0
[05/26/2023-16:10:43] [I] Persistent Cache Ratio: 0
[05/26/2023-16:10:43] [I] Inputs:
[05/26/2023-16:10:43] [I] === Reporting Options ===
[05/26/2023-16:10:43] [I] Verbose: Enabled
[05/26/2023-16:10:43] [I] Averages: 10 inferences
[05/26/2023-16:10:43] [I] Percentiles: 90,95,99
[05/26/2023-16:10:43] [I] Dump refittable layers:Disabled
[05/26/2023-16:10:43] [I] Dump output: Enabled
[05/26/2023-16:10:43] [I] Profile: Enabled
[05/26/2023-16:10:43] [I] Export timing to JSON file: 
[05/26/2023-16:10:43] [I] Export output to JSON file: log/resnet50/infer/infer_output.log
[05/26/2023-16:10:43] [I] Export profile to JSON file: log/resnet50/infer/infer_profile.log
[05/26/2023-16:10:43] [I] 
[05/26/2023-16:10:43] [I] === Device Information ===
[05/26/2023-16:10:43] [I] Selected Device: NVIDIA GeForce RTX 3080
[05/26/2023-16:10:43] [I] Compute Capability: 8.6
[05/26/2023-16:10:43] [I] SMs: 68
[05/26/2023-16:10:43] [I] Compute Clock Rate: 1.74 GHz
[05/26/2023-16:10:43] [I] Device Global Memory: 10009 MiB
[05/26/2023-16:10:43] [I] Shared Memory per SM: 100 KiB
[05/26/2023-16:10:43] [I] Memory Bus Width: 320 bits (ECC disabled)
[05/26/2023-16:10:43] [I] Memory Clock Rate: 9.501 GHz
[05/26/2023-16:10:43] [I] 
[05/26/2023-16:10:43] [I] TensorRT version: 8.5.1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::BatchedNMSDynamic_TRT version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::BatchedNMS_TRT version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::BatchTilePlugin_TRT version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::Clip_TRT version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::CoordConvAC version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::CropAndResizeDynamic version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::CropAndResize version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::DecodeBbox3DPlugin version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::DetectionLayer_TRT version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::EfficientNMS_Explicit_TF_TRT version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::EfficientNMS_Implicit_TF_TRT version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::EfficientNMS_ONNX_TRT version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::EfficientNMS_TRT version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::FlattenConcat_TRT version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::GenerateDetection_TRT version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::GridAnchor_TRT version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::GridAnchorRect_TRT version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::InstanceNormalization_TRT version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::InstanceNormalization_TRT version 2
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::LReLU_TRT version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::MultilevelCropAndResize_TRT version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::MultilevelProposeROI_TRT version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::MultiscaleDeformableAttnPlugin_TRT version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::NMSDynamic_TRT version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::NMS_TRT version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::Normalize_TRT version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::PillarScatterPlugin version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::PriorBox_TRT version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::ProposalDynamic version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::ProposalLayer_TRT version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::Proposal version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::PyramidROIAlign_TRT version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::Region_TRT version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::Reorg_TRT version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::ResizeNearest_TRT version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::ROIAlign_TRT version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::RPROI_TRT version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::ScatterND version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::SpecialSlice_TRT version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::Split version 1
[05/26/2023-16:10:43] [V] [TRT] Registered plugin creator - ::VoxelGeneratorPlugin version 1
[05/26/2023-16:10:43] [I] Engine loaded in 0.048208 sec.
[05/26/2023-16:10:43] [I] [TRT] Loaded engine size: 98 MiB
[05/26/2023-16:10:43] [V] [TRT] Trying to load shared library libcudnn.so.8
[05/26/2023-16:10:43] [V] [TRT] Loaded shared library libcudnn.so.8
[05/26/2023-16:10:43] [V] [TRT] Using cuDNN as plugin tactic source
[05/26/2023-16:10:43] [V] [TRT] Using cuDNN as core library tactic source
[05/26/2023-16:10:43] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +975, GPU +418, now: CPU 1469, GPU 969 (MiB)
[05/26/2023-16:10:43] [V] [TRT] Deserialization required 454925 microseconds.
[05/26/2023-16:10:43] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +97, now: CPU 0, GPU 97 (MiB)
[05/26/2023-16:10:43] [I] Engine deserialized in 0.707977 sec.
[05/26/2023-16:10:43] [V] [TRT] Trying to load shared library libcudnn.so.8
[05/26/2023-16:10:43] [V] [TRT] Loaded shared library libcudnn.so.8
[05/26/2023-16:10:43] [V] [TRT] Using cuDNN as plugin tactic source
[05/26/2023-16:10:43] [V] [TRT] Using cuDNN as core library tactic source
[05/26/2023-16:10:43] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 1469, GPU 969 (MiB)
[05/26/2023-16:10:43] [V] [TRT] Total per-runner device persistent memory is 75776
[05/26/2023-16:10:43] [V] [TRT] Total per-runner host persistent memory is 134448
[05/26/2023-16:10:43] [V] [TRT] Allocated activation device memory of size 139436032
[05/26/2023-16:10:43] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +133, now: CPU 0, GPU 230 (MiB)
[05/26/2023-16:10:43] [I] Setting persistentCacheLimit to 0 bytes.
[05/26/2023-16:10:43] [V] Using enqueueV3.
[05/26/2023-16:10:43] [I] Using random values for input input0
[05/26/2023-16:10:43] [I] Created input binding for input0 with dimensions 1x3x224x224
[05/26/2023-16:10:43] [I] Using random values for output output0
[05/26/2023-16:10:43] [I] Created output binding for output0 with dimensions 1x1000
[05/26/2023-16:10:43] [I] Layer Information:
[05/26/2023-16:10:43] [I] Layers:
Name: /conv1/Conv + /relu/Relu, LayerType: CaskConvolution, Inputs: [ { Name: input0, Location: Device, Dimensions: [1,3,224,224], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /relu/Relu_output_0, Location: Device, Dimensions: [1,64,112,112], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [7,7], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [3,3], PostPadding: [3,3], Stride: [2,2], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Float", "Count": 9408}, Bias: {"Type": "Float", "Count": 64}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: ampere_scudnn_128x64_relu_xregs_large_nn_v1, TacticValue: 0x5deb29b7a8e275f7
Name: /maxpool/MaxPool, LayerType: CaskPooling, Inputs: [ { Name: /relu/Relu_output_0, Location: Device, Dimensions: [1,64,112,112], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /maxpool/MaxPool_output_0, Location: Device, Dimensions: [1,64,56,56], Format/Datatype: Row major linear FP32 }], ParameterType: Pooling, PoolingType: MAX, WindowSize: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], BlendFactor: 0, AverageCountExcludesPadding: 1, TacticName: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll4_tThreads255, TacticValue: 0xfcb5fcaa68fff7ac
Name: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /maxpool/MaxPool_output_0, Location: Device, Dimensions: [1,64,56,56], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer1/layer1.0/relu/Relu_output_0, Location: Device, Dimensions: [1,64,56,56], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Float", "Count": 4096}, Bias: {"Type": "Float", "Count": 64}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4, TacticValue: 0xa31d27de74b895ff
Name: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer1/layer1.0/relu/Relu_output_0, Location: Device, Dimensions: [1,64,56,56], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer1/layer1.0/relu_1/Relu_output_0, Location: Device, Dimensions: [1,64,56,56], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Float", "Count": 36864}, Bias: {"Type": "Float", "Count": 64}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1, TacticValue: 0x94119b4c514b211a
Name: /layer1/layer1.0/downsample/downsample.0/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /maxpool/MaxPool_output_0, Location: Device, Dimensions: [1,64,56,56], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer1/layer1.0/downsample/downsample.0/Conv_output_0, Location: Device, Dimensions: [1,256,56,56], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Float", "Count": 16384}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4, TacticValue: 0x1fc87d7eb370bb7a
Name: /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer1/layer1.0/relu_1/Relu_output_0, Location: Device, Dimensions: [1,64,56,56], Format/Datatype: Row major linear FP32 }, { Name: /layer1/layer1.0/downsample/downsample.0/Conv_output_0, Location: Device, Dimensions: [1,256,56,56], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer1/layer1.0/relu_2/Relu_output_0, Location: Device, Dimensions: [1,256,56,56], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Float", "Count": 16384}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4, TacticValue: 0x865894c4635db7fd
Name: /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer1/layer1.0/relu_2/Relu_output_0, Location: Device, Dimensions: [1,256,56,56], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer1/layer1.1/relu/Relu_output_0, Location: Device, Dimensions: [1,64,56,56], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Float", "Count": 16384}, Bias: {"Type": "Float", "Count": 64}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4, TacticValue: 0xa31d27de74b895ff
Name: /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/relu_1/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer1/layer1.1/relu/Relu_output_0, Location: Device, Dimensions: [1,64,56,56], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer1/layer1.1/relu_1/Relu_output_0, Location: Device, Dimensions: [1,64,56,56], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Float", "Count": 36864}, Bias: {"Type": "Float", "Count": 64}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1, TacticValue: 0x94119b4c514b211a
Name: /layer1/layer1.1/conv3/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/relu_2/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer1/layer1.1/relu_1/Relu_output_0, Location: Device, Dimensions: [1,64,56,56], Format/Datatype: Row major linear FP32 }, { Name: /layer1/layer1.0/relu_2/Relu_output_0, Location: Device, Dimensions: [1,256,56,56], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer1/layer1.1/relu_2/Relu_output_0, Location: Device, Dimensions: [1,256,56,56], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Float", "Count": 16384}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4, TacticValue: 0x865894c4635db7fd
Name: /layer1/layer1.2/conv1/Conv + /layer1/layer1.2/relu/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer1/layer1.1/relu_2/Relu_output_0, Location: Device, Dimensions: [1,256,56,56], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer1/layer1.2/relu/Relu_output_0, Location: Device, Dimensions: [1,64,56,56], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Float", "Count": 16384}, Bias: {"Type": "Float", "Count": 64}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4, TacticValue: 0xa31d27de74b895ff
Name: /layer1/layer1.2/conv2/Conv + /layer1/layer1.2/relu_1/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer1/layer1.2/relu/Relu_output_0, Location: Device, Dimensions: [1,64,56,56], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer1/layer1.2/relu_1/Relu_output_0, Location: Device, Dimensions: [1,64,56,56], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Float", "Count": 36864}, Bias: {"Type": "Float", "Count": 64}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1, TacticValue: 0x94119b4c514b211a
Name: /layer1/layer1.2/conv3/Conv + /layer1/layer1.2/Add + /layer1/layer1.2/relu_2/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer1/layer1.2/relu_1/Relu_output_0, Location: Device, Dimensions: [1,64,56,56], Format/Datatype: Row major linear FP32 }, { Name: /layer1/layer1.1/relu_2/Relu_output_0, Location: Device, Dimensions: [1,256,56,56], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer1/layer1.2/relu_2/Relu_output_0, Location: Device, Dimensions: [1,256,56,56], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Float", "Count": 16384}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4, TacticValue: 0x865894c4635db7fd
Name: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu, LayerType: CudnnConvolution, Inputs: [ { Name: /layer1/layer1.2/relu_2/Relu_output_0, Location: Device, Dimensions: [1,256,56,56], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer2/layer2.0/relu/Relu_output_0, Location: Device, Dimensions: [1,128,56,56], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Float", "Count": 32768}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, TacticValue: 0x0000000000000001
Name: /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu, LayerType: CudnnConvolution, Inputs: [ { Name: /layer2/layer2.0/relu/Relu_output_0, Location: Device, Dimensions: [1,128,56,56], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer2/layer2.0/relu_1/Relu_output_0, Location: Device, Dimensions: [1,128,28,28], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Float", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, TacticValue: 0x0000000000000001
Name: /layer2/layer2.0/downsample/downsample.0/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /layer1/layer1.2/relu_2/Relu_output_0, Location: Device, Dimensions: [1,256,56,56], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer2/layer2.0/downsample/downsample.0/Conv_output_0, Location: Device, Dimensions: [1,512,28,28], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [2,2], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Float", "Count": 131072}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4, TacticValue: 0xc0b05b61d128e46e
Name: /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer2/layer2.0/relu_1/Relu_output_0, Location: Device, Dimensions: [1,128,28,28], Format/Datatype: Row major linear FP32 }, { Name: /layer2/layer2.0/downsample/downsample.0/Conv_output_0, Location: Device, Dimensions: [1,512,28,28], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer2/layer2.0/relu_2/Relu_output_0, Location: Device, Dimensions: [1,512,28,28], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Float", "Count": 65536}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4, TacticValue: 0x1fc87d7eb370bb7a
Name: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu, LayerType: FusedConvActConvolution, Inputs: [ { Name: /layer2/layer2.0/relu_2/Relu_output_0, Location: Device, Dimensions: [1,512,28,28], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer2/layer2.1/relu/Relu_output_0, Location: Device, Dimensions: [1,128,28,28], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Float", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, ConvolutionTacticIndex: 89, TacticValue: 0x000000000059ffff
Name: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu, LayerType: FusedConvActConvolution, Inputs: [ { Name: /layer2/layer2.1/relu/Relu_output_0, Location: Device, Dimensions: [1,128,28,28], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer2/layer2.1/relu_1/Relu_output_0, Location: Device, Dimensions: [1,128,28,28], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Float", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, ConvolutionTacticIndex: 134, TacticValue: 0x000000000086ffff
Name: /layer2/layer2.1/conv3/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/relu_2/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer2/layer2.1/relu_1/Relu_output_0, Location: Device, Dimensions: [1,128,28,28], Format/Datatype: Row major linear FP32 }, { Name: /layer2/layer2.0/relu_2/Relu_output_0, Location: Device, Dimensions: [1,512,28,28], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer2/layer2.1/relu_2/Relu_output_0, Location: Device, Dimensions: [1,512,28,28], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Float", "Count": 65536}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4, TacticValue: 0x1fc87d7eb370bb7a
Name: /layer2/layer2.2/conv1/Conv + /layer2/layer2.2/relu/Relu, LayerType: FusedConvActConvolution, Inputs: [ { Name: /layer2/layer2.1/relu_2/Relu_output_0, Location: Device, Dimensions: [1,512,28,28], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer2/layer2.2/relu/Relu_output_0, Location: Device, Dimensions: [1,128,28,28], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Float", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, ConvolutionTacticIndex: 89, TacticValue: 0x000000000059ffff
Name: /layer2/layer2.2/conv2/Conv + /layer2/layer2.2/relu_1/Relu, LayerType: FusedConvActConvolution, Inputs: [ { Name: /layer2/layer2.2/relu/Relu_output_0, Location: Device, Dimensions: [1,128,28,28], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer2/layer2.2/relu_1/Relu_output_0, Location: Device, Dimensions: [1,128,28,28], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Float", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, ConvolutionTacticIndex: 134, TacticValue: 0x000000000086ffff
Name: /layer2/layer2.2/conv3/Conv + /layer2/layer2.2/Add + /layer2/layer2.2/relu_2/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer2/layer2.2/relu_1/Relu_output_0, Location: Device, Dimensions: [1,128,28,28], Format/Datatype: Row major linear FP32 }, { Name: /layer2/layer2.1/relu_2/Relu_output_0, Location: Device, Dimensions: [1,512,28,28], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer2/layer2.2/relu_2/Relu_output_0, Location: Device, Dimensions: [1,512,28,28], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Float", "Count": 65536}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4, TacticValue: 0x1fc87d7eb370bb7a
Name: /layer2/layer2.3/conv1/Conv + /layer2/layer2.3/relu/Relu, LayerType: FusedConvActConvolution, Inputs: [ { Name: /layer2/layer2.2/relu_2/Relu_output_0, Location: Device, Dimensions: [1,512,28,28], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer2/layer2.3/relu/Relu_output_0, Location: Device, Dimensions: [1,128,28,28], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Float", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, ConvolutionTacticIndex: 89, TacticValue: 0x000000000059ffff
Name: /layer2/layer2.3/conv2/Conv + /layer2/layer2.3/relu_1/Relu, LayerType: FusedConvActConvolution, Inputs: [ { Name: /layer2/layer2.3/relu/Relu_output_0, Location: Device, Dimensions: [1,128,28,28], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer2/layer2.3/relu_1/Relu_output_0, Location: Device, Dimensions: [1,128,28,28], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Float", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, ConvolutionTacticIndex: 134, TacticValue: 0x000000000086ffff
Name: /layer2/layer2.3/conv3/Conv + /layer2/layer2.3/Add + /layer2/layer2.3/relu_2/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer2/layer2.3/relu_1/Relu_output_0, Location: Device, Dimensions: [1,128,28,28], Format/Datatype: Row major linear FP32 }, { Name: /layer2/layer2.2/relu_2/Relu_output_0, Location: Device, Dimensions: [1,512,28,28], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer2/layer2.3/relu_2/Relu_output_0, Location: Device, Dimensions: [1,512,28,28], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Float", "Count": 65536}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4, TacticValue: 0x1fc87d7eb370bb7a
Name: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu, LayerType: CudnnConvolution, Inputs: [ { Name: /layer2/layer2.3/relu_2/Relu_output_0, Location: Device, Dimensions: [1,512,28,28], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer3/layer3.0/relu/Relu_output_0, Location: Device, Dimensions: [1,256,28,28], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Float", "Count": 131072}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, TacticValue: 0x0000000000000001
Name: Reformatting CopyNode for Input Tensor 0 to /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu, LayerType: Reformat, Inputs: [ { Name: /layer3/layer3.0/relu/Relu_output_0, Location: Device, Dimensions: [1,256,28,28], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: Reformatted Input Tensor 0 to /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu, Location: Device, Dimensions: [1,256,28,28], Format/Datatype: Channel major FP32 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x00000000000003ea
Name: /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu, LayerType: CaskConvolution, Inputs: [ { Name: Reformatted Input Tensor 0 to /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu, Location: Device, Dimensions: [1,256,28,28], Format/Datatype: Channel major FP32 format }], Outputs: [ { Name: /layer3/layer3.0/relu_1/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Channel major FP32 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Float", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4, TacticValue: 0x5953bec563d26434
Name: /layer3/layer3.0/downsample/downsample.0/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /layer2/layer2.3/relu_2/Relu_output_0, Location: Device, Dimensions: [1,512,28,28], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer3/layer3.0/downsample/downsample.0/Conv_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [2,2], Dilation: [1,1], OutMaps: 1024, Groups: 1, Weights: {"Type": "Float", "Count": 524288}, Bias: {"Type": "Float", "Count": 1024}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4, TacticValue: 0xa31d27de74b895ff
Name: Reformatting CopyNode for Input Tensor 0 to /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu, LayerType: Reformat, Inputs: [ { Name: /layer3/layer3.0/relu_1/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Channel major FP32 format }], Outputs: [ { Name: Reformatted Input Tensor 0 to /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu, LayerType: CaskConvolution, Inputs: [ { Name: Reformatted Input Tensor 0 to /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }, { Name: /layer3/layer3.0/downsample/downsample.0/Conv_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer3/layer3.0/relu_2/Relu_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 1024, Groups: 1, Weights: {"Type": "Float", "Count": 262144}, Bias: {"Type": "Float", "Count": 1024}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4, TacticValue: 0xa31d27de74b895ff
Name: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu, LayerType: FusedConvActConvolution, Inputs: [ { Name: /layer3/layer3.0/relu_2/Relu_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer3/layer3.1/relu/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Float", "Count": 262144}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, ConvolutionTacticIndex: 31, TacticValue: 0x00000000001fffff
Name: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu, LayerType: FusedConvActConvolution, Inputs: [ { Name: /layer3/layer3.1/relu/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer3/layer3.1/relu_1/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Float", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, ConvolutionTacticIndex: 162, TacticValue: 0x0000000000a2ffff
Name: /layer3/layer3.1/conv3/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/relu_2/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer3/layer3.1/relu_1/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }, { Name: /layer3/layer3.0/relu_2/Relu_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer3/layer3.1/relu_2/Relu_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 1024, Groups: 1, Weights: {"Type": "Float", "Count": 262144}, Bias: {"Type": "Float", "Count": 1024}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4, TacticValue: 0xa31d27de74b895ff
Name: /layer3/layer3.2/conv1/Conv + /layer3/layer3.2/relu/Relu, LayerType: FusedConvActConvolution, Inputs: [ { Name: /layer3/layer3.1/relu_2/Relu_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer3/layer3.2/relu/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Float", "Count": 262144}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, ConvolutionTacticIndex: 31, TacticValue: 0x00000000001fffff
Name: /layer3/layer3.2/conv2/Conv + /layer3/layer3.2/relu_1/Relu, LayerType: FusedConvActConvolution, Inputs: [ { Name: /layer3/layer3.2/relu/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer3/layer3.2/relu_1/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Float", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, ConvolutionTacticIndex: 162, TacticValue: 0x0000000000a2ffff
Name: /layer3/layer3.2/conv3/Conv + /layer3/layer3.2/Add + /layer3/layer3.2/relu_2/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer3/layer3.2/relu_1/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }, { Name: /layer3/layer3.1/relu_2/Relu_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer3/layer3.2/relu_2/Relu_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 1024, Groups: 1, Weights: {"Type": "Float", "Count": 262144}, Bias: {"Type": "Float", "Count": 1024}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4, TacticValue: 0xa31d27de74b895ff
Name: /layer3/layer3.3/conv1/Conv + /layer3/layer3.3/relu/Relu, LayerType: FusedConvActConvolution, Inputs: [ { Name: /layer3/layer3.2/relu_2/Relu_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer3/layer3.3/relu/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Float", "Count": 262144}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, ConvolutionTacticIndex: 31, TacticValue: 0x00000000001fffff
Name: /layer3/layer3.3/conv2/Conv + /layer3/layer3.3/relu_1/Relu, LayerType: FusedConvActConvolution, Inputs: [ { Name: /layer3/layer3.3/relu/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer3/layer3.3/relu_1/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Float", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, ConvolutionTacticIndex: 162, TacticValue: 0x0000000000a2ffff
Name: /layer3/layer3.3/conv3/Conv + /layer3/layer3.3/Add + /layer3/layer3.3/relu_2/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer3/layer3.3/relu_1/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }, { Name: /layer3/layer3.2/relu_2/Relu_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer3/layer3.3/relu_2/Relu_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 1024, Groups: 1, Weights: {"Type": "Float", "Count": 262144}, Bias: {"Type": "Float", "Count": 1024}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4, TacticValue: 0xa31d27de74b895ff
Name: /layer3/layer3.4/conv1/Conv + /layer3/layer3.4/relu/Relu, LayerType: FusedConvActConvolution, Inputs: [ { Name: /layer3/layer3.3/relu_2/Relu_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer3/layer3.4/relu/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Float", "Count": 262144}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, ConvolutionTacticIndex: 31, TacticValue: 0x00000000001fffff
Name: /layer3/layer3.4/conv2/Conv + /layer3/layer3.4/relu_1/Relu, LayerType: FusedConvActConvolution, Inputs: [ { Name: /layer3/layer3.4/relu/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer3/layer3.4/relu_1/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Float", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, ConvolutionTacticIndex: 162, TacticValue: 0x0000000000a2ffff
Name: /layer3/layer3.4/conv3/Conv + /layer3/layer3.4/Add + /layer3/layer3.4/relu_2/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer3/layer3.4/relu_1/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }, { Name: /layer3/layer3.3/relu_2/Relu_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer3/layer3.4/relu_2/Relu_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 1024, Groups: 1, Weights: {"Type": "Float", "Count": 262144}, Bias: {"Type": "Float", "Count": 1024}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4, TacticValue: 0xa31d27de74b895ff
Name: /layer3/layer3.5/conv1/Conv + /layer3/layer3.5/relu/Relu, LayerType: FusedConvActConvolution, Inputs: [ { Name: /layer3/layer3.4/relu_2/Relu_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer3/layer3.5/relu/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Float", "Count": 262144}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, ConvolutionTacticIndex: 31, TacticValue: 0x00000000001fffff
Name: /layer3/layer3.5/conv2/Conv + /layer3/layer3.5/relu_1/Relu, LayerType: FusedConvActConvolution, Inputs: [ { Name: /layer3/layer3.5/relu/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer3/layer3.5/relu_1/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Float", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, ConvolutionTacticIndex: 162, TacticValue: 0x0000000000a2ffff
Name: /layer3/layer3.5/conv3/Conv + /layer3/layer3.5/Add + /layer3/layer3.5/relu_2/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer3/layer3.5/relu_1/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }, { Name: /layer3/layer3.4/relu_2/Relu_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer3/layer3.5/relu_2/Relu_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 1024, Groups: 1, Weights: {"Type": "Float", "Count": 262144}, Bias: {"Type": "Float", "Count": 1024}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4, TacticValue: 0xa31d27de74b895ff
Name: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu, LayerType: CudnnConvolution, Inputs: [ { Name: /layer3/layer3.5/relu_2/Relu_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer4/layer4.0/relu/Relu_output_0, Location: Device, Dimensions: [1,512,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Float", "Count": 524288}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, TacticValue: 0x0000000000000001
Name: Reformatting CopyNode for Input Tensor 0 to /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu, LayerType: Reformat, Inputs: [ { Name: /layer4/layer4.0/relu/Relu_output_0, Location: Device, Dimensions: [1,512,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: Reformatted Input Tensor 0 to /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu, Location: Device, Dimensions: [1,512,14,14], Format/Datatype: Channel major FP32 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu, LayerType: CaskConvolution, Inputs: [ { Name: Reformatted Input Tensor 0 to /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu, Location: Device, Dimensions: [1,512,14,14], Format/Datatype: Channel major FP32 format }], Outputs: [ { Name: /layer4/layer4.0/relu_1/Relu_output_0, Location: Device, Dimensions: [1,512,7,7], Format/Datatype: Channel major FP32 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Float", "Count": 2359296}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4, TacticValue: 0x5953bec563d26434
Name: Reformatting CopyNode for Input Tensor 0 to /layer4/layer4.0/downsample/downsample.0/Conv, LayerType: Reformat, Inputs: [ { Name: /layer3/layer3.5/relu_2/Relu_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: Reformatted Input Tensor 0 to /layer4/layer4.0/downsample/downsample.0/Conv, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Channel major FP32 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: /layer4/layer4.0/downsample/downsample.0/Conv, LayerType: CaskConvolution, Inputs: [ { Name: Reformatted Input Tensor 0 to /layer4/layer4.0/downsample/downsample.0/Conv, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Channel major FP32 format }], Outputs: [ { Name: /layer4/layer4.0/downsample/downsample.0/Conv_output_0, Location: Device, Dimensions: [1,2048,7,7], Format/Datatype: Channel major FP32 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [2,2], Dilation: [1,1], OutMaps: 2048, Groups: 1, Weights: {"Type": "Float", "Count": 2097152}, Bias: {"Type": "Float", "Count": 2048}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4, TacticValue: 0x7121ec1db3f80c67
Name: /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer4/layer4.0/relu_1/Relu_output_0, Location: Device, Dimensions: [1,512,7,7], Format/Datatype: Channel major FP32 format }, { Name: /layer4/layer4.0/downsample/downsample.0/Conv_output_0, Location: Device, Dimensions: [1,2048,7,7], Format/Datatype: Channel major FP32 format }], Outputs: [ { Name: Reformatted Output Tensor 0 to /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu, Location: Device, Dimensions: [1,2048,7,7], Format/Datatype: Channel major FP32 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 2048, Groups: 1, Weights: {"Type": "Float", "Count": 1048576}, Bias: {"Type": "Float", "Count": 2048}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4, TacticValue: 0x1fb90698107bb33a
Name: Reformatting CopyNode for Output Tensor 0 to /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu, Location: Device, Dimensions: [1,2048,7,7], Format/Datatype: Channel major FP32 format }], Outputs: [ { Name: /layer4/layer4.0/relu_2/Relu_output_0, Location: Device, Dimensions: [1,2048,7,7], Format/Datatype: Row major linear FP32 }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x00000000000003e8
Name: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu, LayerType: CudnnConvolution, Inputs: [ { Name: /layer4/layer4.0/relu_2/Relu_output_0, Location: Device, Dimensions: [1,2048,7,7], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer4/layer4.1/relu/Relu_output_0, Location: Device, Dimensions: [1,512,7,7], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Float", "Count": 1048576}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, TacticValue: 0x0000000000000001
Name: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu, LayerType: FusedConvActConvolution, Inputs: [ { Name: /layer4/layer4.1/relu/Relu_output_0, Location: Device, Dimensions: [1,512,7,7], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer4/layer4.1/relu_1/Relu_output_0, Location: Device, Dimensions: [1,512,7,7], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Float", "Count": 2359296}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, ConvolutionTacticIndex: 36, TacticValue: 0x000000000024ffff
Name: /layer4/layer4.1/conv3/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/relu_2/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer4/layer4.1/relu_1/Relu_output_0, Location: Device, Dimensions: [1,512,7,7], Format/Datatype: Row major linear FP32 }, { Name: /layer4/layer4.0/relu_2/Relu_output_0, Location: Device, Dimensions: [1,2048,7,7], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer4/layer4.1/relu_2/Relu_output_0, Location: Device, Dimensions: [1,2048,7,7], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 2048, Groups: 1, Weights: {"Type": "Float", "Count": 1048576}, Bias: {"Type": "Float", "Count": 2048}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4, TacticValue: 0xb0bf940d5e0f9f45
Name: /layer4/layer4.2/conv1/Conv + /layer4/layer4.2/relu/Relu, LayerType: CudnnConvolution, Inputs: [ { Name: /layer4/layer4.1/relu_2/Relu_output_0, Location: Device, Dimensions: [1,2048,7,7], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer4/layer4.2/relu/Relu_output_0, Location: Device, Dimensions: [1,512,7,7], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Float", "Count": 1048576}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, TacticValue: 0x0000000000000001
Name: /layer4/layer4.2/conv2/Conv + /layer4/layer4.2/relu_1/Relu, LayerType: FusedConvActConvolution, Inputs: [ { Name: /layer4/layer4.2/relu/Relu_output_0, Location: Device, Dimensions: [1,512,7,7], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer4/layer4.2/relu_1/Relu_output_0, Location: Device, Dimensions: [1,512,7,7], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Float", "Count": 2359296}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, ConvolutionTacticIndex: 36, TacticValue: 0x000000000024ffff
Name: /layer4/layer4.2/conv3/Conv + /layer4/layer4.2/Add + /layer4/layer4.2/relu_2/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer4/layer4.2/relu_1/Relu_output_0, Location: Device, Dimensions: [1,512,7,7], Format/Datatype: Row major linear FP32 }, { Name: /layer4/layer4.1/relu_2/Relu_output_0, Location: Device, Dimensions: [1,2048,7,7], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer4/layer4.2/relu_2/Relu_output_0, Location: Device, Dimensions: [1,2048,7,7], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 2048, Groups: 1, Weights: {"Type": "Float", "Count": 1048576}, Bias: {"Type": "Float", "Count": 2048}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4, TacticValue: 0xb0bf940d5e0f9f45
Name: /avgpool/GlobalAveragePool, LayerType: CudnnPooling, Inputs: [ { Name: /layer4/layer4.2/relu_2/Relu_output_0, Location: Device, Dimensions: [1,2048,7,7], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /avgpool/GlobalAveragePool_output_0, Location: Device, Dimensions: [1,2048,1,1], Format/Datatype: Row major linear FP32 }], ParameterType: Pooling, PoolingType: AVERAGE, WindowSize: [7,7], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], BlendFactor: 0, AverageCountExcludesPadding: 1, TacticValue: 0xffffffffffffffff
Name: /fc/Gemm, LayerType: CaskGemmConvolution, Inputs: [ { Name: /avgpool/GlobalAveragePool_output_0, Location: Device, Dimensions: [1,2048,1,1], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /fc/Gemm_out_tensor, Location: Device, Dimensions: [1,1000,1,1], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 1000, Groups: 1, Weights: {"Type": "Float", "Count": 2048000}, Bias: {"Type": "Float", "Count": 1000}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_unit_stride, TacticValue: 0x0000000000020460
Name: reshape_after_/fc/Gemm, LayerType: NoOp, Inputs: [ { Name: /fc/Gemm_out_tensor, Location: Device, Dimensions: [1,1000,1,1], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: output0, Location: Device, Dimensions: [1,1000], Format/Datatype: Row major linear FP32 }], TacticValue: 0x0000000000000000

Bindings:
input0
output0
[05/26/2023-16:10:43] [I] Starting inference
[05/26/2023-16:10:47] [I] The e2e network timing is not reported since it is inaccurate due to the extra synchronizations when the profiler is enabled.
[05/26/2023-16:10:47] [I] To show e2e network timing report, add --separateProfileRun to profile layer timing in a separate run or remove --dumpProfile to disable the profiler.
[05/26/2023-16:10:47] [I] Output Tensors:
[05/26/2023-16:10:47] [I] output0: (1x1000)
[05/26/2023-16:10:47] [I] -3.61514 20.5864 11.0214 10.9955 0.469871 10.2569 -20.8846 11.1736 1.4042 6.45995 14.3445 0.91221 -17.3223 11.1886 -9.33976 -0.266301 -2.66785 -1.2273 7.27567 2.4215 -8.39138 22.7788 0.532748 4.39701 1.21249 3.16729 11.9165 -15.8489 2.50388 -7.07671 5.01986 -4.69475 -15.8461 14.8773 2.79506 -1.85218 -16.1026 3.37021 -7.81903 -14.4179 -0.0636368 -13.7821 -12.3901 10.4264 2.53839 -14.1177 10.0553 -11.608 -8.34573 -7.89687 3.07244 -7.78747 -1.68135 -7.66083 -18.5036 3.79383 10.5975 3.26009 -2.25623 5.14645 -7.15223 -4.93529 -6.81551 7.07172 -3.66583 5.99756 16.2912 -2.21476 1.93074 -18.4484 -5.57929 -7.60227 -6.47907 18.0926 -3.20715 -13.0869 -1.00028 -0.999909 -24.3498 -2.12765 -5.6794 5.29786 30.7989 9.22271 -5.75207 -22.9014 -4.66807 13.4157 -18.3165 5.18209 17.0783 6.45506 -2.0124 -17.5931 0.194456 -6.63053 12.3886 -1.34305 -19.5295 4.87452 1.50056 -12.0702 -1.10992 7.56352 -0.799791 3.02006 4.14015 4.95957 1.88931 -1.66887 6.66415 -17.869 -0.598216 7.6126 6.02388 31.2631 13.2972 6.0194 6.25454 -8.42365 -6.58558 -2.6349 -6.47468 0.361755 -7.39555 25.0752 14.5524 -0.356799 9.83181 -18.4363 -6.19991 6.90779 0.186953 -11.6013 5.69616 13.2493 24.8979 -3.22029 -19.9073 -1.21067 -3.46276 -14.2335 0.97074 -18.0945 1.07656 -3.29546 6.71238 -6.35752 3.81764 -1.35331 -7.73418 -5.51072 -1.52701 -13.3458 14.3228 -27.6773 12.2701 10.2538 -5.40543 13.8329 -1.77111 12.7961 14.9134 21.2064 -11.3925 -18.7573 -1.5922 17.3023 -23.0049 2.30465 25.3807 6.19375 10.0725 4.82165 -0.601671 6.88886 -12.3479 3.28114 -14.3472 -25.1817 -1.12925 17.3039 1.20441 -13.8823 -8.49497 -7.29709 9.20095 -0.651281 -9.64182 6.73666 -8.93101 0.0563836 4.52992 21.2885 14.0785 -11.3652 -2.36762 7.77468 -4.82847 10.1022 2.95122 5.37818 -1.30352 -0.467226 6.14346 -9.32022 5.56379 -6.43406 -10.9417 19.2365 5.40593 8.35933 -4.78789 -0.0180093 -3.1675 3.53283 -0.220597 2.16622 -34.8053 -5.46039 5.48391 23.9419 -19.7291 6.44776 -2.79577 -0.32222 3.29026 2.10438 7.35679 3.86183 22.8203 7.21656 5.89943 7.91322 -3.31704 -4.61533 8.45997 2.26265 -9.42314 6.41494 8.29631 8.86901 -1.76397 3.84321 -2.162 -4.03659 -20.4056 -2.40136 10.2492 13.4674 -8.17147 -1.6994 15.5995 9.04113 -3.67875 -3.57743 -17.4031 -28.9528 -9.57329 -23.9697 -2.63503 -15.6086 9.79158 -4.25316 -11.2407 -10.9114 0.583649 -5.47373 -5.46966 18.129 5.14323 15.9895 26.0679 -18.3737 -3.82246 19.8687 -4.8577 6.27197 -8.2189 17.3278 12.0871 -3.17672 -13.475 -16.5719 20.3844 -2.0114 -5.32602 -11.7288 15.6531 -14.5491 9.28674 -3.84718 -0.737823 11.7775 3.35493 -3.91071 12.7301 -5.80079 12.6496 0.533115 -0.998853 -2.75647 -9.62879 -5.92556 -7.06605 10.756 0.0232991 -2.92029 -8.58807 1.12943 -1.49939 8.57768 -25.6641 -11.5327 -10.3914 4.2292 3.06723 4.17305 3.54616 -2.68512 10.1834 -8.59272 -11.7881 -23.9614 6.15331 0.337373 -4.02687 -1.88258 -2.75279 -7.43761 22.0502 -28.0706 -13.5135 -17.0251 4.4169 -2.98445 2.00621 -10.8582 -13.4742 26.1424 -0.148607 6.90982 -19.7207 -4.94349 -1.42505 -11.1037 10.7987 -2.94576 12.5014 7.82407 10.2485 -2.37356 10.855 1.82623 16.0419 -7.57958 -14.0128 9.86934 -0.880015 6.4855 -2.34023 14.4202 -3.01615 7.82779 2.33397 5.99736 11.0976 -2.68717 -10.7136 5.49395 5.79067 -16.9506 -2.222 -2.36339 3.09044 5.37435 -12.9812 5.95623 -5.06856 -13.6488 -0.87705 -5.52981 -2.7546 11.7889 1.12272 1.36535 -14.7137 1.67928 -6.73343 -7.30491 1.16554 5.60998 -0.295571 -5.10509 -11.5007 -18.2209 -2.7267 -19.5646 -6.62082 1.35068 9.53163 -4.75187 12.1226 -7.20931 7.23806 -4.98818 20.3836 -13.8351 0.629596 7.57027 0.511719 -10.4924 -10.9917 -8.64134 13.075 -5.59477 0.77773 -2.59347 0.058621 4.79573 8.82353 9.92916 0.161786 2.57696 6.76486 9.77348 16.2443 13.5981 -11.169 -4.45455 16.4104 7.91393 11.1554 16.5811 0.267288 7.10088 8.85759 15.2648 -22.1897 -6.0551 -6.08153 11.9587 2.47052 18.6309 -0.392699 2.52587 -22.1003 3.49885 6.97781 -22.3409 -18.8118 15.2792 -1.43907 3.04764 -3.52645 9.36657 14.3793 -2.01579 0.96604 3.22347 -6.41403 3.07927 -1.81389 -19.6288 -4.4183 -2.85023 -9.38014 3.63547 -7.05628 8.98519 3.63549 -9.77837 4.76244 6.66646 13.8618 1.98321 5.3266 -5.24981 18.9826 4.96757 10.9306 -5.58977 -17.1901 7.60131 -11.6218 -7.42597 -8.95936 -7.68156 -0.457975 -5.28321 -3.68499 -18.8081 5.21294 -0.846589 9.58988 -7.46311 -6.278 -5.79883 -1.47801 13.9268 4.15199 11.8016 1.229 -7.09913 4.5361 6.44485 -19.3353 5.45726 -15.5018 10.6415 16.2078 -2.79581 25.049 -10.8604 18.2601 -13.6124 -1.53936 10.0385 6.28885 10.4306 -1.27961 14.3956 -5.51691 -13.0456 14.7141 0.415053 -4.60585 4.70583 6.1747 2.02273 10.851 2.41854 13.1397 3.23573 -23.769 3.04503 -3.5225 -10.8145 15.2196 -15.4333 -7.50478 9.39688 -5.91948 -14.8308 -15.7561 7.48063 22.9101 -2.60271 6.58947 -8.89799 5.5754 -15.7239 0.580094 -16.0586 2.16958 -10.3788 -2.66029 -1.97422 4.97372 -6.3757 -2.9772 16.4635 -12.2381 13.1493 -32.7261 6.62787 14.5674 15.5784 9.58833 12.3836 2.39618 10.5699 12.1903 6.62444 -11.0169 -7.07212 -18.0862 10.6454 9.91109 0.561826 -8.0813 -3.92466 6.75809 5.48865 -2.7086 3.54893 1.52392 7.63576 -5.26288 7.21614 12.7913 -0.242614 -11.8295 -13.9975 -3.66513 -12.6808 -8.51004 -5.3356 30.1976 2.93126 -12.4553 23.4495 -5.80684 -0.33229 6.64162 -3.21554 -3.85887 1.09924 7.46454 -2.27829 1.49543 -20.4805 -5.60549 -7.22504 3.47595 15.3468 -3.49121 -25.0662 21.0378 -3.73248 9.81807 -4.23429 -25.5648 23.3276 1.0112 -8.26524 1.71746 18.6142 14.6577 -0.866468 -5.10841 -2.41229 12.1252 -1.92946 -1.14795 10.4803 -9.73934 -16.5714 -9.07791 -2.90332 -2.33988 -3.68204 4.41747 4.52127 -8.35802 20.1286 -5.1253 -10.6119 2.77678 9.41182 -4.8945 -3.01741 -2.07139 22.4524 16.2131 9.58842 -4.73263 -10.9727 -16.482 5.31928 -11.9354 -5.30797 2.73422 -9.27762 -16.7902 -9.02152 6.64376 6.41537 -16.0291 10.5222 -7.20178 -22.1321 -5.55025 -20.1151 16.1008 6.60207 9.18068 -4.14192 9.24078 -2.20243 -18.6966 -6.96757 -4.85715 -15.5228 -2.64255 -3.78848 17.9366 -0.661615 0.280285 -10.0329 11.8295 18.8689 0.529805 -8.51099 1.84596 -14.5028 -14.852 -14.9494 3.95172 -11.5011 -29.4048 -26.9454 1.95334 -10.4363 -24.0042 9.04538 -2.99525 10.3133 1.15173 -13.3186 0.123772 -13.5809 0.0258963 -4.29553 16.306 16.2318 9.28805 11.6168 6.77203 -5.72432 -9.09493 6.56885 0.815302 0.558424 2.40187 10.1191 4.1467 22.2018 -7.68212 5.81884 12.0922 -8.15453 -24.9722 15.3236 -4.37156 -1.43442 -8.98778 2.03559 -15.894 -1.24381 15.7671 10.1753 -5.17801 -7.24286 -18.7058 -15.4356 18.0515 2.24302 4.06533 1.35758 3.80793 2.14719 -0.624644 23.3333 4.11496 8.15446 10.0234 -2.70598 -14.4134 -2.94535 0.37692 5.6674 -4.77695 -10.768 13.7143 10.3384 11.1297 -1.04131 -1.02637 -0.482604 18.1372 -18.8333 -3.56622 -8.41695 9.74002 9.40893 -18.9208 -6.02992 -7.3437 -6.23242 -16.2929 -9.86607 19.7742 -12.3831 8.845 10.2129 12.5193 -3.64406 0.39173 -17.1056 1.42655 -3.32192 -13.4293 6.64151 -14.5653 10.1481 -9.18158 4.33577 5.66889 11.5481 12.8692 19.9995 3.5916 -1.52814 -16.5208 6.74016 -2.98605 6.07932 -7.90435 5.18859 15.6831 -0.3273 3.1198 -3.65359 -9.44084 -2.233 5.49036 -6.12005 -0.301632 -2.23638 -5.89369 -8.51105 -0.753823 1.1811 2.05236 -2.22753 4.14848 5.4189 3.42023 -7.39198 -14.412 12.1508 3.3601 1.42625 -11.5616 -0.500595 19.2485 -5.3356 -14.4425 -3.22287 -5.48961 20.2522 12.359 -4.62178 8.41416 -3.94011 -4.38093 -4.06703 5.42861 -1.91523 -0.909161 11.5446 2.91248 5.07598 -3.90245 -0.397585 -2.09456 -14.032 -10.0492 0.683789 4.37787 7.44828 -3.95382 -3.729 -13.3098 -21.6242 -18.456 -1.92741 -0.69824 -22.3459 -0.887445 -0.0113657 26.6139 17.7094 -2.19315 0.00131248 13.2733 1.98488 10.9469 10.8998 0.207552 -4.37997 -5.8779 -8.60135 6.36556 -18.7309 -2.76195 -1.05574 4.2079 2.88815 -1.90575 12.2432 -13.6672 -5.95639 -15.4531 6.34994 7.72521 19.6564 -4.74903 -1.21759 5.98537 -0.741431 -6.2606 -1.58144 6.38771 16.3309 -4.82653 -24.3406 -5.86519 2.31782 5.31393 -6.79195 -7.19596 -7.292 19.223 -14.4908 -1.088 0.63408 8.17919 0.131037 10.3597 -4.03459 -1.43034 9.30615 -17.2418 -12.3648 8.01003 12.101 5.56332 -10.2734 -1.1703 -6.1239 2.67761 11.5474 8.61113 6.21213 -8.5188 30.0494 16.2187 -14.6422 25.8802 -8.37466 18.2117 -3.757 1.73087 0.571746 -1.88491 20.8549 -2.67961 -6.41379 13.1407 4.96169 18.3228 27.3348 1.02011 -8.41846 7.62927 -13.9642 -14.2713 -6.31213 5.27756 -10.6104 -2.33101 2.10042 -14.0954 2.63182 0.0505723 12.0802 -7.93284 7.62318 -4.36529 -16.2396 0.665287 -5.94725 -4.70193 -14.8225 -18.7065 -12.3163 3.54274 -3.851 -0.449016 11.0777 -16.4143 -5.69115 -4.63633 -9.37737 -17.6801 -13.1725 -15.1596 10.3256 16.1846 8.57198 21.9412 9.94003 16.7924 27.2547 -7.35364 -6.79745 17.6683 -1.47444
[05/26/2023-16:10:47] [I] 
[05/26/2023-16:10:47] [I] === Profile (1941 iterations ) ===
[05/26/2023-16:10:47] [I]                                                                                                                           Layer   Time (ms)   Avg. Time (ms)   Median Time (ms)   Time %
[05/26/2023-16:10:47] [I]                                                                                                        /conv1/Conv + /relu/Relu       54.23           0.0279             0.0276      1.9
[05/26/2023-16:10:47] [I]                                                                                                                /maxpool/MaxPool       11.59           0.0060             0.0061      0.4
[05/26/2023-16:10:47] [I]                                                                        /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu       20.52           0.0106             0.0102      0.7
[05/26/2023-16:10:47] [I]                                                                      /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu       32.90           0.0170             0.0167      1.2
[05/26/2023-16:10:47] [I]                                                                                   /layer1/layer1.0/downsample/downsample.0/Conv       31.10           0.0160             0.0163      1.1
[05/26/2023-16:10:47] [I]                                               /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu       33.62           0.0173             0.0174      1.2
[05/26/2023-16:10:47] [I]                                                                        /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu       34.55           0.0178             0.0174      1.2
[05/26/2023-16:10:47] [I]                                                                      /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/relu_1/Relu       32.87           0.0169             0.0167      1.2
[05/26/2023-16:10:47] [I]                                               /layer1/layer1.1/conv3/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/relu_2/Relu       33.55           0.0173             0.0174      1.2
[05/26/2023-16:10:47] [I]                                                                        /layer1/layer1.2/conv1/Conv + /layer1/layer1.2/relu/Relu       34.08           0.0176             0.0174      1.2
[05/26/2023-16:10:47] [I]                                                                      /layer1/layer1.2/conv2/Conv + /layer1/layer1.2/relu_1/Relu       32.84           0.0169             0.0167      1.2
[05/26/2023-16:10:47] [I]                                               /layer1/layer1.2/conv3/Conv + /layer1/layer1.2/Add + /layer1/layer1.2/relu_2/Relu       33.40           0.0172             0.0174      1.2
[05/26/2023-16:10:47] [I]                                                                        /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu       45.15           0.0233             0.0235      1.6
[05/26/2023-16:10:47] [I]                                                                      /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu       77.04           0.0397             0.0398      2.7
[05/26/2023-16:10:47] [I]                                                                                   /layer2/layer2.0/downsample/downsample.0/Conv       50.41           0.0260             0.0256      1.8
[05/26/2023-16:10:47] [I]                                               /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu       34.04           0.0175             0.0174      1.2
[05/26/2023-16:10:47] [I]                                                                        /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu       43.49           0.0224             0.0225      1.5
[05/26/2023-16:10:47] [I]                                                                      /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu       43.35           0.0223             0.0225      1.5
[05/26/2023-16:10:47] [I]                                               /layer2/layer2.1/conv3/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/relu_2/Relu       31.86           0.0164             0.0164      1.1
[05/26/2023-16:10:47] [I]                                                                        /layer2/layer2.2/conv1/Conv + /layer2/layer2.2/relu/Relu       43.30           0.0223             0.0225      1.5
[05/26/2023-16:10:47] [I]                                                                      /layer2/layer2.2/conv2/Conv + /layer2/layer2.2/relu_1/Relu       42.26           0.0218             0.0215      1.5
[05/26/2023-16:10:47] [I]                                               /layer2/layer2.2/conv3/Conv + /layer2/layer2.2/Add + /layer2/layer2.2/relu_2/Relu       31.96           0.0165             0.0164      1.1
[05/26/2023-16:10:47] [I]                                                                        /layer2/layer2.3/conv1/Conv + /layer2/layer2.3/relu/Relu       43.21           0.0223             0.0225      1.5
[05/26/2023-16:10:47] [I]                                                                      /layer2/layer2.3/conv2/Conv + /layer2/layer2.3/relu_1/Relu       42.99           0.0221             0.0222      1.5
[05/26/2023-16:10:47] [I]                                               /layer2/layer2.3/conv3/Conv + /layer2/layer2.3/Add + /layer2/layer2.3/relu_2/Relu       31.87           0.0164             0.0164      1.1
[05/26/2023-16:10:47] [I]                                                                        /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu       41.36           0.0213             0.0215      1.5
[05/26/2023-16:10:47] [I]                          Reformatting CopyNode for Input Tensor 0 to /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu       17.36           0.0089             0.0092      0.6
[05/26/2023-16:10:47] [I]                                                                      /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu      105.88           0.0545             0.0543      3.8
[05/26/2023-16:10:47] [I]                                                                                   /layer3/layer3.0/downsample/downsample.0/Conv       57.71           0.0297             0.0297      2.0
[05/26/2023-16:10:47] [I]   Reformatting CopyNode for Input Tensor 0 to /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu        9.56           0.0049             0.0051      0.3
[05/26/2023-16:10:47] [I]                                               /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu       34.37           0.0177             0.0174      1.2
[05/26/2023-16:10:47] [I]                                                                        /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu       45.80           0.0236             0.0236      1.6
[05/26/2023-16:10:47] [I]                                                                      /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu       69.72           0.0359             0.0358      2.5
[05/26/2023-16:10:47] [I]                                               /layer3/layer3.1/conv3/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/relu_2/Relu       34.87           0.0180             0.0177      1.2
[05/26/2023-16:10:47] [I]                                                                        /layer3/layer3.2/conv1/Conv + /layer3/layer3.2/relu/Relu       45.79           0.0236             0.0236      1.6
[05/26/2023-16:10:47] [I]                                                                      /layer3/layer3.2/conv2/Conv + /layer3/layer3.2/relu_1/Relu       69.42           0.0358             0.0358      2.5
[05/26/2023-16:10:47] [I]                                               /layer3/layer3.2/conv3/Conv + /layer3/layer3.2/Add + /layer3/layer3.2/relu_2/Relu       35.07           0.0181             0.0182      1.2
[05/26/2023-16:10:47] [I]                                                                        /layer3/layer3.3/conv1/Conv + /layer3/layer3.3/relu/Relu       45.61           0.0235             0.0236      1.6
[05/26/2023-16:10:47] [I]                                                                      /layer3/layer3.3/conv2/Conv + /layer3/layer3.3/relu_1/Relu       69.39           0.0357             0.0358      2.5
[05/26/2023-16:10:47] [I]                                               /layer3/layer3.3/conv3/Conv + /layer3/layer3.3/Add + /layer3/layer3.3/relu_2/Relu       34.52           0.0178             0.0174      1.2
[05/26/2023-16:10:47] [I]                                                                        /layer3/layer3.4/conv1/Conv + /layer3/layer3.4/relu/Relu       45.72           0.0236             0.0236      1.6
[05/26/2023-16:10:47] [I]                                                                      /layer3/layer3.4/conv2/Conv + /layer3/layer3.4/relu_1/Relu       69.38           0.0357             0.0358      2.5
[05/26/2023-16:10:47] [I]                                               /layer3/layer3.4/conv3/Conv + /layer3/layer3.4/Add + /layer3/layer3.4/relu_2/Relu       35.09           0.0181             0.0183      1.2
[05/26/2023-16:10:47] [I]                                                                        /layer3/layer3.5/conv1/Conv + /layer3/layer3.5/relu/Relu       45.53           0.0235             0.0236      1.6
[05/26/2023-16:10:47] [I]                                                                      /layer3/layer3.5/conv2/Conv + /layer3/layer3.5/relu_1/Relu       69.36           0.0357             0.0358      2.5
[05/26/2023-16:10:47] [I]                                               /layer3/layer3.5/conv3/Conv + /layer3/layer3.5/Add + /layer3/layer3.5/relu_2/Relu       34.52           0.0178             0.0174      1.2
[05/26/2023-16:10:47] [I]                                                                        /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu       47.72           0.0246             0.0246      1.7
[05/26/2023-16:10:47] [I]                          Reformatting CopyNode for Input Tensor 0 to /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu       10.51           0.0054             0.0051      0.4
[05/26/2023-16:10:47] [I]                                                                      /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu      204.71           0.1055             0.1052      7.3
[05/26/2023-16:10:47] [I]                                       Reformatting CopyNode for Input Tensor 0 to /layer4/layer4.0/downsample/downsample.0/Conv       13.98           0.0072             0.0072      0.5
[05/26/2023-16:10:47] [I]                                                                                   /layer4/layer4.0/downsample/downsample.0/Conv       69.58           0.0358             0.0358      2.5
[05/26/2023-16:10:47] [I]                                               /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu       40.29           0.0208             0.0205      1.4
[05/26/2023-16:10:47] [I]  Reformatting CopyNode for Output Tensor 0 to /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu        9.29           0.0048             0.0051      0.3
[05/26/2023-16:10:47] [I]                                                                        /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu       43.50           0.0224             0.0225      1.5
[05/26/2023-16:10:47] [I]                                                                      /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu      119.88           0.0618             0.0614      4.3
[05/26/2023-16:10:47] [I]                                               /layer4/layer4.1/conv3/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/relu_2/Relu       45.47           0.0234             0.0236      1.6
[05/26/2023-16:10:47] [I]                                                                        /layer4/layer4.2/conv1/Conv + /layer4/layer4.2/relu/Relu       42.98           0.0221             0.0223      1.5
[05/26/2023-16:10:47] [I]                                                                      /layer4/layer4.2/conv2/Conv + /layer4/layer4.2/relu_1/Relu      119.87           0.0618             0.0614      4.3
[05/26/2023-16:10:47] [I]                                               /layer4/layer4.2/conv3/Conv + /layer4/layer4.2/Add + /layer4/layer4.2/relu_2/Relu       44.65           0.0230             0.0225      1.6
[05/26/2023-16:10:47] [I]                                                                                                      /avgpool/GlobalAveragePool       10.15           0.0052             0.0051      0.4
[05/26/2023-16:10:47] [I]                                                                                                                        /fc/Gemm       31.65           0.0163             0.0164      1.1
[05/26/2023-16:10:47] [I]                                                                                                                           Total     2816.42           1.4510             1.4438    100.0
[05/26/2023-16:10:47] [I] 
&&&& PASSED TensorRT.trtexec [TensorRT v8501] # trtexec --loadEngine=resnet50.engine --verbose --dumpOutput --dumpProfile --dumpLayerInfo --exportOutput=log/resnet50/infer/infer_output.log --exportProfile=log/resnet50/infer/infer_profile.log --exportLayerInfo=log/resnet50/infer/infer_layer_info.log --warmUp=200 --iterations=50
