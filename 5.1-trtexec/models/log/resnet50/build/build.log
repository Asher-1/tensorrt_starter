&&&& RUNNING TensorRT.trtexec [TensorRT v8501] # trtexec --onnx=resnet50.onnx --memPoolSize=workspace:2048 --saveEngine=resnet50.engine --verbose --profilingVerbosity=detailed --dumpOutput --dumpProfile --dumpLayerInfo --exportOutput=log/resnet50/build/build_output.log --exportProfile=log/resnet50/build/build_profile.log --exportLayerInfo=log/resnet50/build/build_layer_info.log --warmUp=200 --iterations=50
[05/26/2023-16:04:34] [I] === Model Options ===
[05/26/2023-16:04:34] [I] Format: ONNX
[05/26/2023-16:04:34] [I] Model: resnet50.onnx
[05/26/2023-16:04:34] [I] Output:
[05/26/2023-16:04:34] [I] === Build Options ===
[05/26/2023-16:04:34] [I] Max batch: explicit batch
[05/26/2023-16:04:34] [I] Memory Pools: workspace: 2048 MiB, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default
[05/26/2023-16:04:34] [I] minTiming: 1
[05/26/2023-16:04:34] [I] avgTiming: 8
[05/26/2023-16:04:34] [I] Precision: FP32
[05/26/2023-16:04:34] [I] LayerPrecisions: 
[05/26/2023-16:04:34] [I] Calibration: 
[05/26/2023-16:04:34] [I] Refit: Disabled
[05/26/2023-16:04:34] [I] Sparsity: Disabled
[05/26/2023-16:04:34] [I] Safe mode: Disabled
[05/26/2023-16:04:34] [I] DirectIO mode: Disabled
[05/26/2023-16:04:34] [I] Restricted mode: Disabled
[05/26/2023-16:04:34] [I] Build only: Disabled
[05/26/2023-16:04:34] [I] Save engine: resnet50.engine
[05/26/2023-16:04:34] [I] Load engine: 
[05/26/2023-16:04:34] [I] Profiling verbosity: 2
[05/26/2023-16:04:34] [I] Tactic sources: Using default tactic sources
[05/26/2023-16:04:34] [I] timingCacheMode: local
[05/26/2023-16:04:34] [I] timingCacheFile: 
[05/26/2023-16:04:34] [I] Heuristic: Disabled
[05/26/2023-16:04:34] [I] Preview Features: Use default preview flags.
[05/26/2023-16:04:34] [I] Input(s)s format: fp32:CHW
[05/26/2023-16:04:34] [I] Output(s)s format: fp32:CHW
[05/26/2023-16:04:34] [I] Input build shapes: model
[05/26/2023-16:04:34] [I] Input calibration shapes: model
[05/26/2023-16:04:34] [I] === System Options ===
[05/26/2023-16:04:34] [I] Device: 0
[05/26/2023-16:04:34] [I] DLACore: 
[05/26/2023-16:04:34] [I] Plugins:
[05/26/2023-16:04:34] [I] === Inference Options ===
[05/26/2023-16:04:34] [I] Batch: Explicit
[05/26/2023-16:04:34] [I] Input inference shapes: model
[05/26/2023-16:04:34] [I] Iterations: 50
[05/26/2023-16:04:34] [I] Duration: 3s (+ 200ms warm up)
[05/26/2023-16:04:34] [I] Sleep time: 0ms
[05/26/2023-16:04:34] [I] Idle time: 0ms
[05/26/2023-16:04:34] [I] Streams: 1
[05/26/2023-16:04:34] [I] ExposeDMA: Disabled
[05/26/2023-16:04:34] [I] Data transfers: Enabled
[05/26/2023-16:04:34] [I] Spin-wait: Disabled
[05/26/2023-16:04:34] [I] Multithreading: Disabled
[05/26/2023-16:04:34] [I] CUDA Graph: Disabled
[05/26/2023-16:04:34] [I] Separate profiling: Disabled
[05/26/2023-16:04:34] [I] Time Deserialize: Disabled
[05/26/2023-16:04:34] [I] Time Refit: Disabled
[05/26/2023-16:04:34] [I] NVTX verbosity: 2
[05/26/2023-16:04:34] [I] Persistent Cache Ratio: 0
[05/26/2023-16:04:34] [I] Inputs:
[05/26/2023-16:04:34] [I] === Reporting Options ===
[05/26/2023-16:04:34] [I] Verbose: Enabled
[05/26/2023-16:04:34] [I] Averages: 10 inferences
[05/26/2023-16:04:34] [I] Percentiles: 90,95,99
[05/26/2023-16:04:34] [I] Dump refittable layers:Disabled
[05/26/2023-16:04:34] [I] Dump output: Enabled
[05/26/2023-16:04:34] [I] Profile: Enabled
[05/26/2023-16:04:34] [I] Export timing to JSON file: 
[05/26/2023-16:04:34] [I] Export output to JSON file: log/resnet50/build/build_output.log
[05/26/2023-16:04:34] [I] Export profile to JSON file: log/resnet50/build/build_profile.log
[05/26/2023-16:04:34] [I] 
[05/26/2023-16:04:34] [I] === Device Information ===
[05/26/2023-16:04:34] [I] Selected Device: NVIDIA GeForce RTX 3080
[05/26/2023-16:04:34] [I] Compute Capability: 8.6
[05/26/2023-16:04:34] [I] SMs: 68
[05/26/2023-16:04:34] [I] Compute Clock Rate: 1.74 GHz
[05/26/2023-16:04:34] [I] Device Global Memory: 10009 MiB
[05/26/2023-16:04:34] [I] Shared Memory per SM: 100 KiB
[05/26/2023-16:04:34] [I] Memory Bus Width: 320 bits (ECC disabled)
[05/26/2023-16:04:34] [I] Memory Clock Rate: 9.501 GHz
[05/26/2023-16:04:34] [I] 
[05/26/2023-16:04:34] [I] TensorRT version: 8.5.1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::BatchedNMSDynamic_TRT version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::BatchedNMS_TRT version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::BatchTilePlugin_TRT version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::Clip_TRT version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::CoordConvAC version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::CropAndResizeDynamic version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::CropAndResize version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::DecodeBbox3DPlugin version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::DetectionLayer_TRT version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::EfficientNMS_Explicit_TF_TRT version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::EfficientNMS_Implicit_TF_TRT version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::EfficientNMS_ONNX_TRT version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::EfficientNMS_TRT version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::FlattenConcat_TRT version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::GenerateDetection_TRT version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::GridAnchor_TRT version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::GridAnchorRect_TRT version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::InstanceNormalization_TRT version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::InstanceNormalization_TRT version 2
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::LReLU_TRT version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::MultilevelCropAndResize_TRT version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::MultilevelProposeROI_TRT version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::MultiscaleDeformableAttnPlugin_TRT version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::NMSDynamic_TRT version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::NMS_TRT version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::Normalize_TRT version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::PillarScatterPlugin version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::PriorBox_TRT version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::ProposalDynamic version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::ProposalLayer_TRT version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::Proposal version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::PyramidROIAlign_TRT version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::Region_TRT version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::Reorg_TRT version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::ResizeNearest_TRT version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::ROIAlign_TRT version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::RPROI_TRT version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::ScatterND version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::SpecialSlice_TRT version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::Split version 1
[05/26/2023-16:04:34] [V] [TRT] Registered plugin creator - ::VoxelGeneratorPlugin version 1
[05/26/2023-16:04:34] [I] [TRT] [MemUsageChange] Init CUDA: CPU +325, GPU +0, now: CPU 337, GPU 451 (MiB)
[05/26/2023-16:04:34] [V] [TRT] Trying to load shared library libnvinfer_builder_resource.so.8.5.1
[05/26/2023-16:04:34] [V] [TRT] Loaded shared library libnvinfer_builder_resource.so.8.5.1
[05/26/2023-16:04:35] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +441, GPU +118, now: CPU 833, GPU 569 (MiB)
[05/26/2023-16:04:35] [I] Start parsing network model
[05/26/2023-16:04:35] [I] [TRT] ----------------------------------------------------------------
[05/26/2023-16:04:35] [I] [TRT] Input filename:   resnet50.onnx
[05/26/2023-16:04:35] [I] [TRT] ONNX IR version:  0.0.8
[05/26/2023-16:04:35] [I] [TRT] Opset version:    15
[05/26/2023-16:04:35] [I] [TRT] Producer name:    pytorch
[05/26/2023-16:04:35] [I] [TRT] Producer version: 2.0.1
[05/26/2023-16:04:35] [I] [TRT] Domain:           
[05/26/2023-16:04:35] [I] [TRT] Model version:    0
[05/26/2023-16:04:35] [I] [TRT] Doc string:       
[05/26/2023-16:04:35] [I] [TRT] ----------------------------------------------------------------
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::BatchedNMSDynamic_TRT version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::BatchedNMS_TRT version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::BatchTilePlugin_TRT version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::Clip_TRT version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::CoordConvAC version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::CropAndResizeDynamic version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::CropAndResize version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::DecodeBbox3DPlugin version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::DetectionLayer_TRT version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::EfficientNMS_Explicit_TF_TRT version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::EfficientNMS_Implicit_TF_TRT version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::EfficientNMS_ONNX_TRT version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::EfficientNMS_TRT version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::FlattenConcat_TRT version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::GenerateDetection_TRT version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::GridAnchor_TRT version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::GridAnchorRect_TRT version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::InstanceNormalization_TRT version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::InstanceNormalization_TRT version 2
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::LReLU_TRT version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::MultilevelCropAndResize_TRT version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::MultilevelProposeROI_TRT version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::MultiscaleDeformableAttnPlugin_TRT version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::NMSDynamic_TRT version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::NMS_TRT version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::Normalize_TRT version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::PillarScatterPlugin version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::PriorBox_TRT version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::ProposalDynamic version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::ProposalLayer_TRT version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::Proposal version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::PyramidROIAlign_TRT version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::Region_TRT version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::Reorg_TRT version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::ResizeNearest_TRT version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::ROIAlign_TRT version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::RPROI_TRT version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::ScatterND version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::SpecialSlice_TRT version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::Split version 1
[05/26/2023-16:04:35] [V] [TRT] Plugin creator already registered - ::VoxelGeneratorPlugin version 1
[05/26/2023-16:04:35] [V] [TRT] Adding network input: input0 with dtype: float32, dimensions: (1, 3, 224, 224)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: input0 for ONNX tensor: input0
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: fc.weight
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: fc.bias
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_497
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_498
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_500
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_503
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_506
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_507
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_509
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_512
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_515
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_518
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_521
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_524
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_527
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_530
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_531
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_533
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_536
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_537
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_539
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_542
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_545
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_548
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_551
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_554
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_557
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_560
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_563
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_566
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_569
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_572
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_575
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_576
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_578
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_581
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_584
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_587
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_590
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_593
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_596
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_599
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_602
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_605
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_608
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_611
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_614
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_617
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_620
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_623
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_626
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_629
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_632
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_633
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_635
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_638
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_641
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_644
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_647
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_650
[05/26/2023-16:04:35] [V] [TRT] Importing initializer: onnx::Conv_653
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /conv1/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: input0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_497
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_498
[05/26/2023-16:04:35] [V] [TRT] /conv1/Conv [Conv] inputs: [input0 -> (1, 3, 224, 224)[FLOAT]], [onnx::Conv_497 -> (64, 3, 7, 7)[FLOAT]], [onnx::Conv_498 -> (64)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 3, 224, 224)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /conv1/Conv for ONNX node: /conv1/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (7, 7), strides: (2, 2), prepadding: (3, 3), postpadding: (3, 3), dilations: (1, 1), numOutputs: 64
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 64, 112, 112)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /conv1/Conv_output_0 for ONNX tensor: /conv1/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /conv1/Conv [Conv] outputs: [/conv1/Conv_output_0 -> (1, 64, 112, 112)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /relu/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /conv1/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /relu/Relu [Relu] inputs: [/conv1/Conv_output_0 -> (1, 64, 112, 112)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /relu/Relu for ONNX node: /relu/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /relu/Relu_output_0 for ONNX tensor: /relu/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /relu/Relu [Relu] outputs: [/relu/Relu_output_0 -> (1, 64, 112, 112)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /maxpool/MaxPool [MaxPool]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /relu/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /maxpool/MaxPool [MaxPool] inputs: [/relu/Relu_output_0 -> (1, 64, 112, 112)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /maxpool/MaxPool for ONNX node: /maxpool/MaxPool
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /maxpool/MaxPool_output_0 for ONNX tensor: /maxpool/MaxPool_output_0
[05/26/2023-16:04:35] [V] [TRT] /maxpool/MaxPool [MaxPool] outputs: [/maxpool/MaxPool_output_0 -> (1, 64, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer1/layer1.0/conv1/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /maxpool/MaxPool_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_500
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_498
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.0/conv1/Conv [Conv] inputs: [/maxpool/MaxPool_output_0 -> (1, 64, 56, 56)[FLOAT]], [onnx::Conv_500 -> (64, 64, 1, 1)[FLOAT]], [onnx::Conv_498 -> (64)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 64, 56, 56)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer1/layer1.0/conv1/Conv for ONNX node: /layer1/layer1.0/conv1/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 64
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 64, 56, 56)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer1/layer1.0/conv1/Conv_output_0 for ONNX tensor: /layer1/layer1.0/conv1/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.0/conv1/Conv [Conv] outputs: [/layer1/layer1.0/conv1/Conv_output_0 -> (1, 64, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer1/layer1.0/relu/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer1/layer1.0/conv1/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.0/relu/Relu [Relu] inputs: [/layer1/layer1.0/conv1/Conv_output_0 -> (1, 64, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer1/layer1.0/relu/Relu for ONNX node: /layer1/layer1.0/relu/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer1/layer1.0/relu/Relu_output_0 for ONNX tensor: /layer1/layer1.0/relu/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.0/relu/Relu [Relu] outputs: [/layer1/layer1.0/relu/Relu_output_0 -> (1, 64, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer1/layer1.0/conv2/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer1/layer1.0/relu/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_503
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_498
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.0/conv2/Conv [Conv] inputs: [/layer1/layer1.0/relu/Relu_output_0 -> (1, 64, 56, 56)[FLOAT]], [onnx::Conv_503 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_498 -> (64)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 64, 56, 56)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer1/layer1.0/conv2/Conv for ONNX node: /layer1/layer1.0/conv2/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 64, 56, 56)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer1/layer1.0/conv2/Conv_output_0 for ONNX tensor: /layer1/layer1.0/conv2/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.0/conv2/Conv [Conv] outputs: [/layer1/layer1.0/conv2/Conv_output_0 -> (1, 64, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer1/layer1.0/relu_1/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer1/layer1.0/conv2/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.0/relu_1/Relu [Relu] inputs: [/layer1/layer1.0/conv2/Conv_output_0 -> (1, 64, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer1/layer1.0/relu_1/Relu for ONNX node: /layer1/layer1.0/relu_1/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer1/layer1.0/relu_1/Relu_output_0 for ONNX tensor: /layer1/layer1.0/relu_1/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.0/relu_1/Relu [Relu] outputs: [/layer1/layer1.0/relu_1/Relu_output_0 -> (1, 64, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer1/layer1.0/conv3/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer1/layer1.0/relu_1/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_506
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_507
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.0/conv3/Conv [Conv] inputs: [/layer1/layer1.0/relu_1/Relu_output_0 -> (1, 64, 56, 56)[FLOAT]], [onnx::Conv_506 -> (256, 64, 1, 1)[FLOAT]], [onnx::Conv_507 -> (256)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 64, 56, 56)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer1/layer1.0/conv3/Conv for ONNX node: /layer1/layer1.0/conv3/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 256
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 256, 56, 56)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer1/layer1.0/conv3/Conv_output_0 for ONNX tensor: /layer1/layer1.0/conv3/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.0/conv3/Conv [Conv] outputs: [/layer1/layer1.0/conv3/Conv_output_0 -> (1, 256, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer1/layer1.0/downsample/downsample.0/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /maxpool/MaxPool_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_509
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_507
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv [Conv] inputs: [/maxpool/MaxPool_output_0 -> (1, 64, 56, 56)[FLOAT]], [onnx::Conv_509 -> (256, 64, 1, 1)[FLOAT]], [onnx::Conv_507 -> (256)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 64, 56, 56)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer1/layer1.0/downsample/downsample.0/Conv for ONNX node: /layer1/layer1.0/downsample/downsample.0/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 256
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 256, 56, 56)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer1/layer1.0/downsample/downsample.0/Conv_output_0 for ONNX tensor: /layer1/layer1.0/downsample/downsample.0/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv [Conv] outputs: [/layer1/layer1.0/downsample/downsample.0/Conv_output_0 -> (1, 256, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer1/layer1.0/Add [Add]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer1/layer1.0/conv3/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer1/layer1.0/downsample/downsample.0/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.0/Add [Add] inputs: [/layer1/layer1.0/conv3/Conv_output_0 -> (1, 256, 56, 56)[FLOAT]], [/layer1/layer1.0/downsample/downsample.0/Conv_output_0 -> (1, 256, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer1/layer1.0/Add for ONNX node: /layer1/layer1.0/Add
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer1/layer1.0/Add_output_0 for ONNX tensor: /layer1/layer1.0/Add_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.0/Add [Add] outputs: [/layer1/layer1.0/Add_output_0 -> (1, 256, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer1/layer1.0/relu_2/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer1/layer1.0/Add_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.0/relu_2/Relu [Relu] inputs: [/layer1/layer1.0/Add_output_0 -> (1, 256, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer1/layer1.0/relu_2/Relu for ONNX node: /layer1/layer1.0/relu_2/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer1/layer1.0/relu_2/Relu_output_0 for ONNX tensor: /layer1/layer1.0/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.0/relu_2/Relu [Relu] outputs: [/layer1/layer1.0/relu_2/Relu_output_0 -> (1, 256, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer1/layer1.1/conv1/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer1/layer1.0/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_512
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_498
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.1/conv1/Conv [Conv] inputs: [/layer1/layer1.0/relu_2/Relu_output_0 -> (1, 256, 56, 56)[FLOAT]], [onnx::Conv_512 -> (64, 256, 1, 1)[FLOAT]], [onnx::Conv_498 -> (64)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 256, 56, 56)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer1/layer1.1/conv1/Conv for ONNX node: /layer1/layer1.1/conv1/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 64
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 64, 56, 56)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer1/layer1.1/conv1/Conv_output_0 for ONNX tensor: /layer1/layer1.1/conv1/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.1/conv1/Conv [Conv] outputs: [/layer1/layer1.1/conv1/Conv_output_0 -> (1, 64, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer1/layer1.1/relu/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer1/layer1.1/conv1/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.1/relu/Relu [Relu] inputs: [/layer1/layer1.1/conv1/Conv_output_0 -> (1, 64, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer1/layer1.1/relu/Relu for ONNX node: /layer1/layer1.1/relu/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer1/layer1.1/relu/Relu_output_0 for ONNX tensor: /layer1/layer1.1/relu/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.1/relu/Relu [Relu] outputs: [/layer1/layer1.1/relu/Relu_output_0 -> (1, 64, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer1/layer1.1/conv2/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer1/layer1.1/relu/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_515
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_498
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.1/conv2/Conv [Conv] inputs: [/layer1/layer1.1/relu/Relu_output_0 -> (1, 64, 56, 56)[FLOAT]], [onnx::Conv_515 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_498 -> (64)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 64, 56, 56)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer1/layer1.1/conv2/Conv for ONNX node: /layer1/layer1.1/conv2/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 64, 56, 56)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer1/layer1.1/conv2/Conv_output_0 for ONNX tensor: /layer1/layer1.1/conv2/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.1/conv2/Conv [Conv] outputs: [/layer1/layer1.1/conv2/Conv_output_0 -> (1, 64, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer1/layer1.1/relu_1/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer1/layer1.1/conv2/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.1/relu_1/Relu [Relu] inputs: [/layer1/layer1.1/conv2/Conv_output_0 -> (1, 64, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer1/layer1.1/relu_1/Relu for ONNX node: /layer1/layer1.1/relu_1/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer1/layer1.1/relu_1/Relu_output_0 for ONNX tensor: /layer1/layer1.1/relu_1/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.1/relu_1/Relu [Relu] outputs: [/layer1/layer1.1/relu_1/Relu_output_0 -> (1, 64, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer1/layer1.1/conv3/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer1/layer1.1/relu_1/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_518
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_507
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.1/conv3/Conv [Conv] inputs: [/layer1/layer1.1/relu_1/Relu_output_0 -> (1, 64, 56, 56)[FLOAT]], [onnx::Conv_518 -> (256, 64, 1, 1)[FLOAT]], [onnx::Conv_507 -> (256)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 64, 56, 56)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer1/layer1.1/conv3/Conv for ONNX node: /layer1/layer1.1/conv3/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 256
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 256, 56, 56)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer1/layer1.1/conv3/Conv_output_0 for ONNX tensor: /layer1/layer1.1/conv3/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.1/conv3/Conv [Conv] outputs: [/layer1/layer1.1/conv3/Conv_output_0 -> (1, 256, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer1/layer1.1/Add [Add]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer1/layer1.1/conv3/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer1/layer1.0/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.1/Add [Add] inputs: [/layer1/layer1.1/conv3/Conv_output_0 -> (1, 256, 56, 56)[FLOAT]], [/layer1/layer1.0/relu_2/Relu_output_0 -> (1, 256, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer1/layer1.1/Add for ONNX node: /layer1/layer1.1/Add
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer1/layer1.1/Add_output_0 for ONNX tensor: /layer1/layer1.1/Add_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.1/Add [Add] outputs: [/layer1/layer1.1/Add_output_0 -> (1, 256, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer1/layer1.1/relu_2/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer1/layer1.1/Add_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.1/relu_2/Relu [Relu] inputs: [/layer1/layer1.1/Add_output_0 -> (1, 256, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer1/layer1.1/relu_2/Relu for ONNX node: /layer1/layer1.1/relu_2/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer1/layer1.1/relu_2/Relu_output_0 for ONNX tensor: /layer1/layer1.1/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.1/relu_2/Relu [Relu] outputs: [/layer1/layer1.1/relu_2/Relu_output_0 -> (1, 256, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer1/layer1.2/conv1/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer1/layer1.1/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_521
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_498
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.2/conv1/Conv [Conv] inputs: [/layer1/layer1.1/relu_2/Relu_output_0 -> (1, 256, 56, 56)[FLOAT]], [onnx::Conv_521 -> (64, 256, 1, 1)[FLOAT]], [onnx::Conv_498 -> (64)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 256, 56, 56)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer1/layer1.2/conv1/Conv for ONNX node: /layer1/layer1.2/conv1/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 64
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 64, 56, 56)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer1/layer1.2/conv1/Conv_output_0 for ONNX tensor: /layer1/layer1.2/conv1/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.2/conv1/Conv [Conv] outputs: [/layer1/layer1.2/conv1/Conv_output_0 -> (1, 64, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer1/layer1.2/relu/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer1/layer1.2/conv1/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.2/relu/Relu [Relu] inputs: [/layer1/layer1.2/conv1/Conv_output_0 -> (1, 64, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer1/layer1.2/relu/Relu for ONNX node: /layer1/layer1.2/relu/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer1/layer1.2/relu/Relu_output_0 for ONNX tensor: /layer1/layer1.2/relu/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.2/relu/Relu [Relu] outputs: [/layer1/layer1.2/relu/Relu_output_0 -> (1, 64, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer1/layer1.2/conv2/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer1/layer1.2/relu/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_524
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_498
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.2/conv2/Conv [Conv] inputs: [/layer1/layer1.2/relu/Relu_output_0 -> (1, 64, 56, 56)[FLOAT]], [onnx::Conv_524 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_498 -> (64)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 64, 56, 56)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer1/layer1.2/conv2/Conv for ONNX node: /layer1/layer1.2/conv2/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 64, 56, 56)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer1/layer1.2/conv2/Conv_output_0 for ONNX tensor: /layer1/layer1.2/conv2/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.2/conv2/Conv [Conv] outputs: [/layer1/layer1.2/conv2/Conv_output_0 -> (1, 64, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer1/layer1.2/relu_1/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer1/layer1.2/conv2/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.2/relu_1/Relu [Relu] inputs: [/layer1/layer1.2/conv2/Conv_output_0 -> (1, 64, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer1/layer1.2/relu_1/Relu for ONNX node: /layer1/layer1.2/relu_1/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer1/layer1.2/relu_1/Relu_output_0 for ONNX tensor: /layer1/layer1.2/relu_1/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.2/relu_1/Relu [Relu] outputs: [/layer1/layer1.2/relu_1/Relu_output_0 -> (1, 64, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer1/layer1.2/conv3/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer1/layer1.2/relu_1/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_527
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_507
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.2/conv3/Conv [Conv] inputs: [/layer1/layer1.2/relu_1/Relu_output_0 -> (1, 64, 56, 56)[FLOAT]], [onnx::Conv_527 -> (256, 64, 1, 1)[FLOAT]], [onnx::Conv_507 -> (256)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 64, 56, 56)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer1/layer1.2/conv3/Conv for ONNX node: /layer1/layer1.2/conv3/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 256
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 256, 56, 56)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer1/layer1.2/conv3/Conv_output_0 for ONNX tensor: /layer1/layer1.2/conv3/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.2/conv3/Conv [Conv] outputs: [/layer1/layer1.2/conv3/Conv_output_0 -> (1, 256, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer1/layer1.2/Add [Add]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer1/layer1.2/conv3/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer1/layer1.1/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.2/Add [Add] inputs: [/layer1/layer1.2/conv3/Conv_output_0 -> (1, 256, 56, 56)[FLOAT]], [/layer1/layer1.1/relu_2/Relu_output_0 -> (1, 256, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer1/layer1.2/Add for ONNX node: /layer1/layer1.2/Add
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer1/layer1.2/Add_output_0 for ONNX tensor: /layer1/layer1.2/Add_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.2/Add [Add] outputs: [/layer1/layer1.2/Add_output_0 -> (1, 256, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer1/layer1.2/relu_2/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer1/layer1.2/Add_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.2/relu_2/Relu [Relu] inputs: [/layer1/layer1.2/Add_output_0 -> (1, 256, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer1/layer1.2/relu_2/Relu for ONNX node: /layer1/layer1.2/relu_2/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer1/layer1.2/relu_2/Relu_output_0 for ONNX tensor: /layer1/layer1.2/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer1/layer1.2/relu_2/Relu [Relu] outputs: [/layer1/layer1.2/relu_2/Relu_output_0 -> (1, 256, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer2/layer2.0/conv1/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer1/layer1.2/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_530
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_531
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.0/conv1/Conv [Conv] inputs: [/layer1/layer1.2/relu_2/Relu_output_0 -> (1, 256, 56, 56)[FLOAT]], [onnx::Conv_530 -> (128, 256, 1, 1)[FLOAT]], [onnx::Conv_531 -> (128)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 256, 56, 56)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer2/layer2.0/conv1/Conv for ONNX node: /layer2/layer2.0/conv1/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 128
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 128, 56, 56)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer2/layer2.0/conv1/Conv_output_0 for ONNX tensor: /layer2/layer2.0/conv1/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.0/conv1/Conv [Conv] outputs: [/layer2/layer2.0/conv1/Conv_output_0 -> (1, 128, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer2/layer2.0/relu/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer2/layer2.0/conv1/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.0/relu/Relu [Relu] inputs: [/layer2/layer2.0/conv1/Conv_output_0 -> (1, 128, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer2/layer2.0/relu/Relu for ONNX node: /layer2/layer2.0/relu/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer2/layer2.0/relu/Relu_output_0 for ONNX tensor: /layer2/layer2.0/relu/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.0/relu/Relu [Relu] outputs: [/layer2/layer2.0/relu/Relu_output_0 -> (1, 128, 56, 56)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer2/layer2.0/conv2/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer2/layer2.0/relu/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_533
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_531
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.0/conv2/Conv [Conv] inputs: [/layer2/layer2.0/relu/Relu_output_0 -> (1, 128, 56, 56)[FLOAT]], [onnx::Conv_533 -> (128, 128, 3, 3)[FLOAT]], [onnx::Conv_531 -> (128)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 128, 56, 56)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer2/layer2.0/conv2/Conv for ONNX node: /layer2/layer2.0/conv2/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (3, 3), strides: (2, 2), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 128
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 128, 28, 28)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer2/layer2.0/conv2/Conv_output_0 for ONNX tensor: /layer2/layer2.0/conv2/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.0/conv2/Conv [Conv] outputs: [/layer2/layer2.0/conv2/Conv_output_0 -> (1, 128, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer2/layer2.0/relu_1/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer2/layer2.0/conv2/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.0/relu_1/Relu [Relu] inputs: [/layer2/layer2.0/conv2/Conv_output_0 -> (1, 128, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer2/layer2.0/relu_1/Relu for ONNX node: /layer2/layer2.0/relu_1/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer2/layer2.0/relu_1/Relu_output_0 for ONNX tensor: /layer2/layer2.0/relu_1/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.0/relu_1/Relu [Relu] outputs: [/layer2/layer2.0/relu_1/Relu_output_0 -> (1, 128, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer2/layer2.0/conv3/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer2/layer2.0/relu_1/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_536
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_537
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.0/conv3/Conv [Conv] inputs: [/layer2/layer2.0/relu_1/Relu_output_0 -> (1, 128, 28, 28)[FLOAT]], [onnx::Conv_536 -> (512, 128, 1, 1)[FLOAT]], [onnx::Conv_537 -> (512)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 128, 28, 28)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer2/layer2.0/conv3/Conv for ONNX node: /layer2/layer2.0/conv3/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 512
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 512, 28, 28)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer2/layer2.0/conv3/Conv_output_0 for ONNX tensor: /layer2/layer2.0/conv3/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.0/conv3/Conv [Conv] outputs: [/layer2/layer2.0/conv3/Conv_output_0 -> (1, 512, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer2/layer2.0/downsample/downsample.0/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer1/layer1.2/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_539
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_537
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv [Conv] inputs: [/layer1/layer1.2/relu_2/Relu_output_0 -> (1, 256, 56, 56)[FLOAT]], [onnx::Conv_539 -> (512, 256, 1, 1)[FLOAT]], [onnx::Conv_537 -> (512)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 256, 56, 56)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer2/layer2.0/downsample/downsample.0/Conv for ONNX node: /layer2/layer2.0/downsample/downsample.0/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (1, 1), strides: (2, 2), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 512
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 512, 28, 28)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer2/layer2.0/downsample/downsample.0/Conv_output_0 for ONNX tensor: /layer2/layer2.0/downsample/downsample.0/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv [Conv] outputs: [/layer2/layer2.0/downsample/downsample.0/Conv_output_0 -> (1, 512, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer2/layer2.0/Add [Add]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer2/layer2.0/conv3/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer2/layer2.0/downsample/downsample.0/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.0/Add [Add] inputs: [/layer2/layer2.0/conv3/Conv_output_0 -> (1, 512, 28, 28)[FLOAT]], [/layer2/layer2.0/downsample/downsample.0/Conv_output_0 -> (1, 512, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer2/layer2.0/Add for ONNX node: /layer2/layer2.0/Add
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer2/layer2.0/Add_output_0 for ONNX tensor: /layer2/layer2.0/Add_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.0/Add [Add] outputs: [/layer2/layer2.0/Add_output_0 -> (1, 512, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer2/layer2.0/relu_2/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer2/layer2.0/Add_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.0/relu_2/Relu [Relu] inputs: [/layer2/layer2.0/Add_output_0 -> (1, 512, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer2/layer2.0/relu_2/Relu for ONNX node: /layer2/layer2.0/relu_2/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer2/layer2.0/relu_2/Relu_output_0 for ONNX tensor: /layer2/layer2.0/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.0/relu_2/Relu [Relu] outputs: [/layer2/layer2.0/relu_2/Relu_output_0 -> (1, 512, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer2/layer2.1/conv1/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer2/layer2.0/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_542
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_531
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.1/conv1/Conv [Conv] inputs: [/layer2/layer2.0/relu_2/Relu_output_0 -> (1, 512, 28, 28)[FLOAT]], [onnx::Conv_542 -> (128, 512, 1, 1)[FLOAT]], [onnx::Conv_531 -> (128)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 512, 28, 28)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer2/layer2.1/conv1/Conv for ONNX node: /layer2/layer2.1/conv1/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 128
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 128, 28, 28)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer2/layer2.1/conv1/Conv_output_0 for ONNX tensor: /layer2/layer2.1/conv1/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.1/conv1/Conv [Conv] outputs: [/layer2/layer2.1/conv1/Conv_output_0 -> (1, 128, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer2/layer2.1/relu/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer2/layer2.1/conv1/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.1/relu/Relu [Relu] inputs: [/layer2/layer2.1/conv1/Conv_output_0 -> (1, 128, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer2/layer2.1/relu/Relu for ONNX node: /layer2/layer2.1/relu/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer2/layer2.1/relu/Relu_output_0 for ONNX tensor: /layer2/layer2.1/relu/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.1/relu/Relu [Relu] outputs: [/layer2/layer2.1/relu/Relu_output_0 -> (1, 128, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer2/layer2.1/conv2/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer2/layer2.1/relu/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_545
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_531
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.1/conv2/Conv [Conv] inputs: [/layer2/layer2.1/relu/Relu_output_0 -> (1, 128, 28, 28)[FLOAT]], [onnx::Conv_545 -> (128, 128, 3, 3)[FLOAT]], [onnx::Conv_531 -> (128)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 128, 28, 28)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer2/layer2.1/conv2/Conv for ONNX node: /layer2/layer2.1/conv2/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 128
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 128, 28, 28)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer2/layer2.1/conv2/Conv_output_0 for ONNX tensor: /layer2/layer2.1/conv2/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.1/conv2/Conv [Conv] outputs: [/layer2/layer2.1/conv2/Conv_output_0 -> (1, 128, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer2/layer2.1/relu_1/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer2/layer2.1/conv2/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.1/relu_1/Relu [Relu] inputs: [/layer2/layer2.1/conv2/Conv_output_0 -> (1, 128, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer2/layer2.1/relu_1/Relu for ONNX node: /layer2/layer2.1/relu_1/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer2/layer2.1/relu_1/Relu_output_0 for ONNX tensor: /layer2/layer2.1/relu_1/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.1/relu_1/Relu [Relu] outputs: [/layer2/layer2.1/relu_1/Relu_output_0 -> (1, 128, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer2/layer2.1/conv3/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer2/layer2.1/relu_1/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_548
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_537
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.1/conv3/Conv [Conv] inputs: [/layer2/layer2.1/relu_1/Relu_output_0 -> (1, 128, 28, 28)[FLOAT]], [onnx::Conv_548 -> (512, 128, 1, 1)[FLOAT]], [onnx::Conv_537 -> (512)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 128, 28, 28)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer2/layer2.1/conv3/Conv for ONNX node: /layer2/layer2.1/conv3/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 512
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 512, 28, 28)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer2/layer2.1/conv3/Conv_output_0 for ONNX tensor: /layer2/layer2.1/conv3/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.1/conv3/Conv [Conv] outputs: [/layer2/layer2.1/conv3/Conv_output_0 -> (1, 512, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer2/layer2.1/Add [Add]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer2/layer2.1/conv3/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer2/layer2.0/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.1/Add [Add] inputs: [/layer2/layer2.1/conv3/Conv_output_0 -> (1, 512, 28, 28)[FLOAT]], [/layer2/layer2.0/relu_2/Relu_output_0 -> (1, 512, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer2/layer2.1/Add for ONNX node: /layer2/layer2.1/Add
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer2/layer2.1/Add_output_0 for ONNX tensor: /layer2/layer2.1/Add_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.1/Add [Add] outputs: [/layer2/layer2.1/Add_output_0 -> (1, 512, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer2/layer2.1/relu_2/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer2/layer2.1/Add_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.1/relu_2/Relu [Relu] inputs: [/layer2/layer2.1/Add_output_0 -> (1, 512, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer2/layer2.1/relu_2/Relu for ONNX node: /layer2/layer2.1/relu_2/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer2/layer2.1/relu_2/Relu_output_0 for ONNX tensor: /layer2/layer2.1/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.1/relu_2/Relu [Relu] outputs: [/layer2/layer2.1/relu_2/Relu_output_0 -> (1, 512, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer2/layer2.2/conv1/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer2/layer2.1/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_551
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_531
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.2/conv1/Conv [Conv] inputs: [/layer2/layer2.1/relu_2/Relu_output_0 -> (1, 512, 28, 28)[FLOAT]], [onnx::Conv_551 -> (128, 512, 1, 1)[FLOAT]], [onnx::Conv_531 -> (128)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 512, 28, 28)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer2/layer2.2/conv1/Conv for ONNX node: /layer2/layer2.2/conv1/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 128
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 128, 28, 28)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer2/layer2.2/conv1/Conv_output_0 for ONNX tensor: /layer2/layer2.2/conv1/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.2/conv1/Conv [Conv] outputs: [/layer2/layer2.2/conv1/Conv_output_0 -> (1, 128, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer2/layer2.2/relu/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer2/layer2.2/conv1/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.2/relu/Relu [Relu] inputs: [/layer2/layer2.2/conv1/Conv_output_0 -> (1, 128, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer2/layer2.2/relu/Relu for ONNX node: /layer2/layer2.2/relu/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer2/layer2.2/relu/Relu_output_0 for ONNX tensor: /layer2/layer2.2/relu/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.2/relu/Relu [Relu] outputs: [/layer2/layer2.2/relu/Relu_output_0 -> (1, 128, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer2/layer2.2/conv2/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer2/layer2.2/relu/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_554
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_531
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.2/conv2/Conv [Conv] inputs: [/layer2/layer2.2/relu/Relu_output_0 -> (1, 128, 28, 28)[FLOAT]], [onnx::Conv_554 -> (128, 128, 3, 3)[FLOAT]], [onnx::Conv_531 -> (128)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 128, 28, 28)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer2/layer2.2/conv2/Conv for ONNX node: /layer2/layer2.2/conv2/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 128
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 128, 28, 28)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer2/layer2.2/conv2/Conv_output_0 for ONNX tensor: /layer2/layer2.2/conv2/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.2/conv2/Conv [Conv] outputs: [/layer2/layer2.2/conv2/Conv_output_0 -> (1, 128, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer2/layer2.2/relu_1/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer2/layer2.2/conv2/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.2/relu_1/Relu [Relu] inputs: [/layer2/layer2.2/conv2/Conv_output_0 -> (1, 128, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer2/layer2.2/relu_1/Relu for ONNX node: /layer2/layer2.2/relu_1/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer2/layer2.2/relu_1/Relu_output_0 for ONNX tensor: /layer2/layer2.2/relu_1/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.2/relu_1/Relu [Relu] outputs: [/layer2/layer2.2/relu_1/Relu_output_0 -> (1, 128, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer2/layer2.2/conv3/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer2/layer2.2/relu_1/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_557
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_537
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.2/conv3/Conv [Conv] inputs: [/layer2/layer2.2/relu_1/Relu_output_0 -> (1, 128, 28, 28)[FLOAT]], [onnx::Conv_557 -> (512, 128, 1, 1)[FLOAT]], [onnx::Conv_537 -> (512)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 128, 28, 28)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer2/layer2.2/conv3/Conv for ONNX node: /layer2/layer2.2/conv3/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 512
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 512, 28, 28)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer2/layer2.2/conv3/Conv_output_0 for ONNX tensor: /layer2/layer2.2/conv3/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.2/conv3/Conv [Conv] outputs: [/layer2/layer2.2/conv3/Conv_output_0 -> (1, 512, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer2/layer2.2/Add [Add]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer2/layer2.2/conv3/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer2/layer2.1/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.2/Add [Add] inputs: [/layer2/layer2.2/conv3/Conv_output_0 -> (1, 512, 28, 28)[FLOAT]], [/layer2/layer2.1/relu_2/Relu_output_0 -> (1, 512, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer2/layer2.2/Add for ONNX node: /layer2/layer2.2/Add
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer2/layer2.2/Add_output_0 for ONNX tensor: /layer2/layer2.2/Add_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.2/Add [Add] outputs: [/layer2/layer2.2/Add_output_0 -> (1, 512, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer2/layer2.2/relu_2/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer2/layer2.2/Add_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.2/relu_2/Relu [Relu] inputs: [/layer2/layer2.2/Add_output_0 -> (1, 512, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer2/layer2.2/relu_2/Relu for ONNX node: /layer2/layer2.2/relu_2/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer2/layer2.2/relu_2/Relu_output_0 for ONNX tensor: /layer2/layer2.2/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.2/relu_2/Relu [Relu] outputs: [/layer2/layer2.2/relu_2/Relu_output_0 -> (1, 512, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer2/layer2.3/conv1/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer2/layer2.2/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_560
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_531
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.3/conv1/Conv [Conv] inputs: [/layer2/layer2.2/relu_2/Relu_output_0 -> (1, 512, 28, 28)[FLOAT]], [onnx::Conv_560 -> (128, 512, 1, 1)[FLOAT]], [onnx::Conv_531 -> (128)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 512, 28, 28)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer2/layer2.3/conv1/Conv for ONNX node: /layer2/layer2.3/conv1/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 128
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 128, 28, 28)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer2/layer2.3/conv1/Conv_output_0 for ONNX tensor: /layer2/layer2.3/conv1/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.3/conv1/Conv [Conv] outputs: [/layer2/layer2.3/conv1/Conv_output_0 -> (1, 128, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer2/layer2.3/relu/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer2/layer2.3/conv1/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.3/relu/Relu [Relu] inputs: [/layer2/layer2.3/conv1/Conv_output_0 -> (1, 128, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer2/layer2.3/relu/Relu for ONNX node: /layer2/layer2.3/relu/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer2/layer2.3/relu/Relu_output_0 for ONNX tensor: /layer2/layer2.3/relu/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.3/relu/Relu [Relu] outputs: [/layer2/layer2.3/relu/Relu_output_0 -> (1, 128, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer2/layer2.3/conv2/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer2/layer2.3/relu/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_563
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_531
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.3/conv2/Conv [Conv] inputs: [/layer2/layer2.3/relu/Relu_output_0 -> (1, 128, 28, 28)[FLOAT]], [onnx::Conv_563 -> (128, 128, 3, 3)[FLOAT]], [onnx::Conv_531 -> (128)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 128, 28, 28)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer2/layer2.3/conv2/Conv for ONNX node: /layer2/layer2.3/conv2/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 128
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 128, 28, 28)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer2/layer2.3/conv2/Conv_output_0 for ONNX tensor: /layer2/layer2.3/conv2/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.3/conv2/Conv [Conv] outputs: [/layer2/layer2.3/conv2/Conv_output_0 -> (1, 128, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer2/layer2.3/relu_1/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer2/layer2.3/conv2/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.3/relu_1/Relu [Relu] inputs: [/layer2/layer2.3/conv2/Conv_output_0 -> (1, 128, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer2/layer2.3/relu_1/Relu for ONNX node: /layer2/layer2.3/relu_1/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer2/layer2.3/relu_1/Relu_output_0 for ONNX tensor: /layer2/layer2.3/relu_1/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.3/relu_1/Relu [Relu] outputs: [/layer2/layer2.3/relu_1/Relu_output_0 -> (1, 128, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer2/layer2.3/conv3/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer2/layer2.3/relu_1/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_566
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_537
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.3/conv3/Conv [Conv] inputs: [/layer2/layer2.3/relu_1/Relu_output_0 -> (1, 128, 28, 28)[FLOAT]], [onnx::Conv_566 -> (512, 128, 1, 1)[FLOAT]], [onnx::Conv_537 -> (512)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 128, 28, 28)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer2/layer2.3/conv3/Conv for ONNX node: /layer2/layer2.3/conv3/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 512
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 512, 28, 28)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer2/layer2.3/conv3/Conv_output_0 for ONNX tensor: /layer2/layer2.3/conv3/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.3/conv3/Conv [Conv] outputs: [/layer2/layer2.3/conv3/Conv_output_0 -> (1, 512, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer2/layer2.3/Add [Add]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer2/layer2.3/conv3/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer2/layer2.2/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.3/Add [Add] inputs: [/layer2/layer2.3/conv3/Conv_output_0 -> (1, 512, 28, 28)[FLOAT]], [/layer2/layer2.2/relu_2/Relu_output_0 -> (1, 512, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer2/layer2.3/Add for ONNX node: /layer2/layer2.3/Add
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer2/layer2.3/Add_output_0 for ONNX tensor: /layer2/layer2.3/Add_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.3/Add [Add] outputs: [/layer2/layer2.3/Add_output_0 -> (1, 512, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer2/layer2.3/relu_2/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer2/layer2.3/Add_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.3/relu_2/Relu [Relu] inputs: [/layer2/layer2.3/Add_output_0 -> (1, 512, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer2/layer2.3/relu_2/Relu for ONNX node: /layer2/layer2.3/relu_2/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer2/layer2.3/relu_2/Relu_output_0 for ONNX tensor: /layer2/layer2.3/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer2/layer2.3/relu_2/Relu [Relu] outputs: [/layer2/layer2.3/relu_2/Relu_output_0 -> (1, 512, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.0/conv1/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer2/layer2.3/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_569
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_507
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.0/conv1/Conv [Conv] inputs: [/layer2/layer2.3/relu_2/Relu_output_0 -> (1, 512, 28, 28)[FLOAT]], [onnx::Conv_569 -> (256, 512, 1, 1)[FLOAT]], [onnx::Conv_507 -> (256)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 512, 28, 28)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.0/conv1/Conv for ONNX node: /layer3/layer3.0/conv1/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 256
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 256, 28, 28)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.0/conv1/Conv_output_0 for ONNX tensor: /layer3/layer3.0/conv1/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.0/conv1/Conv [Conv] outputs: [/layer3/layer3.0/conv1/Conv_output_0 -> (1, 256, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.0/relu/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.0/conv1/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.0/relu/Relu [Relu] inputs: [/layer3/layer3.0/conv1/Conv_output_0 -> (1, 256, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.0/relu/Relu for ONNX node: /layer3/layer3.0/relu/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.0/relu/Relu_output_0 for ONNX tensor: /layer3/layer3.0/relu/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.0/relu/Relu [Relu] outputs: [/layer3/layer3.0/relu/Relu_output_0 -> (1, 256, 28, 28)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.0/conv2/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.0/relu/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_572
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_507
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.0/conv2/Conv [Conv] inputs: [/layer3/layer3.0/relu/Relu_output_0 -> (1, 256, 28, 28)[FLOAT]], [onnx::Conv_572 -> (256, 256, 3, 3)[FLOAT]], [onnx::Conv_507 -> (256)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 256, 28, 28)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.0/conv2/Conv for ONNX node: /layer3/layer3.0/conv2/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (3, 3), strides: (2, 2), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 256
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 256, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.0/conv2/Conv_output_0 for ONNX tensor: /layer3/layer3.0/conv2/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.0/conv2/Conv [Conv] outputs: [/layer3/layer3.0/conv2/Conv_output_0 -> (1, 256, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.0/relu_1/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.0/conv2/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.0/relu_1/Relu [Relu] inputs: [/layer3/layer3.0/conv2/Conv_output_0 -> (1, 256, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.0/relu_1/Relu for ONNX node: /layer3/layer3.0/relu_1/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.0/relu_1/Relu_output_0 for ONNX tensor: /layer3/layer3.0/relu_1/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.0/relu_1/Relu [Relu] outputs: [/layer3/layer3.0/relu_1/Relu_output_0 -> (1, 256, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.0/conv3/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.0/relu_1/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_575
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_576
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.0/conv3/Conv [Conv] inputs: [/layer3/layer3.0/relu_1/Relu_output_0 -> (1, 256, 14, 14)[FLOAT]], [onnx::Conv_575 -> (1024, 256, 1, 1)[FLOAT]], [onnx::Conv_576 -> (1024)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 256, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.0/conv3/Conv for ONNX node: /layer3/layer3.0/conv3/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 1024
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 1024, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.0/conv3/Conv_output_0 for ONNX tensor: /layer3/layer3.0/conv3/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.0/conv3/Conv [Conv] outputs: [/layer3/layer3.0/conv3/Conv_output_0 -> (1, 1024, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.0/downsample/downsample.0/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer2/layer2.3/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_578
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_576
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv [Conv] inputs: [/layer2/layer2.3/relu_2/Relu_output_0 -> (1, 512, 28, 28)[FLOAT]], [onnx::Conv_578 -> (1024, 512, 1, 1)[FLOAT]], [onnx::Conv_576 -> (1024)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 512, 28, 28)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.0/downsample/downsample.0/Conv for ONNX node: /layer3/layer3.0/downsample/downsample.0/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (1, 1), strides: (2, 2), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 1024
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 1024, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.0/downsample/downsample.0/Conv_output_0 for ONNX tensor: /layer3/layer3.0/downsample/downsample.0/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv [Conv] outputs: [/layer3/layer3.0/downsample/downsample.0/Conv_output_0 -> (1, 1024, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.0/Add [Add]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.0/conv3/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.0/downsample/downsample.0/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.0/Add [Add] inputs: [/layer3/layer3.0/conv3/Conv_output_0 -> (1, 1024, 14, 14)[FLOAT]], [/layer3/layer3.0/downsample/downsample.0/Conv_output_0 -> (1, 1024, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.0/Add for ONNX node: /layer3/layer3.0/Add
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.0/Add_output_0 for ONNX tensor: /layer3/layer3.0/Add_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.0/Add [Add] outputs: [/layer3/layer3.0/Add_output_0 -> (1, 1024, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.0/relu_2/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.0/Add_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.0/relu_2/Relu [Relu] inputs: [/layer3/layer3.0/Add_output_0 -> (1, 1024, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.0/relu_2/Relu for ONNX node: /layer3/layer3.0/relu_2/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.0/relu_2/Relu_output_0 for ONNX tensor: /layer3/layer3.0/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.0/relu_2/Relu [Relu] outputs: [/layer3/layer3.0/relu_2/Relu_output_0 -> (1, 1024, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.1/conv1/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.0/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_581
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_507
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.1/conv1/Conv [Conv] inputs: [/layer3/layer3.0/relu_2/Relu_output_0 -> (1, 1024, 14, 14)[FLOAT]], [onnx::Conv_581 -> (256, 1024, 1, 1)[FLOAT]], [onnx::Conv_507 -> (256)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 1024, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.1/conv1/Conv for ONNX node: /layer3/layer3.1/conv1/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 256
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 256, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.1/conv1/Conv_output_0 for ONNX tensor: /layer3/layer3.1/conv1/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.1/conv1/Conv [Conv] outputs: [/layer3/layer3.1/conv1/Conv_output_0 -> (1, 256, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.1/relu/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.1/conv1/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.1/relu/Relu [Relu] inputs: [/layer3/layer3.1/conv1/Conv_output_0 -> (1, 256, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.1/relu/Relu for ONNX node: /layer3/layer3.1/relu/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.1/relu/Relu_output_0 for ONNX tensor: /layer3/layer3.1/relu/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.1/relu/Relu [Relu] outputs: [/layer3/layer3.1/relu/Relu_output_0 -> (1, 256, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.1/conv2/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.1/relu/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_584
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_507
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.1/conv2/Conv [Conv] inputs: [/layer3/layer3.1/relu/Relu_output_0 -> (1, 256, 14, 14)[FLOAT]], [onnx::Conv_584 -> (256, 256, 3, 3)[FLOAT]], [onnx::Conv_507 -> (256)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 256, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.1/conv2/Conv for ONNX node: /layer3/layer3.1/conv2/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 256
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 256, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.1/conv2/Conv_output_0 for ONNX tensor: /layer3/layer3.1/conv2/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.1/conv2/Conv [Conv] outputs: [/layer3/layer3.1/conv2/Conv_output_0 -> (1, 256, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.1/relu_1/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.1/conv2/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.1/relu_1/Relu [Relu] inputs: [/layer3/layer3.1/conv2/Conv_output_0 -> (1, 256, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.1/relu_1/Relu for ONNX node: /layer3/layer3.1/relu_1/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.1/relu_1/Relu_output_0 for ONNX tensor: /layer3/layer3.1/relu_1/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.1/relu_1/Relu [Relu] outputs: [/layer3/layer3.1/relu_1/Relu_output_0 -> (1, 256, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.1/conv3/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.1/relu_1/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_587
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_576
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.1/conv3/Conv [Conv] inputs: [/layer3/layer3.1/relu_1/Relu_output_0 -> (1, 256, 14, 14)[FLOAT]], [onnx::Conv_587 -> (1024, 256, 1, 1)[FLOAT]], [onnx::Conv_576 -> (1024)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 256, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.1/conv3/Conv for ONNX node: /layer3/layer3.1/conv3/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 1024
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 1024, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.1/conv3/Conv_output_0 for ONNX tensor: /layer3/layer3.1/conv3/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.1/conv3/Conv [Conv] outputs: [/layer3/layer3.1/conv3/Conv_output_0 -> (1, 1024, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.1/Add [Add]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.1/conv3/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.0/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.1/Add [Add] inputs: [/layer3/layer3.1/conv3/Conv_output_0 -> (1, 1024, 14, 14)[FLOAT]], [/layer3/layer3.0/relu_2/Relu_output_0 -> (1, 1024, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.1/Add for ONNX node: /layer3/layer3.1/Add
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.1/Add_output_0 for ONNX tensor: /layer3/layer3.1/Add_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.1/Add [Add] outputs: [/layer3/layer3.1/Add_output_0 -> (1, 1024, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.1/relu_2/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.1/Add_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.1/relu_2/Relu [Relu] inputs: [/layer3/layer3.1/Add_output_0 -> (1, 1024, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.1/relu_2/Relu for ONNX node: /layer3/layer3.1/relu_2/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.1/relu_2/Relu_output_0 for ONNX tensor: /layer3/layer3.1/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.1/relu_2/Relu [Relu] outputs: [/layer3/layer3.1/relu_2/Relu_output_0 -> (1, 1024, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.2/conv1/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.1/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_590
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_507
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.2/conv1/Conv [Conv] inputs: [/layer3/layer3.1/relu_2/Relu_output_0 -> (1, 1024, 14, 14)[FLOAT]], [onnx::Conv_590 -> (256, 1024, 1, 1)[FLOAT]], [onnx::Conv_507 -> (256)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 1024, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.2/conv1/Conv for ONNX node: /layer3/layer3.2/conv1/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 256
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 256, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.2/conv1/Conv_output_0 for ONNX tensor: /layer3/layer3.2/conv1/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.2/conv1/Conv [Conv] outputs: [/layer3/layer3.2/conv1/Conv_output_0 -> (1, 256, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.2/relu/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.2/conv1/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.2/relu/Relu [Relu] inputs: [/layer3/layer3.2/conv1/Conv_output_0 -> (1, 256, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.2/relu/Relu for ONNX node: /layer3/layer3.2/relu/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.2/relu/Relu_output_0 for ONNX tensor: /layer3/layer3.2/relu/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.2/relu/Relu [Relu] outputs: [/layer3/layer3.2/relu/Relu_output_0 -> (1, 256, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.2/conv2/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.2/relu/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_593
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_507
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.2/conv2/Conv [Conv] inputs: [/layer3/layer3.2/relu/Relu_output_0 -> (1, 256, 14, 14)[FLOAT]], [onnx::Conv_593 -> (256, 256, 3, 3)[FLOAT]], [onnx::Conv_507 -> (256)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 256, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.2/conv2/Conv for ONNX node: /layer3/layer3.2/conv2/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 256
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 256, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.2/conv2/Conv_output_0 for ONNX tensor: /layer3/layer3.2/conv2/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.2/conv2/Conv [Conv] outputs: [/layer3/layer3.2/conv2/Conv_output_0 -> (1, 256, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.2/relu_1/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.2/conv2/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.2/relu_1/Relu [Relu] inputs: [/layer3/layer3.2/conv2/Conv_output_0 -> (1, 256, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.2/relu_1/Relu for ONNX node: /layer3/layer3.2/relu_1/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.2/relu_1/Relu_output_0 for ONNX tensor: /layer3/layer3.2/relu_1/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.2/relu_1/Relu [Relu] outputs: [/layer3/layer3.2/relu_1/Relu_output_0 -> (1, 256, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.2/conv3/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.2/relu_1/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_596
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_576
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.2/conv3/Conv [Conv] inputs: [/layer3/layer3.2/relu_1/Relu_output_0 -> (1, 256, 14, 14)[FLOAT]], [onnx::Conv_596 -> (1024, 256, 1, 1)[FLOAT]], [onnx::Conv_576 -> (1024)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 256, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.2/conv3/Conv for ONNX node: /layer3/layer3.2/conv3/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 1024
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 1024, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.2/conv3/Conv_output_0 for ONNX tensor: /layer3/layer3.2/conv3/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.2/conv3/Conv [Conv] outputs: [/layer3/layer3.2/conv3/Conv_output_0 -> (1, 1024, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.2/Add [Add]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.2/conv3/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.1/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.2/Add [Add] inputs: [/layer3/layer3.2/conv3/Conv_output_0 -> (1, 1024, 14, 14)[FLOAT]], [/layer3/layer3.1/relu_2/Relu_output_0 -> (1, 1024, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.2/Add for ONNX node: /layer3/layer3.2/Add
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.2/Add_output_0 for ONNX tensor: /layer3/layer3.2/Add_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.2/Add [Add] outputs: [/layer3/layer3.2/Add_output_0 -> (1, 1024, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.2/relu_2/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.2/Add_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.2/relu_2/Relu [Relu] inputs: [/layer3/layer3.2/Add_output_0 -> (1, 1024, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.2/relu_2/Relu for ONNX node: /layer3/layer3.2/relu_2/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.2/relu_2/Relu_output_0 for ONNX tensor: /layer3/layer3.2/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.2/relu_2/Relu [Relu] outputs: [/layer3/layer3.2/relu_2/Relu_output_0 -> (1, 1024, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.3/conv1/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.2/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_599
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_507
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.3/conv1/Conv [Conv] inputs: [/layer3/layer3.2/relu_2/Relu_output_0 -> (1, 1024, 14, 14)[FLOAT]], [onnx::Conv_599 -> (256, 1024, 1, 1)[FLOAT]], [onnx::Conv_507 -> (256)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 1024, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.3/conv1/Conv for ONNX node: /layer3/layer3.3/conv1/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 256
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 256, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.3/conv1/Conv_output_0 for ONNX tensor: /layer3/layer3.3/conv1/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.3/conv1/Conv [Conv] outputs: [/layer3/layer3.3/conv1/Conv_output_0 -> (1, 256, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.3/relu/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.3/conv1/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.3/relu/Relu [Relu] inputs: [/layer3/layer3.3/conv1/Conv_output_0 -> (1, 256, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.3/relu/Relu for ONNX node: /layer3/layer3.3/relu/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.3/relu/Relu_output_0 for ONNX tensor: /layer3/layer3.3/relu/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.3/relu/Relu [Relu] outputs: [/layer3/layer3.3/relu/Relu_output_0 -> (1, 256, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.3/conv2/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.3/relu/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_602
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_507
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.3/conv2/Conv [Conv] inputs: [/layer3/layer3.3/relu/Relu_output_0 -> (1, 256, 14, 14)[FLOAT]], [onnx::Conv_602 -> (256, 256, 3, 3)[FLOAT]], [onnx::Conv_507 -> (256)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 256, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.3/conv2/Conv for ONNX node: /layer3/layer3.3/conv2/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 256
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 256, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.3/conv2/Conv_output_0 for ONNX tensor: /layer3/layer3.3/conv2/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.3/conv2/Conv [Conv] outputs: [/layer3/layer3.3/conv2/Conv_output_0 -> (1, 256, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.3/relu_1/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.3/conv2/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.3/relu_1/Relu [Relu] inputs: [/layer3/layer3.3/conv2/Conv_output_0 -> (1, 256, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.3/relu_1/Relu for ONNX node: /layer3/layer3.3/relu_1/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.3/relu_1/Relu_output_0 for ONNX tensor: /layer3/layer3.3/relu_1/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.3/relu_1/Relu [Relu] outputs: [/layer3/layer3.3/relu_1/Relu_output_0 -> (1, 256, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.3/conv3/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.3/relu_1/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_605
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_576
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.3/conv3/Conv [Conv] inputs: [/layer3/layer3.3/relu_1/Relu_output_0 -> (1, 256, 14, 14)[FLOAT]], [onnx::Conv_605 -> (1024, 256, 1, 1)[FLOAT]], [onnx::Conv_576 -> (1024)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 256, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.3/conv3/Conv for ONNX node: /layer3/layer3.3/conv3/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 1024
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 1024, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.3/conv3/Conv_output_0 for ONNX tensor: /layer3/layer3.3/conv3/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.3/conv3/Conv [Conv] outputs: [/layer3/layer3.3/conv3/Conv_output_0 -> (1, 1024, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.3/Add [Add]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.3/conv3/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.2/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.3/Add [Add] inputs: [/layer3/layer3.3/conv3/Conv_output_0 -> (1, 1024, 14, 14)[FLOAT]], [/layer3/layer3.2/relu_2/Relu_output_0 -> (1, 1024, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.3/Add for ONNX node: /layer3/layer3.3/Add
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.3/Add_output_0 for ONNX tensor: /layer3/layer3.3/Add_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.3/Add [Add] outputs: [/layer3/layer3.3/Add_output_0 -> (1, 1024, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.3/relu_2/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.3/Add_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.3/relu_2/Relu [Relu] inputs: [/layer3/layer3.3/Add_output_0 -> (1, 1024, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.3/relu_2/Relu for ONNX node: /layer3/layer3.3/relu_2/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.3/relu_2/Relu_output_0 for ONNX tensor: /layer3/layer3.3/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.3/relu_2/Relu [Relu] outputs: [/layer3/layer3.3/relu_2/Relu_output_0 -> (1, 1024, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.4/conv1/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.3/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_608
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_507
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.4/conv1/Conv [Conv] inputs: [/layer3/layer3.3/relu_2/Relu_output_0 -> (1, 1024, 14, 14)[FLOAT]], [onnx::Conv_608 -> (256, 1024, 1, 1)[FLOAT]], [onnx::Conv_507 -> (256)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 1024, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.4/conv1/Conv for ONNX node: /layer3/layer3.4/conv1/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 256
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 256, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.4/conv1/Conv_output_0 for ONNX tensor: /layer3/layer3.4/conv1/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.4/conv1/Conv [Conv] outputs: [/layer3/layer3.4/conv1/Conv_output_0 -> (1, 256, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.4/relu/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.4/conv1/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.4/relu/Relu [Relu] inputs: [/layer3/layer3.4/conv1/Conv_output_0 -> (1, 256, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.4/relu/Relu for ONNX node: /layer3/layer3.4/relu/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.4/relu/Relu_output_0 for ONNX tensor: /layer3/layer3.4/relu/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.4/relu/Relu [Relu] outputs: [/layer3/layer3.4/relu/Relu_output_0 -> (1, 256, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.4/conv2/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.4/relu/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_611
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_507
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.4/conv2/Conv [Conv] inputs: [/layer3/layer3.4/relu/Relu_output_0 -> (1, 256, 14, 14)[FLOAT]], [onnx::Conv_611 -> (256, 256, 3, 3)[FLOAT]], [onnx::Conv_507 -> (256)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 256, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.4/conv2/Conv for ONNX node: /layer3/layer3.4/conv2/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 256
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 256, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.4/conv2/Conv_output_0 for ONNX tensor: /layer3/layer3.4/conv2/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.4/conv2/Conv [Conv] outputs: [/layer3/layer3.4/conv2/Conv_output_0 -> (1, 256, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.4/relu_1/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.4/conv2/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.4/relu_1/Relu [Relu] inputs: [/layer3/layer3.4/conv2/Conv_output_0 -> (1, 256, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.4/relu_1/Relu for ONNX node: /layer3/layer3.4/relu_1/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.4/relu_1/Relu_output_0 for ONNX tensor: /layer3/layer3.4/relu_1/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.4/relu_1/Relu [Relu] outputs: [/layer3/layer3.4/relu_1/Relu_output_0 -> (1, 256, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.4/conv3/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.4/relu_1/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_614
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_576
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.4/conv3/Conv [Conv] inputs: [/layer3/layer3.4/relu_1/Relu_output_0 -> (1, 256, 14, 14)[FLOAT]], [onnx::Conv_614 -> (1024, 256, 1, 1)[FLOAT]], [onnx::Conv_576 -> (1024)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 256, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.4/conv3/Conv for ONNX node: /layer3/layer3.4/conv3/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 1024
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 1024, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.4/conv3/Conv_output_0 for ONNX tensor: /layer3/layer3.4/conv3/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.4/conv3/Conv [Conv] outputs: [/layer3/layer3.4/conv3/Conv_output_0 -> (1, 1024, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.4/Add [Add]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.4/conv3/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.3/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.4/Add [Add] inputs: [/layer3/layer3.4/conv3/Conv_output_0 -> (1, 1024, 14, 14)[FLOAT]], [/layer3/layer3.3/relu_2/Relu_output_0 -> (1, 1024, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.4/Add for ONNX node: /layer3/layer3.4/Add
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.4/Add_output_0 for ONNX tensor: /layer3/layer3.4/Add_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.4/Add [Add] outputs: [/layer3/layer3.4/Add_output_0 -> (1, 1024, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.4/relu_2/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.4/Add_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.4/relu_2/Relu [Relu] inputs: [/layer3/layer3.4/Add_output_0 -> (1, 1024, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.4/relu_2/Relu for ONNX node: /layer3/layer3.4/relu_2/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.4/relu_2/Relu_output_0 for ONNX tensor: /layer3/layer3.4/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.4/relu_2/Relu [Relu] outputs: [/layer3/layer3.4/relu_2/Relu_output_0 -> (1, 1024, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.5/conv1/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.4/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_617
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_507
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.5/conv1/Conv [Conv] inputs: [/layer3/layer3.4/relu_2/Relu_output_0 -> (1, 1024, 14, 14)[FLOAT]], [onnx::Conv_617 -> (256, 1024, 1, 1)[FLOAT]], [onnx::Conv_507 -> (256)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 1024, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.5/conv1/Conv for ONNX node: /layer3/layer3.5/conv1/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 256
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 256, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.5/conv1/Conv_output_0 for ONNX tensor: /layer3/layer3.5/conv1/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.5/conv1/Conv [Conv] outputs: [/layer3/layer3.5/conv1/Conv_output_0 -> (1, 256, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.5/relu/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.5/conv1/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.5/relu/Relu [Relu] inputs: [/layer3/layer3.5/conv1/Conv_output_0 -> (1, 256, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.5/relu/Relu for ONNX node: /layer3/layer3.5/relu/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.5/relu/Relu_output_0 for ONNX tensor: /layer3/layer3.5/relu/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.5/relu/Relu [Relu] outputs: [/layer3/layer3.5/relu/Relu_output_0 -> (1, 256, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.5/conv2/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.5/relu/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_620
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_507
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.5/conv2/Conv [Conv] inputs: [/layer3/layer3.5/relu/Relu_output_0 -> (1, 256, 14, 14)[FLOAT]], [onnx::Conv_620 -> (256, 256, 3, 3)[FLOAT]], [onnx::Conv_507 -> (256)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 256, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.5/conv2/Conv for ONNX node: /layer3/layer3.5/conv2/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 256
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 256, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.5/conv2/Conv_output_0 for ONNX tensor: /layer3/layer3.5/conv2/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.5/conv2/Conv [Conv] outputs: [/layer3/layer3.5/conv2/Conv_output_0 -> (1, 256, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.5/relu_1/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.5/conv2/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.5/relu_1/Relu [Relu] inputs: [/layer3/layer3.5/conv2/Conv_output_0 -> (1, 256, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.5/relu_1/Relu for ONNX node: /layer3/layer3.5/relu_1/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.5/relu_1/Relu_output_0 for ONNX tensor: /layer3/layer3.5/relu_1/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.5/relu_1/Relu [Relu] outputs: [/layer3/layer3.5/relu_1/Relu_output_0 -> (1, 256, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.5/conv3/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.5/relu_1/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_623
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_576
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.5/conv3/Conv [Conv] inputs: [/layer3/layer3.5/relu_1/Relu_output_0 -> (1, 256, 14, 14)[FLOAT]], [onnx::Conv_623 -> (1024, 256, 1, 1)[FLOAT]], [onnx::Conv_576 -> (1024)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 256, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.5/conv3/Conv for ONNX node: /layer3/layer3.5/conv3/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 1024
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 1024, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.5/conv3/Conv_output_0 for ONNX tensor: /layer3/layer3.5/conv3/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.5/conv3/Conv [Conv] outputs: [/layer3/layer3.5/conv3/Conv_output_0 -> (1, 1024, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.5/Add [Add]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.5/conv3/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.4/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.5/Add [Add] inputs: [/layer3/layer3.5/conv3/Conv_output_0 -> (1, 1024, 14, 14)[FLOAT]], [/layer3/layer3.4/relu_2/Relu_output_0 -> (1, 1024, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.5/Add for ONNX node: /layer3/layer3.5/Add
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.5/Add_output_0 for ONNX tensor: /layer3/layer3.5/Add_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.5/Add [Add] outputs: [/layer3/layer3.5/Add_output_0 -> (1, 1024, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer3/layer3.5/relu_2/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.5/Add_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.5/relu_2/Relu [Relu] inputs: [/layer3/layer3.5/Add_output_0 -> (1, 1024, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer3/layer3.5/relu_2/Relu for ONNX node: /layer3/layer3.5/relu_2/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer3/layer3.5/relu_2/Relu_output_0 for ONNX tensor: /layer3/layer3.5/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer3/layer3.5/relu_2/Relu [Relu] outputs: [/layer3/layer3.5/relu_2/Relu_output_0 -> (1, 1024, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer4/layer4.0/conv1/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.5/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_626
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_537
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.0/conv1/Conv [Conv] inputs: [/layer3/layer3.5/relu_2/Relu_output_0 -> (1, 1024, 14, 14)[FLOAT]], [onnx::Conv_626 -> (512, 1024, 1, 1)[FLOAT]], [onnx::Conv_537 -> (512)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 1024, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer4/layer4.0/conv1/Conv for ONNX node: /layer4/layer4.0/conv1/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 512
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 512, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer4/layer4.0/conv1/Conv_output_0 for ONNX tensor: /layer4/layer4.0/conv1/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.0/conv1/Conv [Conv] outputs: [/layer4/layer4.0/conv1/Conv_output_0 -> (1, 512, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer4/layer4.0/relu/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer4/layer4.0/conv1/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.0/relu/Relu [Relu] inputs: [/layer4/layer4.0/conv1/Conv_output_0 -> (1, 512, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer4/layer4.0/relu/Relu for ONNX node: /layer4/layer4.0/relu/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer4/layer4.0/relu/Relu_output_0 for ONNX tensor: /layer4/layer4.0/relu/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.0/relu/Relu [Relu] outputs: [/layer4/layer4.0/relu/Relu_output_0 -> (1, 512, 14, 14)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer4/layer4.0/conv2/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer4/layer4.0/relu/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_629
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_537
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.0/conv2/Conv [Conv] inputs: [/layer4/layer4.0/relu/Relu_output_0 -> (1, 512, 14, 14)[FLOAT]], [onnx::Conv_629 -> (512, 512, 3, 3)[FLOAT]], [onnx::Conv_537 -> (512)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 512, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer4/layer4.0/conv2/Conv for ONNX node: /layer4/layer4.0/conv2/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (3, 3), strides: (2, 2), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 512
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 512, 7, 7)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer4/layer4.0/conv2/Conv_output_0 for ONNX tensor: /layer4/layer4.0/conv2/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.0/conv2/Conv [Conv] outputs: [/layer4/layer4.0/conv2/Conv_output_0 -> (1, 512, 7, 7)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer4/layer4.0/relu_1/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer4/layer4.0/conv2/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.0/relu_1/Relu [Relu] inputs: [/layer4/layer4.0/conv2/Conv_output_0 -> (1, 512, 7, 7)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer4/layer4.0/relu_1/Relu for ONNX node: /layer4/layer4.0/relu_1/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer4/layer4.0/relu_1/Relu_output_0 for ONNX tensor: /layer4/layer4.0/relu_1/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.0/relu_1/Relu [Relu] outputs: [/layer4/layer4.0/relu_1/Relu_output_0 -> (1, 512, 7, 7)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer4/layer4.0/conv3/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer4/layer4.0/relu_1/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_632
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_633
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.0/conv3/Conv [Conv] inputs: [/layer4/layer4.0/relu_1/Relu_output_0 -> (1, 512, 7, 7)[FLOAT]], [onnx::Conv_632 -> (2048, 512, 1, 1)[FLOAT]], [onnx::Conv_633 -> (2048)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 512, 7, 7)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer4/layer4.0/conv3/Conv for ONNX node: /layer4/layer4.0/conv3/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 2048
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 2048, 7, 7)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer4/layer4.0/conv3/Conv_output_0 for ONNX tensor: /layer4/layer4.0/conv3/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.0/conv3/Conv [Conv] outputs: [/layer4/layer4.0/conv3/Conv_output_0 -> (1, 2048, 7, 7)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer4/layer4.0/downsample/downsample.0/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer3/layer3.5/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_635
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_633
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv [Conv] inputs: [/layer3/layer3.5/relu_2/Relu_output_0 -> (1, 1024, 14, 14)[FLOAT]], [onnx::Conv_635 -> (2048, 1024, 1, 1)[FLOAT]], [onnx::Conv_633 -> (2048)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 1024, 14, 14)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer4/layer4.0/downsample/downsample.0/Conv for ONNX node: /layer4/layer4.0/downsample/downsample.0/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (1, 1), strides: (2, 2), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 2048
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 2048, 7, 7)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer4/layer4.0/downsample/downsample.0/Conv_output_0 for ONNX tensor: /layer4/layer4.0/downsample/downsample.0/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv [Conv] outputs: [/layer4/layer4.0/downsample/downsample.0/Conv_output_0 -> (1, 2048, 7, 7)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer4/layer4.0/Add [Add]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer4/layer4.0/conv3/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer4/layer4.0/downsample/downsample.0/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.0/Add [Add] inputs: [/layer4/layer4.0/conv3/Conv_output_0 -> (1, 2048, 7, 7)[FLOAT]], [/layer4/layer4.0/downsample/downsample.0/Conv_output_0 -> (1, 2048, 7, 7)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer4/layer4.0/Add for ONNX node: /layer4/layer4.0/Add
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer4/layer4.0/Add_output_0 for ONNX tensor: /layer4/layer4.0/Add_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.0/Add [Add] outputs: [/layer4/layer4.0/Add_output_0 -> (1, 2048, 7, 7)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer4/layer4.0/relu_2/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer4/layer4.0/Add_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.0/relu_2/Relu [Relu] inputs: [/layer4/layer4.0/Add_output_0 -> (1, 2048, 7, 7)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer4/layer4.0/relu_2/Relu for ONNX node: /layer4/layer4.0/relu_2/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer4/layer4.0/relu_2/Relu_output_0 for ONNX tensor: /layer4/layer4.0/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.0/relu_2/Relu [Relu] outputs: [/layer4/layer4.0/relu_2/Relu_output_0 -> (1, 2048, 7, 7)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer4/layer4.1/conv1/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer4/layer4.0/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_638
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_537
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.1/conv1/Conv [Conv] inputs: [/layer4/layer4.0/relu_2/Relu_output_0 -> (1, 2048, 7, 7)[FLOAT]], [onnx::Conv_638 -> (512, 2048, 1, 1)[FLOAT]], [onnx::Conv_537 -> (512)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 2048, 7, 7)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer4/layer4.1/conv1/Conv for ONNX node: /layer4/layer4.1/conv1/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 512
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 512, 7, 7)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer4/layer4.1/conv1/Conv_output_0 for ONNX tensor: /layer4/layer4.1/conv1/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.1/conv1/Conv [Conv] outputs: [/layer4/layer4.1/conv1/Conv_output_0 -> (1, 512, 7, 7)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer4/layer4.1/relu/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer4/layer4.1/conv1/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.1/relu/Relu [Relu] inputs: [/layer4/layer4.1/conv1/Conv_output_0 -> (1, 512, 7, 7)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer4/layer4.1/relu/Relu for ONNX node: /layer4/layer4.1/relu/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer4/layer4.1/relu/Relu_output_0 for ONNX tensor: /layer4/layer4.1/relu/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.1/relu/Relu [Relu] outputs: [/layer4/layer4.1/relu/Relu_output_0 -> (1, 512, 7, 7)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer4/layer4.1/conv2/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer4/layer4.1/relu/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_641
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_537
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.1/conv2/Conv [Conv] inputs: [/layer4/layer4.1/relu/Relu_output_0 -> (1, 512, 7, 7)[FLOAT]], [onnx::Conv_641 -> (512, 512, 3, 3)[FLOAT]], [onnx::Conv_537 -> (512)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 512, 7, 7)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer4/layer4.1/conv2/Conv for ONNX node: /layer4/layer4.1/conv2/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 512
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 512, 7, 7)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer4/layer4.1/conv2/Conv_output_0 for ONNX tensor: /layer4/layer4.1/conv2/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.1/conv2/Conv [Conv] outputs: [/layer4/layer4.1/conv2/Conv_output_0 -> (1, 512, 7, 7)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer4/layer4.1/relu_1/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer4/layer4.1/conv2/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.1/relu_1/Relu [Relu] inputs: [/layer4/layer4.1/conv2/Conv_output_0 -> (1, 512, 7, 7)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer4/layer4.1/relu_1/Relu for ONNX node: /layer4/layer4.1/relu_1/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer4/layer4.1/relu_1/Relu_output_0 for ONNX tensor: /layer4/layer4.1/relu_1/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.1/relu_1/Relu [Relu] outputs: [/layer4/layer4.1/relu_1/Relu_output_0 -> (1, 512, 7, 7)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer4/layer4.1/conv3/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer4/layer4.1/relu_1/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_644
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_633
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.1/conv3/Conv [Conv] inputs: [/layer4/layer4.1/relu_1/Relu_output_0 -> (1, 512, 7, 7)[FLOAT]], [onnx::Conv_644 -> (2048, 512, 1, 1)[FLOAT]], [onnx::Conv_633 -> (2048)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 512, 7, 7)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer4/layer4.1/conv3/Conv for ONNX node: /layer4/layer4.1/conv3/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 2048
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 2048, 7, 7)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer4/layer4.1/conv3/Conv_output_0 for ONNX tensor: /layer4/layer4.1/conv3/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.1/conv3/Conv [Conv] outputs: [/layer4/layer4.1/conv3/Conv_output_0 -> (1, 2048, 7, 7)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer4/layer4.1/Add [Add]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer4/layer4.1/conv3/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer4/layer4.0/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.1/Add [Add] inputs: [/layer4/layer4.1/conv3/Conv_output_0 -> (1, 2048, 7, 7)[FLOAT]], [/layer4/layer4.0/relu_2/Relu_output_0 -> (1, 2048, 7, 7)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer4/layer4.1/Add for ONNX node: /layer4/layer4.1/Add
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer4/layer4.1/Add_output_0 for ONNX tensor: /layer4/layer4.1/Add_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.1/Add [Add] outputs: [/layer4/layer4.1/Add_output_0 -> (1, 2048, 7, 7)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer4/layer4.1/relu_2/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer4/layer4.1/Add_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.1/relu_2/Relu [Relu] inputs: [/layer4/layer4.1/Add_output_0 -> (1, 2048, 7, 7)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer4/layer4.1/relu_2/Relu for ONNX node: /layer4/layer4.1/relu_2/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer4/layer4.1/relu_2/Relu_output_0 for ONNX tensor: /layer4/layer4.1/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.1/relu_2/Relu [Relu] outputs: [/layer4/layer4.1/relu_2/Relu_output_0 -> (1, 2048, 7, 7)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer4/layer4.2/conv1/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer4/layer4.1/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_647
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_537
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.2/conv1/Conv [Conv] inputs: [/layer4/layer4.1/relu_2/Relu_output_0 -> (1, 2048, 7, 7)[FLOAT]], [onnx::Conv_647 -> (512, 2048, 1, 1)[FLOAT]], [onnx::Conv_537 -> (512)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 2048, 7, 7)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer4/layer4.2/conv1/Conv for ONNX node: /layer4/layer4.2/conv1/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 512
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 512, 7, 7)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer4/layer4.2/conv1/Conv_output_0 for ONNX tensor: /layer4/layer4.2/conv1/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.2/conv1/Conv [Conv] outputs: [/layer4/layer4.2/conv1/Conv_output_0 -> (1, 512, 7, 7)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer4/layer4.2/relu/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer4/layer4.2/conv1/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.2/relu/Relu [Relu] inputs: [/layer4/layer4.2/conv1/Conv_output_0 -> (1, 512, 7, 7)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer4/layer4.2/relu/Relu for ONNX node: /layer4/layer4.2/relu/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer4/layer4.2/relu/Relu_output_0 for ONNX tensor: /layer4/layer4.2/relu/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.2/relu/Relu [Relu] outputs: [/layer4/layer4.2/relu/Relu_output_0 -> (1, 512, 7, 7)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer4/layer4.2/conv2/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer4/layer4.2/relu/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_650
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_537
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.2/conv2/Conv [Conv] inputs: [/layer4/layer4.2/relu/Relu_output_0 -> (1, 512, 7, 7)[FLOAT]], [onnx::Conv_650 -> (512, 512, 3, 3)[FLOAT]], [onnx::Conv_537 -> (512)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 512, 7, 7)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer4/layer4.2/conv2/Conv for ONNX node: /layer4/layer4.2/conv2/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 512
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 512, 7, 7)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer4/layer4.2/conv2/Conv_output_0 for ONNX tensor: /layer4/layer4.2/conv2/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.2/conv2/Conv [Conv] outputs: [/layer4/layer4.2/conv2/Conv_output_0 -> (1, 512, 7, 7)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer4/layer4.2/relu_1/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer4/layer4.2/conv2/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.2/relu_1/Relu [Relu] inputs: [/layer4/layer4.2/conv2/Conv_output_0 -> (1, 512, 7, 7)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer4/layer4.2/relu_1/Relu for ONNX node: /layer4/layer4.2/relu_1/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer4/layer4.2/relu_1/Relu_output_0 for ONNX tensor: /layer4/layer4.2/relu_1/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.2/relu_1/Relu [Relu] outputs: [/layer4/layer4.2/relu_1/Relu_output_0 -> (1, 512, 7, 7)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer4/layer4.2/conv3/Conv [Conv]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer4/layer4.2/relu_1/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_653
[05/26/2023-16:04:35] [V] [TRT] Searching for input: onnx::Conv_633
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.2/conv3/Conv [Conv] inputs: [/layer4/layer4.2/relu_1/Relu_output_0 -> (1, 512, 7, 7)[FLOAT]], [onnx::Conv_653 -> (2048, 512, 1, 1)[FLOAT]], [onnx::Conv_633 -> (2048)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Convolution input dimensions: (1, 512, 7, 7)
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer4/layer4.2/conv3/Conv for ONNX node: /layer4/layer4.2/conv3/Conv
[05/26/2023-16:04:35] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 2048
[05/26/2023-16:04:35] [V] [TRT] Convolution output dimensions: (1, 2048, 7, 7)
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer4/layer4.2/conv3/Conv_output_0 for ONNX tensor: /layer4/layer4.2/conv3/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.2/conv3/Conv [Conv] outputs: [/layer4/layer4.2/conv3/Conv_output_0 -> (1, 2048, 7, 7)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer4/layer4.2/Add [Add]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer4/layer4.2/conv3/Conv_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer4/layer4.1/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.2/Add [Add] inputs: [/layer4/layer4.2/conv3/Conv_output_0 -> (1, 2048, 7, 7)[FLOAT]], [/layer4/layer4.1/relu_2/Relu_output_0 -> (1, 2048, 7, 7)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer4/layer4.2/Add for ONNX node: /layer4/layer4.2/Add
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer4/layer4.2/Add_output_0 for ONNX tensor: /layer4/layer4.2/Add_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.2/Add [Add] outputs: [/layer4/layer4.2/Add_output_0 -> (1, 2048, 7, 7)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /layer4/layer4.2/relu_2/Relu [Relu]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer4/layer4.2/Add_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.2/relu_2/Relu [Relu] inputs: [/layer4/layer4.2/Add_output_0 -> (1, 2048, 7, 7)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /layer4/layer4.2/relu_2/Relu for ONNX node: /layer4/layer4.2/relu_2/Relu
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /layer4/layer4.2/relu_2/Relu_output_0 for ONNX tensor: /layer4/layer4.2/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /layer4/layer4.2/relu_2/Relu [Relu] outputs: [/layer4/layer4.2/relu_2/Relu_output_0 -> (1, 2048, 7, 7)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /avgpool/GlobalAveragePool [GlobalAveragePool]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /layer4/layer4.2/relu_2/Relu_output_0
[05/26/2023-16:04:35] [V] [TRT] /avgpool/GlobalAveragePool [GlobalAveragePool] inputs: [/layer4/layer4.2/relu_2/Relu_output_0 -> (1, 2048, 7, 7)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] GlobalAveragePool operators are implemented via Reduce layers rather than Pooling layers
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /avgpool/GlobalAveragePool for ONNX node: /avgpool/GlobalAveragePool
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /avgpool/GlobalAveragePool_output_0 for ONNX tensor: /avgpool/GlobalAveragePool_output_0
[05/26/2023-16:04:35] [V] [TRT] /avgpool/GlobalAveragePool [GlobalAveragePool] outputs: [/avgpool/GlobalAveragePool_output_0 -> (1, 2048, 1, 1)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /Flatten [Flatten]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /avgpool/GlobalAveragePool_output_0
[05/26/2023-16:04:35] [V] [TRT] /Flatten [Flatten] inputs: [/avgpool/GlobalAveragePool_output_0 -> (1, 2048, 1, 1)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /Flatten for ONNX node: /Flatten
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: /Flatten_output_0 for ONNX tensor: /Flatten_output_0
[05/26/2023-16:04:35] [V] [TRT] /Flatten [Flatten] outputs: [/Flatten_output_0 -> (1, 2048)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Parsing node: /fc/Gemm [Gemm]
[05/26/2023-16:04:35] [V] [TRT] Searching for input: /Flatten_output_0
[05/26/2023-16:04:35] [V] [TRT] Searching for input: fc.weight
[05/26/2023-16:04:35] [V] [TRT] Searching for input: fc.bias
[05/26/2023-16:04:35] [V] [TRT] /fc/Gemm [Gemm] inputs: [/Flatten_output_0 -> (1, 2048)[FLOAT]], [fc.weight -> (1000, 2048)[FLOAT]], [fc.bias -> (1000)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Registering layer: fc.weight for ONNX node: fc.weight
[05/26/2023-16:04:35] [V] [TRT] Using opA: 0 opB: 1
[05/26/2023-16:04:35] [V] [TRT] Registering layer: /fc/Gemm for ONNX node: /fc/Gemm
[05/26/2023-16:04:35] [V] [TRT] Registering layer: fc.bias for ONNX node: fc.bias
[05/26/2023-16:04:35] [V] [TRT] Registering tensor: output0_0 for ONNX tensor: output0
[05/26/2023-16:04:35] [V] [TRT] /fc/Gemm [Gemm] outputs: [output0 -> (1, 1000)[FLOAT]], 
[05/26/2023-16:04:35] [V] [TRT] Marking output0_0 as output: output0
[05/26/2023-16:04:35] [I] Finish parsing network model
[05/26/2023-16:04:35] [V] [TRT] Original: 126 layers
[05/26/2023-16:04:35] [V] [TRT] After dead-layer removal: 126 layers
[05/26/2023-16:04:35] [V] [TRT] Applying generic optimizations to the graph for inference.
[05/26/2023-16:04:35] [V] [TRT] Running: ConstShuffleFusion on fc.bias
[05/26/2023-16:04:35] [V] [TRT] ConstShuffleFusion: Fusing fc.bias with (Unnamed Layer* 124) [Shuffle]
[05/26/2023-16:04:35] [V] [TRT] After Myelin optimization: 125 layers
[05/26/2023-16:04:35] [V] [TRT] Running: MatMulToConvTransform on /fc/Gemm
[05/26/2023-16:04:35] [V] [TRT] Convert layer type of /fc/Gemm from MATRIX_MULTIPLY to CONVOLUTION
[05/26/2023-16:04:35] [V] [TRT] Running: ShuffleShuffleFusion on /Flatten
[05/26/2023-16:04:35] [V] [TRT] ShuffleShuffleFusion: Fusing /Flatten with reshape_before_/fc/Gemm
[05/26/2023-16:04:35] [V] [TRT] Running: ShuffleErasure on /Flatten + reshape_before_/fc/Gemm
[05/26/2023-16:04:35] [V] [TRT] Removing /Flatten + reshape_before_/fc/Gemm
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReshapeBiasAddFusion on /fc/Gemm
[05/26/2023-16:04:35] [V] [TRT] Applying ScaleNodes fusions.
[05/26/2023-16:04:35] [V] [TRT] After scale fusion: 122 layers
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /conv1/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /conv1/Conv with /relu/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer1/layer1.0/conv1/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer1/layer1.0/conv1/Conv with /layer1/layer1.0/relu/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer1/layer1.0/conv2/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer1/layer1.0/conv2/Conv with /layer1/layer1.0/relu_1/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvEltwiseSumFusion on /layer1/layer1.0/conv3/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer1/layer1.0/conv3/Conv with /layer1/layer1.0/Add
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add with /layer1/layer1.0/relu_2/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer1/layer1.1/conv1/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer1/layer1.1/conv1/Conv with /layer1/layer1.1/relu/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer1/layer1.1/conv2/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer1/layer1.1/conv2/Conv with /layer1/layer1.1/relu_1/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvEltwiseSumFusion on /layer1/layer1.1/conv3/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer1/layer1.1/conv3/Conv with /layer1/layer1.1/Add
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer1/layer1.1/conv3/Conv + /layer1/layer1.1/Add
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer1/layer1.1/conv3/Conv + /layer1/layer1.1/Add with /layer1/layer1.1/relu_2/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer1/layer1.2/conv1/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer1/layer1.2/conv1/Conv with /layer1/layer1.2/relu/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer1/layer1.2/conv2/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer1/layer1.2/conv2/Conv with /layer1/layer1.2/relu_1/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvEltwiseSumFusion on /layer1/layer1.2/conv3/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer1/layer1.2/conv3/Conv with /layer1/layer1.2/Add
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer1/layer1.2/conv3/Conv + /layer1/layer1.2/Add
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer1/layer1.2/conv3/Conv + /layer1/layer1.2/Add with /layer1/layer1.2/relu_2/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer2/layer2.0/conv1/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer2/layer2.0/conv1/Conv with /layer2/layer2.0/relu/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer2/layer2.0/conv2/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer2/layer2.0/conv2/Conv with /layer2/layer2.0/relu_1/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvEltwiseSumFusion on /layer2/layer2.0/conv3/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer2/layer2.0/conv3/Conv with /layer2/layer2.0/Add
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add with /layer2/layer2.0/relu_2/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer2/layer2.1/conv1/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer2/layer2.1/conv1/Conv with /layer2/layer2.1/relu/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer2/layer2.1/conv2/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer2/layer2.1/conv2/Conv with /layer2/layer2.1/relu_1/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvEltwiseSumFusion on /layer2/layer2.1/conv3/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer2/layer2.1/conv3/Conv with /layer2/layer2.1/Add
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer2/layer2.1/conv3/Conv + /layer2/layer2.1/Add
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer2/layer2.1/conv3/Conv + /layer2/layer2.1/Add with /layer2/layer2.1/relu_2/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer2/layer2.2/conv1/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer2/layer2.2/conv1/Conv with /layer2/layer2.2/relu/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer2/layer2.2/conv2/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer2/layer2.2/conv2/Conv with /layer2/layer2.2/relu_1/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvEltwiseSumFusion on /layer2/layer2.2/conv3/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer2/layer2.2/conv3/Conv with /layer2/layer2.2/Add
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer2/layer2.2/conv3/Conv + /layer2/layer2.2/Add
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer2/layer2.2/conv3/Conv + /layer2/layer2.2/Add with /layer2/layer2.2/relu_2/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer2/layer2.3/conv1/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer2/layer2.3/conv1/Conv with /layer2/layer2.3/relu/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer2/layer2.3/conv2/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer2/layer2.3/conv2/Conv with /layer2/layer2.3/relu_1/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvEltwiseSumFusion on /layer2/layer2.3/conv3/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer2/layer2.3/conv3/Conv with /layer2/layer2.3/Add
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer2/layer2.3/conv3/Conv + /layer2/layer2.3/Add
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer2/layer2.3/conv3/Conv + /layer2/layer2.3/Add with /layer2/layer2.3/relu_2/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer3/layer3.0/conv1/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer3/layer3.0/conv1/Conv with /layer3/layer3.0/relu/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer3/layer3.0/conv2/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer3/layer3.0/conv2/Conv with /layer3/layer3.0/relu_1/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvEltwiseSumFusion on /layer3/layer3.0/conv3/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer3/layer3.0/conv3/Conv with /layer3/layer3.0/Add
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add with /layer3/layer3.0/relu_2/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer3/layer3.1/conv1/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer3/layer3.1/conv1/Conv with /layer3/layer3.1/relu/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer3/layer3.1/conv2/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer3/layer3.1/conv2/Conv with /layer3/layer3.1/relu_1/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvEltwiseSumFusion on /layer3/layer3.1/conv3/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer3/layer3.1/conv3/Conv with /layer3/layer3.1/Add
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer3/layer3.1/conv3/Conv + /layer3/layer3.1/Add
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer3/layer3.1/conv3/Conv + /layer3/layer3.1/Add with /layer3/layer3.1/relu_2/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer3/layer3.2/conv1/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer3/layer3.2/conv1/Conv with /layer3/layer3.2/relu/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer3/layer3.2/conv2/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer3/layer3.2/conv2/Conv with /layer3/layer3.2/relu_1/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvEltwiseSumFusion on /layer3/layer3.2/conv3/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer3/layer3.2/conv3/Conv with /layer3/layer3.2/Add
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer3/layer3.2/conv3/Conv + /layer3/layer3.2/Add
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer3/layer3.2/conv3/Conv + /layer3/layer3.2/Add with /layer3/layer3.2/relu_2/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer3/layer3.3/conv1/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer3/layer3.3/conv1/Conv with /layer3/layer3.3/relu/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer3/layer3.3/conv2/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer3/layer3.3/conv2/Conv with /layer3/layer3.3/relu_1/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvEltwiseSumFusion on /layer3/layer3.3/conv3/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer3/layer3.3/conv3/Conv with /layer3/layer3.3/Add
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer3/layer3.3/conv3/Conv + /layer3/layer3.3/Add
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer3/layer3.3/conv3/Conv + /layer3/layer3.3/Add with /layer3/layer3.3/relu_2/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer3/layer3.4/conv1/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer3/layer3.4/conv1/Conv with /layer3/layer3.4/relu/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer3/layer3.4/conv2/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer3/layer3.4/conv2/Conv with /layer3/layer3.4/relu_1/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvEltwiseSumFusion on /layer3/layer3.4/conv3/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer3/layer3.4/conv3/Conv with /layer3/layer3.4/Add
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer3/layer3.4/conv3/Conv + /layer3/layer3.4/Add
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer3/layer3.4/conv3/Conv + /layer3/layer3.4/Add with /layer3/layer3.4/relu_2/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer3/layer3.5/conv1/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer3/layer3.5/conv1/Conv with /layer3/layer3.5/relu/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer3/layer3.5/conv2/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer3/layer3.5/conv2/Conv with /layer3/layer3.5/relu_1/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvEltwiseSumFusion on /layer3/layer3.5/conv3/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer3/layer3.5/conv3/Conv with /layer3/layer3.5/Add
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer3/layer3.5/conv3/Conv + /layer3/layer3.5/Add
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer3/layer3.5/conv3/Conv + /layer3/layer3.5/Add with /layer3/layer3.5/relu_2/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer4/layer4.0/conv1/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer4/layer4.0/conv1/Conv with /layer4/layer4.0/relu/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer4/layer4.0/conv2/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer4/layer4.0/conv2/Conv with /layer4/layer4.0/relu_1/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvEltwiseSumFusion on /layer4/layer4.0/conv3/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer4/layer4.0/conv3/Conv with /layer4/layer4.0/Add
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add with /layer4/layer4.0/relu_2/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer4/layer4.1/conv1/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer4/layer4.1/conv1/Conv with /layer4/layer4.1/relu/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer4/layer4.1/conv2/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer4/layer4.1/conv2/Conv with /layer4/layer4.1/relu_1/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvEltwiseSumFusion on /layer4/layer4.1/conv3/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer4/layer4.1/conv3/Conv with /layer4/layer4.1/Add
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer4/layer4.1/conv3/Conv + /layer4/layer4.1/Add
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer4/layer4.1/conv3/Conv + /layer4/layer4.1/Add with /layer4/layer4.1/relu_2/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer4/layer4.2/conv1/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer4/layer4.2/conv1/Conv with /layer4/layer4.2/relu/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer4/layer4.2/conv2/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer4/layer4.2/conv2/Conv with /layer4/layer4.2/relu_1/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ConvEltwiseSumFusion on /layer4/layer4.2/conv3/Conv
[05/26/2023-16:04:35] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer4/layer4.2/conv3/Conv with /layer4/layer4.2/Add
[05/26/2023-16:04:35] [V] [TRT] Running: ConvReluFusion on /layer4/layer4.2/conv3/Conv + /layer4/layer4.2/Add
[05/26/2023-16:04:35] [V] [TRT] ConvReluFusion: Fusing /layer4/layer4.2/conv3/Conv + /layer4/layer4.2/Add with /layer4/layer4.2/relu_2/Relu
[05/26/2023-16:04:35] [V] [TRT] Running: ReduceToPoolingFusion on /avgpool/GlobalAveragePool
[05/26/2023-16:04:35] [V] [TRT] Swap the layer type of /avgpool/GlobalAveragePool from REDUCE to POOLING
[05/26/2023-16:04:35] [V] [TRT] After dupe layer removal: 57 layers
[05/26/2023-16:04:35] [V] [TRT] After final dead-layer removal: 57 layers
[05/26/2023-16:04:35] [V] [TRT] After tensor merging: 57 layers
[05/26/2023-16:04:35] [V] [TRT] After vertical fusions: 57 layers
[05/26/2023-16:04:35] [V] [TRT] After dupe layer removal: 57 layers
[05/26/2023-16:04:35] [V] [TRT] After final dead-layer removal: 57 layers
[05/26/2023-16:04:35] [V] [TRT] After tensor merging: 57 layers
[05/26/2023-16:04:35] [V] [TRT] After slice removal: 57 layers
[05/26/2023-16:04:35] [V] [TRT] After concat removal: 57 layers
[05/26/2023-16:04:35] [V] [TRT] Trying to split Reshape and strided tensor
[05/26/2023-16:04:35] [V] [TRT] Graph construction and optimization completed in 0.14637 seconds.
[05/26/2023-16:04:35] [V] [TRT] Trying to load shared library libcublas.so.11
[05/26/2023-16:04:35] [V] [TRT] Loaded shared library libcublas.so.11
[05/26/2023-16:04:36] [V] [TRT] Using cublas as plugin tactic source
[05/26/2023-16:04:36] [V] [TRT] Trying to load shared library libcublasLt.so.11
[05/26/2023-16:04:36] [V] [TRT] Loaded shared library libcublasLt.so.11
[05/26/2023-16:04:36] [V] [TRT] Using cublasLt as core library tactic source
[05/26/2023-16:04:36] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +850, GPU +368, now: CPU 1781, GPU 937 (MiB)
[05/26/2023-16:04:36] [V] [TRT] Trying to load shared library libcudnn.so.8
[05/26/2023-16:04:36] [V] [TRT] Loaded shared library libcudnn.so.8
[05/26/2023-16:04:36] [V] [TRT] Using cuDNN as plugin tactic source
[05/26/2023-16:04:36] [V] [TRT] Using cuDNN as core library tactic source
[05/26/2023-16:04:36] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +126, GPU +58, now: CPU 1907, GPU 995 (MiB)
[05/26/2023-16:04:36] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.
[05/26/2023-16:04:36] [V] [TRT] Constructing optimization profile number 0 [1/1].
[05/26/2023-16:04:36] [V] [TRT] Reserving memory for host IO tensors. Host: 0 bytes
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(150528,50176,224,1) -> Float(150528,1,672,3) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00414994
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00513992
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00381498
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00381498
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(150528,50176,224,1) -> Float(50176,1:4,224,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00562092
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00496962
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00567455
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.00496962
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,12544,112,1) -> Float(200704,1:4,1792,16) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/relu/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.015056
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0116273
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0151406
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.0116273
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,7168,64) -> Float(802816,12544,112,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/relu/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0161275
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0140821
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0161199
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.0140821
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,7168,64) -> Float(200704,1:4,1792,16) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/relu/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0130481
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.011757
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0129957
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.011757
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1:4,1792,16) -> Float(802816,12544,112,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/relu/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0161239
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0141494
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0161407
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.0141494
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /maxpool/MaxPool_output_0) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00487406
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00642306
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00485151
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00485151
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /maxpool/MaxPool_output_0) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0049961
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00643558
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00501565
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.0049961
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /maxpool/MaxPool_output_0) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0062241
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00699385
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00625312
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.0062241
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /maxpool/MaxPool_output_0) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00454857
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0060781
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00457443
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00454857
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0048544
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00640338
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00485867
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.0048544
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00502227
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00640914
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00502542
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00502227
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00625033
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00698863
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00628293
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00625033
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00456414
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00601276
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00456414
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00456414
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00624676
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00701932
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00624119
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00624119
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00458043
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00601371
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.004591
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00458043
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Float(802816,1,14336,256) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer1/layer1.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0193646
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0117015
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0192914
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.0117015
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Float(200704,1:4,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer1/layer1.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0194194
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0117925
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0194491
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.0117925
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Float(802816,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer1/layer1.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0173872
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0143776
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0173765
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.0143776
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Float(200704,1:4,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer1/layer1.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0130065
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0115573
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0130194
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.0115573
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1:4,3584,64) -> Float(802816,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer1/layer1.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0176193
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0143767
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0175335
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.0143767
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1:4,3584,64) -> Float(802816,1,14336,256) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer1/layer1.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0129783
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0116121
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0129684
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.0116121
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Float(802816,1,14336,256) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer1/layer1.0/relu_2/Relu_output_0) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0192377
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.011718
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0193469
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.011718
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Float(200704,1:4,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer1/layer1.0/relu_2/Relu_output_0) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0194423
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0116589
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0194377
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.0116589
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Float(802816,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer1/layer1.0/relu_2/Relu_output_0) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0174273
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0143214
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0173379
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.0143214
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Float(200704,1:4,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer1/layer1.0/relu_2/Relu_output_0) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0129585
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0116438
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0129463
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.0116438
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1:4,3584,64) -> Float(802816,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer1/layer1.0/relu_2/Relu_output_0) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.017538
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0143808
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0175543
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.0143808
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1:4,3584,64) -> Float(802816,1,14336,256) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer1/layer1.0/relu_2/Relu_output_0) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0129829
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0115341
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0129928
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.0115341
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Float(802816,1,14336,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Float(200704,1:4,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Float(802816,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Float(200704,1:4,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1:4,3584,64) -> Float(802816,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1:4,3584,64) -> Float(802816,1,14336,256) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Float(802816,1,14336,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Float(200704,1:4,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Float(802816,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Float(200704,1:4,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1:4,3584,64) -> Float(802816,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1:4,3584,64) -> Float(802816,1,14336,256) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Float(802816,1,14336,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Float(200704,1:4,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Float(802816,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Float(200704,1:4,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1:4,3584,64) -> Float(802816,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1:4,3584,64) -> Float(802816,1,14336,256) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Float(802816,1,14336,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Float(200704,1:4,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Float(802816,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Float(200704,1:4,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1:4,3584,64) -> Float(802816,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1:4,3584,64) -> Float(802816,1,14336,256) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Float(802816,1,14336,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Float(200704,1:4,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Float(802816,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Float(200704,1:4,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1:4,3584,64) -> Float(802816,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1:4,3584,64) -> Float(802816,1,14336,256) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Float(802816,1,14336,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Float(200704,1:4,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Float(802816,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Float(200704,1:4,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1:4,3584,64) -> Float(802816,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1:4,3584,64) -> Float(802816,1,14336,256) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Float(802816,1,14336,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Float(200704,1:4,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Float(802816,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Float(200704,1:4,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1:4,3584,64) -> Float(802816,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1:4,3584,64) -> Float(802816,1,14336,256) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,3136,56,1) -> Float(401408,1,7168,128) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer2/layer2.0/relu/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00967192
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00698558
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00966339
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.00698558
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,3136,56,1) -> Float(100352,1:4,1792,32) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer2/layer2.0/relu/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0098691
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00699342
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00987185
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.00699342
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,7168,128) -> Float(401408,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer2/layer2.0/relu/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00994987
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00765329
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0100148
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.00765329
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,7168,128) -> Float(100352,1:4,1792,32) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer2/layer2.0/relu/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0065467
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00648209
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00654816
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.00648209
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1:4,1792,32) -> Float(401408,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer2/layer2.0/relu/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0100937
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00765329
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0101108
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.00765329
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1:4,1792,32) -> Float(401408,1,7168,128) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer2/layer2.0/relu/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00657267
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0064642
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00657621
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.0064642
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Float(802816,1,14336,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Float(200704,1:4,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Float(802816,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Float(200704,1:4,3584,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1:4,3584,64) -> Float(802816,3136,56,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1:4,3584,64) -> Float(802816,1,14336,256) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer2/layer2.0/relu_1/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00406336
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00650722
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00406628
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00406336
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(25088,1:4,896,32) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer2/layer2.0/relu_1/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00413623
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00651636
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00413231
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00413231
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer2/layer2.0/relu_1/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0047009
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00699059
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00468011
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00468011
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(25088,1:4,896,32) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer2/layer2.0/relu_1/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00338637
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00631374
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00341355
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00338637
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer2/layer2.0/relu_1/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00476602
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00698689
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00476937
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00476602
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,1,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer2/layer2.0/relu_1/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00344664
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00635349
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00348169
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00344664
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(401408,1,14336,512) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer2/layer2.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0104921
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00719497
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0105032
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.00719497
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(100352,1:4,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer2/layer2.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0106276
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0071904
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0106057
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.0071904
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(401408,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer2/layer2.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0101044
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0084359
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0100898
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.0084359
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(100352,1:4,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer2/layer2.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00655543
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00694618
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00657621
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00655543
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1:4,3584,128) -> Float(401408,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer2/layer2.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.01024
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0084679
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.01024
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.0084679
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1:4,3584,128) -> Float(401408,1,14336,512) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer2/layer2.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00655626
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00695837
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00656291
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00655626
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(401408,1,14336,512) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer2/layer2.0/relu_2/Relu_output_0) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0104624
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00721303
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0104653
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.00721303
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(100352,1:4,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer2/layer2.0/relu_2/Relu_output_0) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0106384
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00720983
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0106021
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.00720983
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(401408,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer2/layer2.0/relu_2/Relu_output_0) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0101141
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0084722
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0101132
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.0084722
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(100352,1:4,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer2/layer2.0/relu_2/Relu_output_0) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00654795
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0069588
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00655501
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00654795
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1:4,3584,128) -> Float(401408,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer2/layer2.0/relu_2/Relu_output_0) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.010241
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00839721
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0102642
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.00839721
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1:4,3584,128) -> Float(401408,1,14336,512) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer2/layer2.0/relu_2/Relu_output_0) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00655979
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00694879
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00657621
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00655979
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(401408,1,14336,512) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(100352,1:4,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(401408,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(100352,1:4,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1:4,3584,128) -> Float(401408,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1:4,3584,128) -> Float(401408,1,14336,512) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(25088,1:4,896,32) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(25088,1:4,896,32) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,1,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(25088,1:4,896,32) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(25088,1:4,896,32) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,1,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(401408,1,14336,512) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(100352,1:4,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(401408,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(100352,1:4,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1:4,3584,128) -> Float(401408,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1:4,3584,128) -> Float(401408,1,14336,512) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(401408,1,14336,512) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(100352,1:4,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(401408,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(100352,1:4,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1:4,3584,128) -> Float(401408,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1:4,3584,128) -> Float(401408,1,14336,512) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(401408,1,14336,512) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(100352,1:4,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(401408,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(100352,1:4,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1:4,3584,128) -> Float(401408,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1:4,3584,128) -> Float(401408,1,14336,512) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(25088,1:4,896,32) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(25088,1:4,896,32) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,1,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(25088,1:4,896,32) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(25088,1:4,896,32) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,1,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(401408,1,14336,512) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(100352,1:4,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(401408,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(100352,1:4,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1:4,3584,128) -> Float(401408,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1:4,3584,128) -> Float(401408,1,14336,512) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(401408,1,14336,512) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(100352,1:4,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(401408,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(100352,1:4,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1:4,3584,128) -> Float(401408,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1:4,3584,128) -> Float(401408,1,14336,512) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(401408,1,14336,512) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(100352,1:4,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(401408,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(100352,1:4,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1:4,3584,128) -> Float(401408,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1:4,3584,128) -> Float(401408,1,14336,512) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(25088,1:4,896,32) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(25088,1:4,896,32) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,1,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(25088,1:4,896,32) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(25088,1:4,896,32) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,1,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(401408,1,14336,512) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(100352,1:4,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(401408,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(100352,1:4,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1:4,3584,128) -> Float(401408,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1:4,3584,128) -> Float(401408,1,14336,512) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(401408,1,14336,512) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(100352,1:4,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(401408,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(100352,1:4,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1:4,3584,128) -> Float(401408,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1:4,3584,128) -> Float(401408,1,14336,512) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(401408,1,14336,512) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(100352,1:4,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(401408,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(100352,1:4,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1:4,3584,128) -> Float(401408,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1:4,3584,128) -> Float(401408,1,14336,512) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,784,28,1) -> Float(200704,1,7168,256) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer3/layer3.0/relu/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00676488
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00664873
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0067493
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.00664873
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,784,28,1) -> Float(50176,1:4,1792,64) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer3/layer3.0/relu/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00689437
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00656956
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00690852
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.00656956
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,7168,256) -> Float(200704,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer3/layer3.0/relu/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00635985
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00752722
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00634594
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00634594
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,7168,256) -> Float(50176,1:4,1792,64) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer3/layer3.0/relu/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00458057
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00638171
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00457129
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00457129
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,1792,64) -> Float(200704,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer3/layer3.0/relu/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00587685
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00672852
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00561512
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00561512
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,1792,64) -> Float(200704,1,7168,256) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer3/layer3.0/relu/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00408842
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00574976
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00407924
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00407924
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(401408,1,14336,512) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(100352,1:4,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(401408,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(100352,1:4,3584,128) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1:4,3584,128) -> Float(401408,784,28,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1:4,3584,128) -> Float(401408,1,14336,512) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer3/layer3.0/relu_1/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00322743
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00582656
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00324581
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00322743
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer3/layer3.0/relu_1/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00326982
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00584539
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00328208
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00326982
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer3/layer3.0/relu_1/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00322966
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00641192
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00322915
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00322915
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer3/layer3.0/relu_1/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00269054
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0056807
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00268563
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00268563
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer3/layer3.0/relu_1/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00325808
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0064157
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00328499
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00325808
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer3/layer3.0/relu_1/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00266768
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00568492
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00268157
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00266768
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer3/layer3.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00563007
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00598819
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00561969
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00561969
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer3/layer3.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00563692
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00606343
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00566998
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00563692
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer3/layer3.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00559402
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00712664
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00558576
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00558576
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer3/layer3.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00406755
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00595905
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00407873
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00406755
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer3/layer3.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00567297
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00712468
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0056735
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00567297
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer3/layer3.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00409025
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00591451
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00407771
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00407771
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer3/layer3.0/relu_2/Relu_output_0) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00559209
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0060541
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00559824
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00559209
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer3/layer3.0/relu_2/Relu_output_0) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00567578
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00606419
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00564905
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00564905
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer3/layer3.0/relu_2/Relu_output_0) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00557943
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00713513
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00557767
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00557767
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer3/layer3.0/relu_2/Relu_output_0) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00408176
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00588489
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00406197
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00406197
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer3/layer3.0/relu_2/Relu_output_0) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00566892
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00711227
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00567807
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00566892
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer3/layer3.0/relu_2/Relu_output_0) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00408051
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00591836
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00408764
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00408051
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,196,14,1) -> Float(100352,1,7168,512) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer4/layer4.0/relu/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00414119
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00593353
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0040849
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.0040849
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,196,14,1) -> Float(25088,1:4,1792,128) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer4/layer4.0/relu/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00418246
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00595943
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00417855
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00417855
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,7168,512) -> Float(100352,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer4/layer4.0/relu/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00380644
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0065228
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00383134
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00380644
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,7168,512) -> Float(25088,1:4,1792,128) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer4/layer4.0/relu/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00311294
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00581632
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00308299
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00308299
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,1792,128) -> Float(100352,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer4/layer4.0/relu/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00390054
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00652571
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00388485
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00388485
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,1792,128) -> Float(100352,1,7168,512) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer4/layer4.0/relu/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00312298
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0058816
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00309738
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00309738
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,3584,256) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(25088,1,3584,512) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer4/layer4.0/relu_1/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00281412
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00592988
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00280408
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00280408
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(6272,1:4,896,128) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer4/layer4.0/relu_1/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00282748
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00594267
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00283133
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00282748
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(25088,49,7,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer4/layer4.0/relu_1/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00269401
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00606952
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00267522
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00267522
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(6272,1:4,896,128) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer4/layer4.0/relu_1/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00254637
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00562637
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00248189
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00248189
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,49,7,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer4/layer4.0/relu_1/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00271552
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.005988
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00269181
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00269181
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,1,3584,512) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer4/layer4.0/relu_1/Relu_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00249647
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00556853
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00249056
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00249056
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Float(100352,1,14336,2048) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer4/layer4.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00399365
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00582583
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0039779
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.0039779
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Float(25088,1:4,3584,512) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer4/layer4.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0040734
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00585015
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0040814
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.0040734
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Float(100352,49,7,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer4/layer4.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00341072
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00599067
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00340452
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00340452
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Float(25088,1:4,3584,512) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer4/layer4.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00307988
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00553389
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00307657
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00307657
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,3584,512) -> Float(100352,49,7,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer4/layer4.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00344316
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00596667
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00343543
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00343543
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,3584,512) -> Float(100352,1,14336,2048) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer4/layer4.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00314176
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00759916
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00416131
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00314176
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Float(100352,1,14336,2048) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer4/layer4.0/relu_2/Relu_output_0) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00410357
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0070818
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00400114
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00400114
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Float(25088,1:4,3584,512) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer4/layer4.0/relu_2/Relu_output_0) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00406692
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00581577
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00408895
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00406692
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Float(100352,49,7,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer4/layer4.0/relu_2/Relu_output_0) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00338371
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00595238
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00340267
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00338371
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Float(25088,1:4,3584,512) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer4/layer4.0/relu_2/Relu_output_0) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00306937
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0055149
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00306169
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00306169
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,3584,512) -> Float(100352,49,7,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer4/layer4.0/relu_2/Relu_output_0) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00343967
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00595295
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00343989
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00343967
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,3584,512) -> Float(100352,1,14336,2048) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer4/layer4.0/relu_2/Relu_output_0) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00322641
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00550822
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00309262
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00309262
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Float(100352,1,14336,2048) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Float(25088,1:4,3584,512) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Float(100352,49,7,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Float(25088,1:4,3584,512) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,3584,512) -> Float(100352,49,7,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,3584,512) -> Float(100352,1,14336,2048) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(25088,1,3584,512) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(6272,1:4,896,128) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(25088,49,7,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(6272,1:4,896,128) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,49,7,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,1,3584,512) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(25088,1,3584,512) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(6272,1:4,896,128) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(25088,49,7,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(6272,1:4,896,128) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,49,7,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,1,3584,512) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Float(100352,1,14336,2048) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Float(25088,1:4,3584,512) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Float(100352,49,7,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Float(25088,1:4,3584,512) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,3584,512) -> Float(100352,49,7,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,3584,512) -> Float(100352,1,14336,2048) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Float(100352,1,14336,2048) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Float(25088,1:4,3584,512) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Float(100352,49,7,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Float(25088,1:4,3584,512) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,3584,512) -> Float(100352,49,7,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,3584,512) -> Float(100352,1,14336,2048) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Float(100352,1,14336,2048) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Float(25088,1:4,3584,512) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Float(100352,49,7,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Float(25088,1:4,3584,512) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,3584,512) -> Float(100352,49,7,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,3584,512) -> Float(100352,1,14336,2048) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(25088,1,3584,512) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(6272,1:4,896,128) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(25088,49,7,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(6272,1:4,896,128) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,49,7,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,1,3584,512) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(25088,1,3584,512) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(6272,1:4,896,128) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(25088,49,7,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(6272,1:4,896,128) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,49,7,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,1,3584,512) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Float(100352,1,14336,2048) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Float(25088,1:4,3584,512) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Float(100352,49,7,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Float(25088,1:4,3584,512) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,3584,512) -> Float(100352,49,7,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,3584,512) -> Float(100352,1,14336,2048) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Float(25088,1:4,3584,512) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Float(100352,49,7,1) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Float(25088,1:4,3584,512) ***************
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,3584,512) -> Float(100352,49,7,1) ***************
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(2048,1,1,1) -> Float(2048,1,2048,2048) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/avgpool/GlobalAveragePool_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00228738
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00408542
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00240442
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00228738
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(2048,1,1,1) -> Float(512,1:4,512,512) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/avgpool/GlobalAveragePool_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00230614
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00562321
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00225321
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00225321
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(2048,1,2048,2048) -> Float(2048,1,1,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/avgpool/GlobalAveragePool_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00227265
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00410148
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.002272
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.002272
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(2048,1,2048,2048) -> Float(512,1:4,512,512) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/avgpool/GlobalAveragePool_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00224679
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00532707
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00225343
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00224679
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(512,1:4,512,512) -> Float(2048,1,1,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/avgpool/GlobalAveragePool_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00225914
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0056647
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00228078
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00225914
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(512,1:4,512,512) -> Float(2048,1,2048,2048) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/avgpool/GlobalAveragePool_output_0 -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00226607
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00532775
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00225721
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00225721
[05/26/2023-16:04:36] [V] [TRT] =============== Computing reformatting costs: 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(1000,1,1000,1000) -> Float(1000,1,1,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/fc/Gemm_out_tensor -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00226961
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00397752
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00225529
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00225529
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning Reformat: Float(250,1:4,250,250) -> Float(1000,1,1,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/fc/Gemm_out_tensor -> <out>) (Reformat)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00228956
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00559719
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00229464
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00228956
[05/26/2023-16:04:36] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:36] [V] [TRT] *************** Autotuning format combination: Float(150528,50176,224,1) -> Float(802816,12544,112,1) ***************
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /relu/Relu (CudaDepthwiseConvolution)
[05/26/2023-16:04:36] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /relu/Relu (FusedConvActConvolution)
[05/26/2023-16:04:36] [V] [TRT] Tactic: 0x000000000001ffff Time: 0.0402891
[05/26/2023-16:04:36] [V] [TRT] Fastest Tactic: 0x000000000001ffff Time: 0.0402891
[05/26/2023-16:04:36] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /relu/Relu (CudnnConvolution)
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0421303
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.0379246
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000000002 Time: 0.184027
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000000005 Time: 0.647314
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000000038 Time: 0.0397166
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000000039 Time: 0.0379977
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000000003a Time: 0.183881
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000000003d Time: 0.638683
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000000070 Time: 0.0397166
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000000071 Time: 0.0380217
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000000072 Time: 0.184027
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000000075 Time: 0.635026
[05/26/2023-16:04:37] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 0.0379246
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /relu/Relu (CaskGemmConvolution)
[05/26/2023-16:04:37] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /relu/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:37] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /relu/Relu (CaskConvolution)
[05/26/2023-16:04:37] [V] [TRT] /conv1/Conv + /relu/Relu Set Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x5deb29b7a8e275f7 Time: 0.0254293
[05/26/2023-16:04:37] [V] [TRT] /conv1/Conv + /relu/Relu Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 0.0271139
[05/26/2023-16:04:37] [V] [TRT] /conv1/Conv + /relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 0x5403ad713f811a18
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x5403ad713f811a18 Time: 0.0452251
[05/26/2023-16:04:37] [V] [TRT] /conv1/Conv + /relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xcb8a43f748d8a338 Time: 0.0405211
[05/26/2023-16:04:37] [V] [TRT] /conv1/Conv + /relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 0.0275581
[05/26/2023-16:04:37] [V] [TRT] /conv1/Conv + /relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x40a12e3938221818 Time: 0.058173
[05/26/2023-16:04:37] [V] [TRT] /conv1/Conv + /relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xa9366041633a5135 Time: 0.0367177
[05/26/2023-16:04:37] [V] [TRT] /conv1/Conv + /relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 0.0415509
[05/26/2023-16:04:37] [V] [TRT] Fastest Tactic: 0x5deb29b7a8e275f7 Time: 0.0254293
[05/26/2023-16:04:37] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5deb29b7a8e275f7
[05/26/2023-16:04:37] [V] [TRT] *************** Autotuning format combination: Float(150528,1,672,3) -> Float(802816,1,7168,64) ***************
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /relu/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:37] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /relu/Relu (CaskConvolution)
[05/26/2023-16:04:37] [V] [TRT] /conv1/Conv + /relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xd9031472c05adf51 Time: 0.091648
[05/26/2023-16:04:37] [V] [TRT] /conv1/Conv + /relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 0.0439589
[05/26/2023-16:04:37] [V] [TRT] /conv1/Conv + /relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0a143be7a52f301a Time: 0.063232
[05/26/2023-16:04:37] [V] [TRT] /conv1/Conv + /relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x4fd3c46622e98342 Time: 0.0753371
[05/26/2023-16:04:37] [V] [TRT] /conv1/Conv + /relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xa6448a1e79f1ca6f Time: 0.049152
[05/26/2023-16:04:37] [V] [TRT] /conv1/Conv + /relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: 0xca7eeb8d9143d738
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xca7eeb8d9143d738 Time: 0.092144
[05/26/2023-16:04:37] [V] [TRT] Fastest Tactic: 0x19b688348f983aa0 Time: 0.0439589
[05/26/2023-16:04:37] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x19b688348f983aa0
[05/26/2023-16:04:37] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,224,1) -> Float(200704,1:4,1792,16) ***************
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /relu/Relu (CudaDepthwiseConvolution)
[05/26/2023-16:04:37] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /relu/Relu (CaskGemmConvolution)
[05/26/2023-16:04:37] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /relu/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:37] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /relu/Relu (CaskConvolution)
[05/26/2023-16:04:37] [V] [TRT] /conv1/Conv + /relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xd9031472c05adf51 Time: 0.0917211
[05/26/2023-16:04:37] [V] [TRT] /conv1/Conv + /relu/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xbdfdef6b84f7ccc9 Time: 0.100206
[05/26/2023-16:04:37] [V] [TRT] /conv1/Conv + /relu/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x412c44dfeaf9161d Time: 0.101961
[05/26/2023-16:04:37] [V] [TRT] /conv1/Conv + /relu/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x7bc32c782b800c48 Time: 0.088336
[05/26/2023-16:04:37] [V] [TRT] /conv1/Conv + /relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 0.0439589
[05/26/2023-16:04:37] [V] [TRT] /conv1/Conv + /relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0a143be7a52f301a Time: 0.0632442
[05/26/2023-16:04:37] [V] [TRT] /conv1/Conv + /relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x4fd3c46622e98342 Time: 0.0750446
[05/26/2023-16:04:37] [V] [TRT] /conv1/Conv + /relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 0.121397
[05/26/2023-16:04:37] [V] [TRT] /conv1/Conv + /relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xa6448a1e79f1ca6f Time: 0.0492008
[05/26/2023-16:04:37] [V] [TRT] /conv1/Conv + /relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: 0xca7eeb8d9143d738
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xca7eeb8d9143d738 Time: 0.0920869
[05/26/2023-16:04:37] [V] [TRT] /conv1/Conv + /relu/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0x634e99502974e4da
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x634e99502974e4da Time: 0.0892091
[05/26/2023-16:04:37] [V] [TRT] /conv1/Conv + /relu/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x3e191488237fab8f Time: 0.157257
[05/26/2023-16:04:37] [V] [TRT] Fastest Tactic: 0x19b688348f983aa0 Time: 0.0439589
[05/26/2023-16:04:37] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x19b688348f983aa0
[05/26/2023-16:04:37] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:37] [V] [TRT] *************** Autotuning format combination: Float(802816,12544,112,1) -> Float(200704,3136,56,1) ***************
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /maxpool/MaxPool (TiledPooling)
[05/26/2023-16:04:37] [V] [TRT] TiledPooling has no valid tactics for this config, skipping
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /maxpool/MaxPool (CudnnPooling)
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xffffffffffffffff Time: 0.00581321
[05/26/2023-16:04:37] [V] [TRT] Fastest Tactic: 0xffffffffffffffff Time: 0.00581321
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /maxpool/MaxPool (CaskPooling)
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll5_tThreads1017 Tactic: 0x7b9e5e445528b90a
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x7b9e5e445528b90a Time: 0.00601524
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll3_tThreads513 Tactic: 0xcb3875826530ea38
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xcb3875826530ea38 Time: 0.0058869
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll5_tThreads791 Tactic: 0x7f97b1a406293c0b
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x7f97b1a406293c0b Time: 0.00656291
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll5_tThreads841 Tactic: 0x7bd883ae684d33e0
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x7bd883ae684d33e0 Time: 0.00713927
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll1_tThreads855 Tactic: 0xf86a4e1f189f4821
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xf86a4e1f189f4821 Time: 0.0121509
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll6_tThreads855 Tactic: 0x5f5e601b4a0531ed
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x5f5e601b4a0531ed Time: 0.00643359
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll1_tThreads1017 Tactic: 0x4cf88ed475f74f6e
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x4cf88ed475f74f6e Time: 0.0112865
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll6_tThreads513 Tactic: 0x4587105059a26a2b
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x4587105059a26a2b Time: 0.00542866
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll4_tThreads855 Tactic: 0x76d52bcd240dc832
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x76d52bcd240dc832 Time: 0.00653257
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll6_tThreads791 Tactic: 0x8bb5080c88a2b679
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x8bb5080c88a2b679 Time: 0.00668883
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll2_tThreads791 Tactic: 0xd8a39fa054b345c7
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xd8a39fa054b345c7 Time: 0.0085284
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll3_tThreads1017 Tactic: 0x574be69c6598b45c
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x574be69c6598b45c Time: 0.00676696
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll4_tThreads791 Tactic: 0xa23e43dae6aa4fa6
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xa23e43dae6aa4fa6 Time: 0.0069181
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll1_tThreads791 Tactic: 0x2c812608da38cfb5
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x2c812608da38cfb5 Time: 0.013211
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll6_tThreads255 Tactic: 0xd53eb77c06f70e73
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xd53eb77c06f70e73 Time: 0.00490972
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_fw_4d_FP32FP32NCHW_Max Tactic: 0xb59f9cfb90407c92
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xb59f9cfb90407c92 Time: 0.00586935
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll1_tThreads513 Tactic: 0xe2b33e540b3813e7
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xe2b33e540b3813e7 Time: 0.0101138
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll5_tThreads513 Tactic: 0xb1a5a9f8d729e059
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xb1a5a9f8d729e059 Time: 0.00544609
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll2_tThreads513 Tactic: 0x169187fc85b39995
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x169187fc85b39995 Time: 0.00666369
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll3_tThreads225 Tactic: 0x552fb57ee00d44f2
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x552fb57ee00d44f2 Time: 0.00535958
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll2_tThreads841 Tactic: 0xdcecadaa3ad74a2c
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xdcecadaa3ad74a2c Time: 0.00842299
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll6_tThreads1017 Tactic: 0x6df482284d70bfa1
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x6df482284d70bfa1 Time: 0.0063352
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_nd_NCDHW_kMAX_kGENERIC_3D_POOLING_MODE_kFLOAT_0 Tactic: 0x5faf4a0a8a5670ed
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x5faf4a0a8a5670ed Time: 0.00745691
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll3_tThreads791 Tactic: 0x050a6ddeb430366a
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x050a6ddeb430366a Time: 0.00785227
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll5_tThreads255 Tactic: 0x211c0ed4887c8401
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x211c0ed4887c8401 Time: 0.0049053
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll3_tThreads841 Tactic: 0x01455fd4da543981
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x01455fd4da543981 Time: 0.00773293
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll4_tThreads225 Tactic: 0xf21b9b7ab2973d3e
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xf21b9b7ab2973d3e Time: 0.00518939
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_custom_tP4_tQ32_tRS3_tUV2 Tactic: 0x2639d3932b27ac67
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x2639d3932b27ac67 Time: 0.00498869
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll4_tThreads255 Tactic: 0xfcb5fcaa68fff7ac
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xfcb5fcaa68fff7ac Time: 0.00480137
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll5_tThreads855 Tactic: 0xab7cd9b3c48ebb9f
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xab7cd9b3c48ebb9f Time: 0.00660135
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll5_tThreads225 Tactic: 0x2fb2690452144e93
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x2fb2690452144e93 Time: 0.00520033
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll3_tThreads255 Tactic: 0x5b81d2ae3a658e60
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x5b81d2ae3a658e60 Time: 0.00492282
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll6_tThreads225 Tactic: 0xdb90d0acdc9fc4e1
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xdb90d0acdc9fc4e1 Time: 0.00519755
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll4_tThreads841 Tactic: 0xa67171d088ce404d
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xa67171d088ce404d Time: 0.00699516
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll2_tThreads1017 Tactic: 0x5a9252b86daf49c5
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x5a9252b86daf49c5 Time: 0.00748846
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll1_tThreads225 Tactic: 0x7ca4fea88e05bd2d
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x7ca4fea88e05bd2d Time: 0.00790713
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll3_tThreads855 Tactic: 0xd1e105c97697b1fe
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xd1e105c97697b1fe Time: 0.00737188
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll2_tThreads255 Tactic: 0x862820d0dae6fdcd
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x862820d0dae6fdcd Time: 0.00526906
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll6_tThreads841 Tactic: 0x8ffa3a06e6c6b992
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x8ffa3a06e6c6b992 Time: 0.00676093
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll4_tThreads1017 Tactic: 0x7647ea605d1f4493
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x7647ea605d1f4493 Time: 0.00674348
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll1_tThreads255 Tactic: 0x720a9978546d77bf
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x720a9978546d77bf Time: 0.00705655
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll2_tThreads225 Tactic: 0x88864700008e375f
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x88864700008e375f Time: 0.00575708
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll2_tThreads855 Tactic: 0x0c48f7b79614c253
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0c48f7b79614c253 Time: 0.008256
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll4_tThreads513 Tactic: 0x6c0c5b8637aa93f4
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x6c0c5b8637aa93f4 Time: 0.00570128
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll1_tThreads841 Tactic: 0x28ce1402b45cc05e
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x28ce1402b45cc05e Time: 0.0123688
[05/26/2023-16:04:37] [V] [TRT] Fastest Tactic: 0xfcb5fcaa68fff7ac Time: 0.00480137
[05/26/2023-16:04:37] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0xfcb5fcaa68fff7ac
[05/26/2023-16:04:37] [V] [TRT] *************** Autotuning format combination: Float(200704,1:4,1792,16) -> Float(50176,1:4,896,16) ***************
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /maxpool/MaxPool (CaskPooling)
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_3_PROPAGATE_NAN_2D Tactic: 0xbd3963b8ccd084c6
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xbd3963b8ccd084c6 Time: 0.00509572
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_3_NOT_PROPAGATE_NAN_3D Tactic: 0x2c7251cbae30cf74
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x2c7251cbae30cf74 Time: 0.00828241
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_0_PROPAGATE_NAN_2D Tactic: 0x8382d5c464539e87
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x8382d5c464539e87 Time: 0.0068345
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_0_NOT_PROPAGATE_NAN_2D Tactic: 0xaec8628e8180bced
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xaec8628e8180bced Time: 0.00573458
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_3_PROPAGATE_NAN_3D Tactic: 0xe9d01a2a900075cb
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xe9d01a2a900075cb Time: 0.00828343
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_0_NOT_PROPAGATE_NAN_3D Tactic: 0xfa211b1cdd504de0
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xfa211b1cdd504de0 Time: 0.00594944
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_3_NOT_PROPAGATE_NAN_2D Tactic: 0x789b2859f2e03e79
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x789b2859f2e03e79 Time: 0.00493383
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_fw_4d_FP32FP32NHWC_Max_CAlign4 Tactic: 0x22fb1bb4a70e340d
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x22fb1bb4a70e340d Time: 0.00858622
[05/26/2023-16:04:37] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_0_PROPAGATE_NAN_3D Tactic: 0xd76bac5638836f8a
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xd76bac5638836f8a Time: 0.00731908
[05/26/2023-16:04:37] [V] [TRT] Fastest Tactic: 0x789b2859f2e03e79 Time: 0.00493383
[05/26/2023-16:04:37] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0x789b2859f2e03e79
[05/26/2023-16:04:37] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:37] [V] [TRT] *************** Autotuning format combination: Float(200704,3136,56,1) -> Float(200704,3136,56,1) ***************
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu (CudaDepthwiseConvolution)
[05/26/2023-16:04:37] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu (FusedConvActConvolution)
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000008ffff Time: 0.0226109
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000009ffff Time: 0.0152421
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x00000000000bffff Time: 0.0137642
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x00000000000cffff Time: 0.0161839
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000011ffff Time: 0.011997
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000018ffff Time: 0.0130589
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x00000000001bffff Time: 0.0142691
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x00000000001fffff Time: 0.0132929
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x00000000002effff Time: 0.0140301
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000035ffff Time: 0.0133436
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x00000000003cffff Time: 0.0130992
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x00000000003dffff Time: 0.0150834
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000040ffff Time: 0.0121661
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000041ffff Time: 0.0130597
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000045ffff Time: 0.0129619
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000046ffff Time: 0.0126088
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x00000000004affff Time: 0.0126549
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x00000000004effff Time: 0.0119192
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000051ffff Time: 0.012861
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000055ffff Time: 0.0150505
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000059ffff Time: 0.0128088
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x00000000005effff Time: 0.012877
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x00000000005fffff Time: 0.0119787
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000061ffff Time: 0.013642
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000066ffff Time: 0.0129999
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x00000000006affff Time: 0.0133519
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x00000000006bffff Time: 0.0124747
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x00000000006fffff Time: 0.0130693
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000070ffff Time: 0.0131765
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000073ffff Time: 0.0129695
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000075ffff Time: 0.0138814
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x00000000007cffff Time: 0.0126903
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x00000000007dffff Time: 0.0125977
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x00000000007effff Time: 0.0135377
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000083ffff Time: 0.0132829
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x00000000008affff Time: 0.0135555
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000091ffff Time: 0.0126621
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x00000000009dffff Time: 0.0117254
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000a3ffff Time: 0.0137376
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000a6ffff Time: 0.0145554
[05/26/2023-16:04:37] [V] [TRT] Fastest Tactic: 0x00000000009dffff Time: 0.0117254
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu (CudnnConvolution)
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0132189
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.010543
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000000002 Time: 0.0349038
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000000004 Time: 0.435639
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000000005 Time: 0.0318263
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000000038 Time: 0.0131524
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000000039 Time: 0.010543
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000000003a Time: 0.0349038
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000000003c Time: 0.435346
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000000003d Time: 0.031861
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000000070 Time: 0.0132189
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000000071 Time: 0.0132189
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000000072 Time: 0.034933
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000000074 Time: 0.434907
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000000075 Time: 0.031851
[05/26/2023-16:04:37] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 0.010543
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu (CublasConvolution)
[05/26/2023-16:04:37] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu (CaskGemmConvolution)
[05/26/2023-16:04:37] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:37] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu (CaskConvolution)
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 0.0126171
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 0.0117542
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: 0xa8ef60e712f8ad24
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xa8ef60e712f8ad24 Time: 0.0135335
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xc3cf6e1d1c6aff27
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xc3cf6e1d1c6aff27 Time: 0.0110277
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 0.013787
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x9808072e706def96
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x9808072e706def96 Time: 0.0123566
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xcb8a43f748d8a338 Time: 0.0101315
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 0.00984838
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xa31d27de74b895ff Time: 0.00721806
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfff46c7893896eb1
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xfff46c7893896eb1 Time: 0.0241684
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x40a12e3938221818 Time: 0.0137675
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x7f0145cb49517338
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x7f0145cb49517338 Time: 0.0176422
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa419b3b68f2da07b
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xa419b3b68f2da07b Time: 0.0167426
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9cd5cdc35441c505
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x9cd5cdc35441c505 Time: 0.0104281
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fc87d7eb370bb7a
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x1fc87d7eb370bb7a Time: 0.0072592
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x12dbf7d94ee3696d Time: 0.0120442
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xe5603263b7f00303
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xe5603263b7f00303 Time: 0.011971
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x828d0ea88c66fce7
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x828d0ea88c66fce7 Time: 0.00768144
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9de226a0c44627c4
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x9de226a0c44627c4 Time: 0.0195109
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xa9366041633a5135 Time: 0.0121516
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x8e3884f0eaec3ecd
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x8e3884f0eaec3ecd Time: 0.010705
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xbb8c3889c7eacd30 Time: 0.0264952
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xb0bf940d5e0f9f45 Time: 0.00806857
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xc0b05b61d128e46e
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xc0b05b61d128e46e Time: 0.0109287
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x9d9fdb5fd9945f64 Time: 0.00828013
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x5aa723e0481da855 Time: 0.0111177
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x2ee10e11d6651675
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x2ee10e11d6651675 Time: 0.02048
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x865894c4635db7fd
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x865894c4635db7fd Time: 0.00725189
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90f8f2915f87ed77
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x90f8f2915f87ed77 Time: 0.00779525
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 0.0114599
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 0.0137808
[05/26/2023-16:04:37] [V] [TRT] Fastest Tactic: 0xa31d27de74b895ff Time: 0.00721806
[05/26/2023-16:04:37] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:37] [V] [TRT] *************** Autotuning format combination: Float(200704,1,3584,64) -> Float(200704,1,3584,64) ***************
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu (CublasConvolution)
[05/26/2023-16:04:37] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:37] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu (CaskConvolution)
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0x17173deba0b64484
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x17173deba0b64484 Time: 0.0126171
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x3e2b881168d9689d Time: 0.00800356
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xf90060ce8193b811 Time: 0.0122968
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xd9031472c05adf51 Time: 0.012741
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xe47307053a42b3e4
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xe47307053a42b3e4 Time: 0.0122312
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x412c44dfeaf9161d Time: 0.009468
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x27b316f52c109002
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x27b316f52c109002 Time: 0.00958415
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xae0c89d047932ba3
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xae0c89d047932ba3 Time: 0.00903
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x7bc32c782b800c48 Time: 0.0122659
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x35f26f9c09557d86
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x35f26f9c09557d86 Time: 0.00992366
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xc7b3afceb5fb03c0
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xc7b3afceb5fb03c0 Time: 0.00954453
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x7121ec1db3f80c67 Time: 0.00703456
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x5030121339a48bf3 Time: 0.012811
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd9eb6ca56ddc3a22
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xd9eb6ca56ddc3a22 Time: 0.00702716
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xbc0bba0ff1a92939
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xbc0bba0ff1a92939 Time: 0.0177493
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 0.0073216
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0a143be7a52f301a Time: 0.00744
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x62835fce994f06dd Time: 0.00727269
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xc7feb33970feefa7
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xc7feb33970feefa7 Time: 0.00774304
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x1022069e6f8d9aeb
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x1022069e6f8d9aeb Time: 0.00979474
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90898977fc8ce537
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x90898977fc8ce537 Time: 0.00695902
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x92ed3100c35fc43e Time: 0.00790183
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x4fd3c46622e98342 Time: 0.00921914
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd55ee6fd0b56f808
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xd55ee6fd0b56f808 Time: 0.00949343
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x1da91d865428f237 Time: 0.0106857
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xa6448a1e79f1ca6f Time: 0.00790929
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fb90698107bb33a
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x1fb90698107bb33a Time: 0.00696446
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x55d80c17b1cd982d
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x55d80c17b1cd982d Time: 0.007168
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x3e191488237fab8f Time: 0.00804114
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x8014228ec08b4d49 Time: 0.0189366
[05/26/2023-16:04:37] [V] [TRT] Fastest Tactic: 0x90898977fc8ce537 Time: 0.00695902
[05/26/2023-16:04:37] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x90898977fc8ce537
[05/26/2023-16:04:37] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16) -> Float(50176,1:4,896,16) ***************
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu (CudaDepthwiseConvolution)
[05/26/2023-16:04:37] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu (CublasConvolution)
[05/26/2023-16:04:37] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu (CaskGemmConvolution)
[05/26/2023-16:04:37] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:37] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu (CaskConvolution)
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x9dece0dc37e90462
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x9dece0dc37e90462 Time: 0.00968442
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x130df49cb195156b
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x130df49cb195156b Time: 0.00994743
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 0.0100556
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 0.00993554
[05/26/2023-16:04:37] [V] [TRT] Fastest Tactic: 0x9dece0dc37e90462 Time: 0.00968442
[05/26/2023-16:04:37] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9dece0dc37e90462
[05/26/2023-16:04:37] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:37] [V] [TRT] *************** Autotuning format combination: Float(200704,3136,56,1) -> Float(200704,3136,56,1) ***************
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu (CudaDepthwiseConvolution)
[05/26/2023-16:04:37] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu (FusedConvActConvolution)
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000007ffff Time: 0.0346597
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x00000000000affff Time: 0.0307785
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x00000000000effff Time: 0.0250789
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x00000000000fffff Time: 0.0313929
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000019ffff Time: 0.0221936
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x00000000001affff Time: 0.0308114
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000024ffff Time: 0.0311881
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000027ffff Time: 0.0227847
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x00000000002dffff Time: 0.0235311
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000036ffff Time: 0.0246004
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x00000000004cffff Time: 0.0274255
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000062ffff Time: 0.0277943
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x00000000006effff Time: 0.0276427
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000077ffff Time: 0.0238263
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000086ffff Time: 0.0243284
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000089ffff Time: 0.0225339
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000097ffff Time: 0.0276602
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000098ffff Time: 0.0299529
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x00000000009fffff Time: 0.0228624
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000a2ffff Time: 0.0226534
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000a4ffff Time: 0.0246491
[05/26/2023-16:04:37] [V] [TRT] Fastest Tactic: 0x000000000019ffff Time: 0.0221936
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu (CudnnConvolution)
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0678766
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.0270385
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000000002 Time: 0.118784
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000000004 Time: 0.438272
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000000005 Time: 0.17803
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000000006 Time: 0.0220264
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000000038 Time: 0.0679741
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000000039 Time: 0.0268678
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000000003a Time: 0.117979
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000000003c Time: 0.438418
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000000003d Time: 0.177445
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000000003e Time: 0.0217959
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000000070 Time: 0.0671451
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000000071 Time: 0.0735086
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000000072 Time: 0.117687
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000000074 Time: 0.437687
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000000075 Time: 0.176567
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0000000000000076 Time: 0.0217757
[05/26/2023-16:04:37] [V] [TRT] Fastest Tactic: 0x0000000000000076 Time: 0.0217757
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu (CaskGemmConvolution)
[05/26/2023-16:04:37] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:37] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu (CaskConvolution)
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 0.0659901
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x5deb29b7a8e275f7 Time: 0.0544716
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 0.0578804
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 0.0668526
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 0x5403ad713f811a18
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x5403ad713f811a18 Time: 0.0681691
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x94119b4c514b211a Time: 0.0147895
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xcb8a43f748d8a338 Time: 0.0504198
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 0.0484236
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xa31d27de74b895ff Time: 0.0261448
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x40a12e3938221818 Time: 0.078048
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x12dbf7d94ee3696d Time: 0.044032
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xa9366041633a5135 Time: 0.0631954
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xbb8c3889c7eacd30 Time: 0.121271
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xb0bf940d5e0f9f45 Time: 0.0305179
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x4727434768e46395 Time: 0.0258438
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x9d9fdb5fd9945f64 Time: 0.0353426
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x5aa723e0481da855 Time: 0.038912
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x4efce38acc876f5c Time: 0.119662
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x8ad32616b1424be4 Time: 0.029813
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x01cf8ce2da913006 Time: 0.0385097
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 0.0522301
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 0.0676328
[05/26/2023-16:04:37] [V] [TRT] Fastest Tactic: 0x94119b4c514b211a Time: 0.0147895
[05/26/2023-16:04:37] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x94119b4c514b211a
[05/26/2023-16:04:37] [V] [TRT] *************** Autotuning format combination: Float(200704,1,3584,64) -> Float(200704,1,3584,64) ***************
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:37] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu (CaskConvolution)
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x3e2b881168d9689d Time: 0.0238933
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xf90060ce8193b811 Time: 0.0641219
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xd9031472c05adf51 Time: 0.0658286
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xbdfdef6b84f7ccc9 Time: 0.0396434
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x412c44dfeaf9161d Time: 0.0404926
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x27b316f52c109002
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x27b316f52c109002 Time: 0.0404709
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x7bc32c782b800c48 Time: 0.0636404
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x7121ec1db3f80c67 Time: 0.0264472
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x5030121339a48bf3 Time: 0.0666088
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x5953bec563d26434 Time: 0.0262507
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 0.0266423
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x0a143be7a52f301a Time: 0.0316763
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x62835fce994f06dd Time: 0.0243131
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x94a7db94ba744c45 Time: 0.0241341
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x92ed3100c35fc43e Time: 0.0327095
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x32059de4888dfdda Time: 0.0390194
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x4fd3c46622e98342 Time: 0.0437863
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x1da91d865428f237 Time: 0.037632
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xa6448a1e79f1ca6f Time: 0.0309047
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x3f0c846d6379bc98 Time: 0.0705996
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xf48db81f02eca9ee Time: 0.036608
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: 0xca7eeb8d9143d738
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xca7eeb8d9143d738 Time: 0.0670476
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xd15dd11d64344e83 Time: 0.0361326
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0x634e99502974e4da
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x634e99502974e4da Time: 0.0650484
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x3e191488237fab8f Time: 0.0243322
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x8014228ec08b4d49 Time: 0.0638293
[05/26/2023-16:04:37] [V] [TRT] Fastest Tactic: 0x3e2b881168d9689d Time: 0.0238933
[05/26/2023-16:04:37] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3e2b881168d9689d
[05/26/2023-16:04:37] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16) -> Float(50176,1:4,896,16) ***************
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu (CudaDepthwiseConvolution)
[05/26/2023-16:04:37] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu (CaskGemmConvolution)
[05/26/2023-16:04:37] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:37] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu (CaskConvolution)
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 0.0463246
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x999e005e3b016ea6 Time: 0.0442274
[05/26/2023-16:04:37] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 0.0448137
[05/26/2023-16:04:37] [V] [TRT] Fastest Tactic: 0x999e005e3b016ea6 Time: 0.0442274
[05/26/2023-16:04:37] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6
[05/26/2023-16:04:37] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:37] [V] [TRT] *************** Autotuning format combination: Float(200704,3136,56,1) -> Float(802816,3136,56,1) ***************
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/downsample/downsample.0/Conv (CudaDepthwiseConvolution)
[05/26/2023-16:04:37] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:37] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/downsample/downsample.0/Conv (FusedConvActConvolution)
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000008ffff Time: 0.0422766
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000009ffff Time: 0.0390583
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x00000000000bffff Time: 0.03932
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x00000000000cffff Time: 0.0403051
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000011ffff Time: 0.0405943
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000018ffff Time: 0.0394606
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x00000000001bffff Time: 0.0411794
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x00000000001fffff Time: 0.041472
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x00000000002effff Time: 0.0407771
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x000000000035ffff Time: 0.0382171
[05/26/2023-16:04:37] [V] [TRT] Tactic: 0x00000000003cffff Time: 0.04096
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000003dffff Time: 0.0408869
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000040ffff Time: 0.0396754
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000041ffff Time: 0.0385097
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000045ffff Time: 0.0388023
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000046ffff Time: 0.0384366
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000004affff Time: 0.0387657
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000004effff Time: 0.0419109
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000051ffff Time: 0.044544
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000055ffff Time: 0.0406663
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000059ffff Time: 0.0395703
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000005effff Time: 0.0381074
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000005fffff Time: 0.0407406
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000061ffff Time: 0.0395337
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000066ffff Time: 0.0434834
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000006affff Time: 0.0414411
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000006bffff Time: 0.038176
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000006fffff Time: 0.0401554
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000070ffff Time: 0.0394971
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000073ffff Time: 0.0402651
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000075ffff Time: 0.0392411
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000007cffff Time: 0.0426274
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000007dffff Time: 0.0393966
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000007effff Time: 0.0424229
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000083ffff Time: 0.0425691
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000008affff Time: 0.0418011
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000091ffff Time: 0.0388389
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000009dffff Time: 0.039784
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000a3ffff Time: 0.0370103
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000a6ffff Time: 0.03824
[05/26/2023-16:04:38] [V] [TRT] Fastest Tactic: 0x0000000000a3ffff Time: 0.0370103
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/downsample/downsample.0/Conv (CudnnConvolution)
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0234475
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.0156562
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000002 Time: 0.0411794
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000004 Time: 1.65961
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000005 Time: 0.0562712
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000038 Time: 0.0234116
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000039 Time: 0.0156562
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000000003a Time: 0.0411714
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000000003c Time: 1.6599
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000000003d Time: 0.056707
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000070 Time: 0.0234684
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000071 Time: 0.021506
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000072 Time: 0.0412469
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000074 Time: 1.66064
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000075 Time: 0.0563627
[05/26/2023-16:04:38] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 0.0156562
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/downsample/downsample.0/Conv (CublasConvolution)
[05/26/2023-16:04:38] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/downsample/downsample.0/Conv (CaskGemmConvolution)
[05/26/2023-16:04:38] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/downsample/downsample.0/Conv (CaskFlattenConvolution)
[05/26/2023-16:04:38] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/downsample/downsample.0/Conv (CaskConvolution)
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 0.0159451
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 0.014965
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: 0xa8ef60e712f8ad24
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xa8ef60e712f8ad24 Time: 0.0136204
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xc3cf6e1d1c6aff27
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xc3cf6e1d1c6aff27 Time: 0.0137775
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 0.013814
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x9808072e706def96
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x9808072e706def96 Time: 0.0150683
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xcb8a43f748d8a338 Time: 0.021248
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 0.0152082
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xa31d27de74b895ff Time: 0.0127002
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfff46c7893896eb1
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xfff46c7893896eb1 Time: 0.0241859
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x40a12e3938221818 Time: 0.0290231
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x7f0145cb49517338
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x7f0145cb49517338 Time: 0.0175553
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa419b3b68f2da07b
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xa419b3b68f2da07b Time: 0.0166603
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9cd5cdc35441c505
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x9cd5cdc35441c505 Time: 0.0148718
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fc87d7eb370bb7a
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x1fc87d7eb370bb7a Time: 0.0125741
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x12dbf7d94ee3696d Time: 0.0160914
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xe5603263b7f00303
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xe5603263b7f00303 Time: 0.0159457
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x828d0ea88c66fce7
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x828d0ea88c66fce7 Time: 0.0137791
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9de226a0c44627c4
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x9de226a0c44627c4 Time: 0.0193611
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xa9366041633a5135 Time: 0.0187771
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x8e3884f0eaec3ecd
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x8e3884f0eaec3ecd Time: 0.0155561
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xbb8c3889c7eacd30 Time: 0.0262827
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xb0bf940d5e0f9f45 Time: 0.0173445
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xc0b05b61d128e46e
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xc0b05b61d128e46e Time: 0.0152507
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x9d9fdb5fd9945f64 Time: 0.0194286
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x5aa723e0481da855 Time: 0.0152123
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x2ee10e11d6651675
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x2ee10e11d6651675 Time: 0.0204676
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x865894c4635db7fd
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x865894c4635db7fd Time: 0.0126411
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90f8f2915f87ed77
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x90f8f2915f87ed77 Time: 0.0177062
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 0.0146871
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 0.013802
[05/26/2023-16:04:38] [V] [TRT] Fastest Tactic: 0x1fc87d7eb370bb7a Time: 0.0125741
[05/26/2023-16:04:38] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1fc87d7eb370bb7a
[05/26/2023-16:04:38] [V] [TRT] *************** Autotuning format combination: Float(200704,1,3584,64) -> Float(802816,1,14336,256) ***************
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/downsample/downsample.0/Conv (CublasConvolution)
[05/26/2023-16:04:38] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/downsample/downsample.0/Conv (CaskFlattenConvolution)
[05/26/2023-16:04:38] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/downsample/downsample.0/Conv (CaskConvolution)
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0x17173deba0b64484
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x17173deba0b64484 Time: 0.0129097
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x3e2b881168d9689d Time: 0.0154784
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xf90060ce8193b811 Time: 0.0124587
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xd9031472c05adf51 Time: 0.0130726
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xe47307053a42b3e4
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xe47307053a42b3e4 Time: 0.0123832
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x412c44dfeaf9161d Time: 0.0143799
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x27b316f52c109002
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x27b316f52c109002 Time: 0.0143214
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xae0c89d047932ba3
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xae0c89d047932ba3 Time: 0.0131636
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x7bc32c782b800c48 Time: 0.0124472
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x35f26f9c09557d86
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x35f26f9c09557d86 Time: 0.0143634
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xc7b3afceb5fb03c0
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xc7b3afceb5fb03c0 Time: 0.0138174
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x7121ec1db3f80c67 Time: 0.015477
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x5030121339a48bf3 Time: 0.0130992
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd9eb6ca56ddc3a22
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xd9eb6ca56ddc3a22 Time: 0.0128453
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xbc0bba0ff1a92939
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xbc0bba0ff1a92939 Time: 0.0178011
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 0.0127215
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0a143be7a52f301a Time: 0.0167578
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x62835fce994f06dd Time: 0.0127147
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xc7feb33970feefa7
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xc7feb33970feefa7 Time: 0.0151579
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x1022069e6f8d9aeb
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x1022069e6f8d9aeb Time: 0.0141265
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90898977fc8ce537
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x90898977fc8ce537 Time: 0.0122347
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x92ed3100c35fc43e Time: 0.0167619
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x4fd3c46622e98342 Time: 0.0199497
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd55ee6fd0b56f808
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xd55ee6fd0b56f808 Time: 0.0138074
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x1da91d865428f237 Time: 0.0143662
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xa6448a1e79f1ca6f Time: 0.0145938
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fb90698107bb33a
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x1fb90698107bb33a Time: 0.0150679
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x55d80c17b1cd982d
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x55d80c17b1cd982d Time: 0.0124362
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x3e191488237fab8f Time: 0.015584
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x8014228ec08b4d49 Time: 0.0189806
[05/26/2023-16:04:38] [V] [TRT] Fastest Tactic: 0x90898977fc8ce537 Time: 0.0122347
[05/26/2023-16:04:38] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x90898977fc8ce537
[05/26/2023-16:04:38] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16) -> Float(200704,1:4,3584,64) ***************
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/downsample/downsample.0/Conv (CudaDepthwiseConvolution)
[05/26/2023-16:04:38] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/downsample/downsample.0/Conv (CublasConvolution)
[05/26/2023-16:04:38] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/downsample/downsample.0/Conv (CaskGemmConvolution)
[05/26/2023-16:04:38] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/downsample/downsample.0/Conv (CaskFlattenConvolution)
[05/26/2023-16:04:38] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/downsample/downsample.0/Conv (CaskConvolution)
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x9dece0dc37e90462
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x9dece0dc37e90462 Time: 0.0106377
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x130df49cb195156b
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x130df49cb195156b Time: 0.0107716
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 0.0106567
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 0.0107958
[05/26/2023-16:04:38] [V] [TRT] Fastest Tactic: 0x9dece0dc37e90462 Time: 0.0106377
[05/26/2023-16:04:38] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9dece0dc37e90462
[05/26/2023-16:04:38] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:38] [V] [TRT] *************** Autotuning format combination: Float(200704,3136,56,1), Float(802816,3136,56,1) -> Float(802816,3136,56,1) ***************
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu (CudnnConvolution)
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.039936
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.0321536
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000002 Time: 0.0671314
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000004 Time: 1.67673
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000005 Time: 0.0729097
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000038 Time: 0.0398697
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000039 Time: 0.0321243
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000000003a Time: 0.0671939
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000000003c Time: 1.676
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000000003d Time: 0.0729211
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000070 Time: 0.0398697
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000071 Time: 0.0379977
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000072 Time: 0.0671086
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000074 Time: 1.67658
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000075 Time: 0.0730926
[05/26/2023-16:04:38] [V] [TRT] Fastest Tactic: 0x0000000000000039 Time: 0.0321243
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu (CublasConvolution)
[05/26/2023-16:04:38] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu (CaskGemmConvolution)
[05/26/2023-16:04:38] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:38] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu (CaskConvolution)
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 0.0226155
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 0.0201143
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: 0xa8ef60e712f8ad24
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xa8ef60e712f8ad24 Time: 0.0193309
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xc3cf6e1d1c6aff27
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xc3cf6e1d1c6aff27 Time: 0.0190537
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 0.0194863
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x9808072e706def96
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x9808072e706def96 Time: 0.0220056
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xcb8a43f748d8a338 Time: 0.0237192
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 0.0205688
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xa31d27de74b895ff Time: 0.0187154
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfff46c7893896eb1
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xfff46c7893896eb1 Time: 0.0492023
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x40a12e3938221818 Time: 0.031861
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x7f0145cb49517338
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x7f0145cb49517338 Time: 0.0428617
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa419b3b68f2da07b
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xa419b3b68f2da07b Time: 0.0432583
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9cd5cdc35441c505
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x9cd5cdc35441c505 Time: 0.019596
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fc87d7eb370bb7a
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x1fc87d7eb370bb7a Time: 0.01874
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x12dbf7d94ee3696d Time: 0.0200269
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xe5603263b7f00303
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xe5603263b7f00303 Time: 0.0202783
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x828d0ea88c66fce7
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x828d0ea88c66fce7 Time: 0.0197857
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9de226a0c44627c4
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x9de226a0c44627c4 Time: 0.0479817
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xa9366041633a5135 Time: 0.0242705
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x8e3884f0eaec3ecd
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x8e3884f0eaec3ecd Time: 0.0201663
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xbb8c3889c7eacd30 Time: 0.0549547
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xb0bf940d5e0f9f45 Time: 0.020689
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xc0b05b61d128e46e
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xc0b05b61d128e46e Time: 0.0193349
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x9d9fdb5fd9945f64 Time: 0.0222772
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x5aa723e0481da855 Time: 0.0204101
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x2ee10e11d6651675
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x2ee10e11d6651675 Time: 0.0483718
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x865894c4635db7fd
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x865894c4635db7fd Time: 0.0186171
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90f8f2915f87ed77
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x90f8f2915f87ed77 Time: 0.0203006
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 0.01984
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 0.0194886
[05/26/2023-16:04:38] [V] [TRT] Fastest Tactic: 0x865894c4635db7fd Time: 0.0186171
[05/26/2023-16:04:38] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x865894c4635db7fd
[05/26/2023-16:04:38] [V] [TRT] *************** Autotuning format combination: Float(200704,1,3584,64), Float(802816,1,14336,256) -> Float(802816,1,14336,256) ***************
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu (CublasConvolution)
[05/26/2023-16:04:38] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:38] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu (CaskConvolution)
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0x17173deba0b64484
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x17173deba0b64484 Time: 0.0220036
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x3e2b881168d9689d Time: 0.0239177
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xf90060ce8193b811 Time: 0.0219847
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xd9031472c05adf51 Time: 0.0222981
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xe47307053a42b3e4
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xe47307053a42b3e4 Time: 0.0217104
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x412c44dfeaf9161d Time: 0.0206256
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x27b316f52c109002
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x27b316f52c109002 Time: 0.0205636
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xae0c89d047932ba3
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xae0c89d047932ba3 Time: 0.0195051
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x7bc32c782b800c48 Time: 0.0220264
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x35f26f9c09557d86
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x35f26f9c09557d86 Time: 0.0194714
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xc7b3afceb5fb03c0
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xc7b3afceb5fb03c0 Time: 0.0192549
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x7121ec1db3f80c67 Time: 0.01792
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x5030121339a48bf3 Time: 0.0224026
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd9eb6ca56ddc3a22
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xd9eb6ca56ddc3a22 Time: 0.0179617
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xbc0bba0ff1a92939
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xbc0bba0ff1a92939 Time: 0.0408869
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 0.0179989
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0a143be7a52f301a Time: 0.0191817
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x62835fce994f06dd Time: 0.0180663
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xc7feb33970feefa7
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xc7feb33970feefa7 Time: 0.0234214
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x1022069e6f8d9aeb
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x1022069e6f8d9aeb Time: 0.0193331
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90898977fc8ce537
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x90898977fc8ce537 Time: 0.017695
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x92ed3100c35fc43e Time: 0.021504
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x4fd3c46622e98342 Time: 0.0229669
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd55ee6fd0b56f808
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xd55ee6fd0b56f808 Time: 0.019712
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x1da91d865428f237 Time: 0.0194206
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xa6448a1e79f1ca6f Time: 0.0193966
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fb90698107bb33a
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x1fb90698107bb33a Time: 0.0174354
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x55d80c17b1cd982d
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x55d80c17b1cd982d Time: 0.0178966
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x3e191488237fab8f Time: 0.0240396
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x8014228ec08b4d49 Time: 0.0449577
[05/26/2023-16:04:38] [V] [TRT] Fastest Tactic: 0x1fb90698107bb33a Time: 0.0174354
[05/26/2023-16:04:38] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1fb90698107bb33a
[05/26/2023-16:04:38] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16), Float(200704,1:4,3584,64) -> Float(200704,1:4,3584,64) ***************
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu (CublasConvolution)
[05/26/2023-16:04:38] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu (CaskGemmConvolution)
[05/26/2023-16:04:38] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:38] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu (CaskConvolution)
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x9dece0dc37e90462
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x9dece0dc37e90462 Time: 0.0167771
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x130df49cb195156b
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x130df49cb195156b Time: 0.0169712
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 0.0169031
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 0.0170504
[05/26/2023-16:04:38] [V] [TRT] Fastest Tactic: 0x9dece0dc37e90462 Time: 0.0167771
[05/26/2023-16:04:38] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9dece0dc37e90462
[05/26/2023-16:04:38] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:38] [V] [TRT] *************** Autotuning format combination: Float(802816,3136,56,1) -> Float(200704,3136,56,1) ***************
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu (CudaDepthwiseConvolution)
[05/26/2023-16:04:38] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu (FusedConvActConvolution)
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000008ffff Time: 0.0609036
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000009ffff Time: 0.0382903
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000000bffff Time: 0.0241615
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000000cffff Time: 0.0317211
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000011ffff Time: 0.0173973
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000018ffff Time: 0.0224444
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000001bffff Time: 0.0261851
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000001fffff Time: 0.0219011
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000002effff Time: 0.0273973
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000035ffff Time: 0.0210011
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000003cffff Time: 0.0221518
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000003dffff Time: 0.0290807
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000040ffff Time: 0.019072
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000041ffff Time: 0.0209189
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000045ffff Time: 0.0207739
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000046ffff Time: 0.0181069
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000004affff Time: 0.0204382
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000004effff Time: 0.0185783
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000051ffff Time: 0.0220264
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000055ffff Time: 0.0290523
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000059ffff Time: 0.0183794
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000005effff Time: 0.0211278
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000005fffff Time: 0.0174187
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000061ffff Time: 0.0260693
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000066ffff Time: 0.0227781
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000006affff Time: 0.0219847
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000006bffff Time: 0.0206681
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000006fffff Time: 0.0192
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000070ffff Time: 0.0204382
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000073ffff Time: 0.0221309
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000075ffff Time: 0.0228036
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000007cffff Time: 0.0230433
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000007dffff Time: 0.0205414
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000007effff Time: 0.0246979
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000083ffff Time: 0.0235102
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000008affff Time: 0.0223543
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000091ffff Time: 0.0203931
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000009dffff Time: 0.0184337
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000a3ffff Time: 0.021856
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000a6ffff Time: 0.0269326
[05/26/2023-16:04:38] [V] [TRT] Fastest Tactic: 0x000000000011ffff Time: 0.0173973
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu (CudnnConvolution)
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0226743
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.0143506
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000002 Time: 0.14336
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000004 Time: 1.66196
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000005 Time: 0.0710949
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000038 Time: 0.0226952
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000039 Time: 0.0143653
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000000003a Time: 0.143415
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000000003c Time: 1.66064
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000000003d Time: 0.0711406
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000070 Time: 0.0226952
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000071 Time: 0.0226743
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000072 Time: 0.14336
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000074 Time: 1.66195
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000075 Time: 0.0708008
[05/26/2023-16:04:38] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 0.0143506
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu (CublasConvolution)
[05/26/2023-16:04:38] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu (CaskGemmConvolution)
[05/26/2023-16:04:38] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:38] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu (CaskConvolution)
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 0.0327095
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 0.0275825
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: 0xa8ef60e712f8ad24
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xa8ef60e712f8ad24 Time: 0.0330606
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xc3cf6e1d1c6aff27
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xc3cf6e1d1c6aff27 Time: 0.0249905
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 0.0336073
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x9808072e706def96
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x9808072e706def96 Time: 0.0316599
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xcb8a43f748d8a338 Time: 0.0237793
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 0.0236983
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xa31d27de74b895ff Time: 0.0141764
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfff46c7893896eb1
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xfff46c7893896eb1 Time: 0.0587581
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x40a12e3938221818 Time: 0.0353115
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x7f0145cb49517338
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x7f0145cb49517338 Time: 0.0321893
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa419b3b68f2da07b
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xa419b3b68f2da07b Time: 0.0312686
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9cd5cdc35441c505
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x9cd5cdc35441c505 Time: 0.0206309
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fc87d7eb370bb7a
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x1fc87d7eb370bb7a Time: 0.0146153
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x12dbf7d94ee3696d Time: 0.0236108
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xe5603263b7f00303
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xe5603263b7f00303 Time: 0.0235102
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x828d0ea88c66fce7
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x828d0ea88c66fce7 Time: 0.0161224
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9de226a0c44627c4
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x9de226a0c44627c4 Time: 0.0392777
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xa9366041633a5135 Time: 0.0302336
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x8e3884f0eaec3ecd
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x8e3884f0eaec3ecd Time: 0.022257
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xbb8c3889c7eacd30 Time: 0.061245
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xb0bf940d5e0f9f45 Time: 0.0158872
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xc0b05b61d128e46e
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xc0b05b61d128e46e Time: 0.0210586
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x9d9fdb5fd9945f64 Time: 0.0176711
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x5aa723e0481da855 Time: 0.021295
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x2ee10e11d6651675
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x2ee10e11d6651675 Time: 0.0406674
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x865894c4635db7fd
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x865894c4635db7fd Time: 0.0145253
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90f8f2915f87ed77
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x90f8f2915f87ed77 Time: 0.0155995
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 0.0267695
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 0.0333477
[05/26/2023-16:04:38] [V] [TRT] Fastest Tactic: 0xa31d27de74b895ff Time: 0.0141764
[05/26/2023-16:04:38] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:38] [V] [TRT] *************** Autotuning format combination: Float(802816,1,14336,256) -> Float(200704,1,3584,64) ***************
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu (CublasConvolution)
[05/26/2023-16:04:38] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:38] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu (CaskConvolution)
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0x17173deba0b64484
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x17173deba0b64484 Time: 0.0330898
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x3e2b881168d9689d Time: 0.0140305
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xf90060ce8193b811 Time: 0.0315867
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xd9031472c05adf51 Time: 0.0327095
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xe47307053a42b3e4
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xe47307053a42b3e4 Time: 0.0312759
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x412c44dfeaf9161d Time: 0.0208013
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x27b316f52c109002
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x27b316f52c109002 Time: 0.0211487
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xae0c89d047932ba3
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xae0c89d047932ba3 Time: 0.0189074
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x7bc32c782b800c48 Time: 0.031296
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x35f26f9c09557d86
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x35f26f9c09557d86 Time: 0.0199109
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xc7b3afceb5fb03c0
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xc7b3afceb5fb03c0 Time: 0.019236
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x7121ec1db3f80c67 Time: 0.0143017
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x5030121339a48bf3 Time: 0.0334345
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd9eb6ca56ddc3a22
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xd9eb6ca56ddc3a22 Time: 0.014112
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xbc0bba0ff1a92939
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xbc0bba0ff1a92939 Time: 0.0350647
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 0.014624
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0a143be7a52f301a Time: 0.016579
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x62835fce994f06dd Time: 0.0136087
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xc7feb33970feefa7
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xc7feb33970feefa7 Time: 0.0136578
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x1022069e6f8d9aeb
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x1022069e6f8d9aeb Time: 0.0195657
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90898977fc8ce537
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x90898977fc8ce537 Time: 0.0131532
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x92ed3100c35fc43e Time: 0.0172622
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x4fd3c46622e98342 Time: 0.0224065
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd55ee6fd0b56f808
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xd55ee6fd0b56f808 Time: 0.0195171
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x1da91d865428f237 Time: 0.020898
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xa6448a1e79f1ca6f Time: 0.017378
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fb90698107bb33a
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x1fb90698107bb33a Time: 0.0141964
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x55d80c17b1cd982d
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x55d80c17b1cd982d Time: 0.0134579
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x3e191488237fab8f Time: 0.0142038
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x8014228ec08b4d49 Time: 0.0354011
[05/26/2023-16:04:38] [V] [TRT] Fastest Tactic: 0x90898977fc8ce537 Time: 0.0131532
[05/26/2023-16:04:38] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x90898977fc8ce537
[05/26/2023-16:04:38] [V] [TRT] *************** Autotuning format combination: Float(200704,1:4,3584,64) -> Float(50176,1:4,896,16) ***************
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu (CudaDepthwiseConvolution)
[05/26/2023-16:04:38] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu (CublasConvolution)
[05/26/2023-16:04:38] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu (CaskGemmConvolution)
[05/26/2023-16:04:38] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:38] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu (CaskConvolution)
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x9dece0dc37e90462
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x9dece0dc37e90462 Time: 0.0223321
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x130df49cb195156b
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x130df49cb195156b Time: 0.0225816
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 0.0235148
[05/26/2023-16:04:38] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 0.0229433
[05/26/2023-16:04:38] [V] [TRT] Fastest Tactic: 0x9dece0dc37e90462 Time: 0.0223321
[05/26/2023-16:04:38] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9dece0dc37e90462
[05/26/2023-16:04:38] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:38] [V] [TRT] *************** Autotuning format combination: Float(200704,3136,56,1) -> Float(200704,3136,56,1) ***************
[05/26/2023-16:04:38] [V] [TRT] *************** Autotuning format combination: Float(200704,1,3584,64) -> Float(200704,1,3584,64) ***************
[05/26/2023-16:04:38] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16) -> Float(50176,1:4,896,16) ***************
[05/26/2023-16:04:38] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:38] [V] [TRT] *************** Autotuning format combination: Float(200704,3136,56,1), Float(802816,3136,56,1) -> Float(802816,3136,56,1) ***************
[05/26/2023-16:04:38] [V] [TRT] *************** Autotuning format combination: Float(200704,1,3584,64), Float(802816,1,14336,256) -> Float(802816,1,14336,256) ***************
[05/26/2023-16:04:38] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16), Float(200704,1:4,3584,64) -> Float(200704,1:4,3584,64) ***************
[05/26/2023-16:04:38] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:38] [V] [TRT] *************** Autotuning format combination: Float(802816,3136,56,1) -> Float(200704,3136,56,1) ***************
[05/26/2023-16:04:38] [V] [TRT] *************** Autotuning format combination: Float(802816,1,14336,256) -> Float(200704,1,3584,64) ***************
[05/26/2023-16:04:38] [V] [TRT] *************** Autotuning format combination: Float(200704,1:4,3584,64) -> Float(50176,1:4,896,16) ***************
[05/26/2023-16:04:38] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:38] [V] [TRT] *************** Autotuning format combination: Float(200704,3136,56,1) -> Float(200704,3136,56,1) ***************
[05/26/2023-16:04:38] [V] [TRT] *************** Autotuning format combination: Float(200704,1,3584,64) -> Float(200704,1,3584,64) ***************
[05/26/2023-16:04:38] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16) -> Float(50176,1:4,896,16) ***************
[05/26/2023-16:04:38] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:38] [V] [TRT] *************** Autotuning format combination: Float(200704,3136,56,1), Float(802816,3136,56,1) -> Float(802816,3136,56,1) ***************
[05/26/2023-16:04:38] [V] [TRT] *************** Autotuning format combination: Float(200704,1,3584,64), Float(802816,1,14336,256) -> Float(802816,1,14336,256) ***************
[05/26/2023-16:04:38] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16), Float(200704,1:4,3584,64) -> Float(200704,1:4,3584,64) ***************
[05/26/2023-16:04:38] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:38] [V] [TRT] *************** Autotuning format combination: Float(802816,3136,56,1) -> Float(401408,3136,56,1) ***************
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu (CudaDepthwiseConvolution)
[05/26/2023-16:04:38] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu (FusedConvActConvolution)
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000008ffff Time: 0.0680229
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000009ffff Time: 0.0476907
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000000bffff Time: 0.0393874
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000000cffff Time: 0.0415577
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000011ffff Time: 0.0333285
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000018ffff Time: 0.0365349
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000001bffff Time: 0.0397497
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000001fffff Time: 0.0390583
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000002effff Time: 0.0358034
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000035ffff Time: 0.0400457
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000003cffff Time: 0.0384446
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000003dffff Time: 0.0392411
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000040ffff Time: 0.0339072
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000041ffff Time: 0.0349038
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000045ffff Time: 0.0358034
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000046ffff Time: 0.0326208
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000004affff Time: 0.0364514
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000004effff Time: 0.0369737
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000051ffff Time: 0.0453314
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000055ffff Time: 0.0392777
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000059ffff Time: 0.0339383
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000005effff Time: 0.0341221
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000005fffff Time: 0.0332946
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000061ffff Time: 0.0343186
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000066ffff Time: 0.0448731
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000006affff Time: 0.038912
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000006bffff Time: 0.0316919
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000006fffff Time: 0.0351314
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000070ffff Time: 0.0380709
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000073ffff Time: 0.0417246
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000075ffff Time: 0.0322999
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000007cffff Time: 0.0443246
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000007dffff Time: 0.0369737
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000007effff Time: 0.045968
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000083ffff Time: 0.04576
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000008affff Time: 0.0388046
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000091ffff Time: 0.0364617
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x00000000009dffff Time: 0.0336165
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000a3ffff Time: 0.0309129
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000a6ffff Time: 0.0374937
[05/26/2023-16:04:38] [V] [TRT] Fastest Tactic: 0x0000000000a3ffff Time: 0.0309129
[05/26/2023-16:04:38] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu (CudnnConvolution)
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0324754
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.0202629
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000002 Time: 0.148896
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000004 Time: 3.25749
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000005 Time: 0.0770926
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000038 Time: 0.032651
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000039 Time: 0.0203937
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000000003a Time: 0.149211
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000000003c Time: 3.25647
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x000000000000003d Time: 0.0768
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000070 Time: 0.0324544
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000071 Time: 0.0323666
[05/26/2023-16:04:38] [V] [TRT] Tactic: 0x0000000000000072 Time: 0.148773
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000074 Time: 3.25632
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000075 Time: 0.0771657
[05/26/2023-16:04:39] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 0.0202629
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu (CublasConvolution)
[05/26/2023-16:04:39] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu (CaskGemmConvolution)
[05/26/2023-16:04:39] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:39] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu (CaskConvolution)
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 0.0351963
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 0.027965
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: 0xa8ef60e712f8ad24
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xa8ef60e712f8ad24 Time: 0.0333824
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xc3cf6e1d1c6aff27
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xc3cf6e1d1c6aff27 Time: 0.0253768
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 0.0340224
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x9808072e706def96
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x9808072e706def96 Time: 0.0326217
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xcb8a43f748d8a338 Time: 0.0306907
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 0.0330313
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xa31d27de74b895ff Time: 0.0223817
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfff46c7893896eb1
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xfff46c7893896eb1 Time: 0.0598811
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x40a12e3938221818 Time: 0.0434469
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x7f0145cb49517338
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x7f0145cb49517338 Time: 0.0325358
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa419b3b68f2da07b
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xa419b3b68f2da07b Time: 0.0314807
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9cd5cdc35441c505
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x9cd5cdc35441c505 Time: 0.0209985
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fc87d7eb370bb7a
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x1fc87d7eb370bb7a Time: 0.0223347
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x12dbf7d94ee3696d Time: 0.023872
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xe5603263b7f00303
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xe5603263b7f00303 Time: 0.0236147
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x828d0ea88c66fce7
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x828d0ea88c66fce7 Time: 0.024835
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9de226a0c44627c4
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x9de226a0c44627c4 Time: 0.0393143
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xa9366041633a5135 Time: 0.0411429
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x8e3884f0eaec3ecd
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x8e3884f0eaec3ecd Time: 0.0225358
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xbb8c3889c7eacd30 Time: 0.065731
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xb0bf940d5e0f9f45 Time: 0.0237198
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xc0b05b61d128e46e
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xc0b05b61d128e46e Time: 0.0215667
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x9d9fdb5fd9945f64 Time: 0.0259657
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x5aa723e0481da855 Time: 0.0217241
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x2ee10e11d6651675
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x2ee10e11d6651675 Time: 0.0407406
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x865894c4635db7fd
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x865894c4635db7fd Time: 0.0225698
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90f8f2915f87ed77
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x90f8f2915f87ed77 Time: 0.0234743
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 0.0269074
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 0.0333906
[05/26/2023-16:04:39] [V] [TRT] Fastest Tactic: 0x9cd5cdc35441c505 Time: 0.0209985
[05/26/2023-16:04:39] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000001
[05/26/2023-16:04:39] [V] [TRT] *************** Autotuning format combination: Float(802816,1,14336,256) -> Float(401408,1,7168,128) ***************
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu (CublasConvolution)
[05/26/2023-16:04:39] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:39] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu (CaskConvolution)
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0x17173deba0b64484
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x17173deba0b64484 Time: 0.0336658
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x3e2b881168d9689d Time: 0.0231634
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xf90060ce8193b811 Time: 0.0320073
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xd9031472c05adf51 Time: 0.033792
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xe47307053a42b3e4
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xe47307053a42b3e4 Time: 0.031595
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x412c44dfeaf9161d Time: 0.0211265
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x27b316f52c109002
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x27b316f52c109002 Time: 0.0214165
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xae0c89d047932ba3
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xae0c89d047932ba3 Time: 0.0192
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x7bc32c782b800c48 Time: 0.031627
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x35f26f9c09557d86
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x35f26f9c09557d86 Time: 0.0200954
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xc7b3afceb5fb03c0
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xc7b3afceb5fb03c0 Time: 0.0200411
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x7121ec1db3f80c67 Time: 0.0238028
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x5030121339a48bf3 Time: 0.0341431
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd9eb6ca56ddc3a22
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xd9eb6ca56ddc3a22 Time: 0.0229061
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xbc0bba0ff1a92939
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xbc0bba0ff1a92939 Time: 0.0351086
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 0.022946
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0a143be7a52f301a Time: 0.0265752
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x62835fce994f06dd Time: 0.0215458
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xc7feb33970feefa7
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xc7feb33970feefa7 Time: 0.0227997
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x1022069e6f8d9aeb
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x1022069e6f8d9aeb Time: 0.0202971
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90898977fc8ce537
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x90898977fc8ce537 Time: 0.0211069
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x92ed3100c35fc43e Time: 0.0248229
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x4fd3c46622e98342 Time: 0.0301934
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd55ee6fd0b56f808
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xd55ee6fd0b56f808 Time: 0.0198091
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x1da91d865428f237 Time: 0.021086
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xa6448a1e79f1ca6f Time: 0.0270583
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fb90698107bb33a
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x1fb90698107bb33a Time: 0.0236356
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x55d80c17b1cd982d
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x55d80c17b1cd982d Time: 0.0213786
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x3e191488237fab8f Time: 0.0234834
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x8014228ec08b4d49 Time: 0.0354597
[05/26/2023-16:04:39] [V] [TRT] Fastest Tactic: 0xae0c89d047932ba3 Time: 0.0192
[05/26/2023-16:04:39] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xae0c89d047932ba3
[05/26/2023-16:04:39] [V] [TRT] *************** Autotuning format combination: Float(200704,1:4,3584,64) -> Float(100352,1:4,1792,32) ***************
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu (CudaDepthwiseConvolution)
[05/26/2023-16:04:39] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu (CublasConvolution)
[05/26/2023-16:04:39] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu (CaskGemmConvolution)
[05/26/2023-16:04:39] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:39] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu (CaskConvolution)
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x9dece0dc37e90462
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x9dece0dc37e90462 Time: 0.0227788
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x130df49cb195156b
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x130df49cb195156b Time: 0.0229878
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 0.0237192
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 0.0232366
[05/26/2023-16:04:39] [V] [TRT] Fastest Tactic: 0x9dece0dc37e90462 Time: 0.0227788
[05/26/2023-16:04:39] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9dece0dc37e90462
[05/26/2023-16:04:39] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:39] [V] [TRT] *************** Autotuning format combination: Float(401408,3136,56,1) -> Float(100352,784,28,1) ***************
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu (CudaDepthwiseConvolution)
[05/26/2023-16:04:39] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu (FusedConvActConvolution)
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000006ffff Time: 0.0669501
[05/26/2023-16:04:39] [V] [TRT] Fastest Tactic: 0x000000000006ffff Time: 0.0669501
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu (CudnnConvolution)
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.128146
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.036608
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000002 Time: 0.161646
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000005 Time: 0.570807
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000038 Time: 0.128146
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000039 Time: 0.0366811
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000000003a Time: 0.161646
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000000003d Time: 0.571493
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000070 Time: 0.128073
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000071 Time: 0.131657
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000072 Time: 0.161646
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000075 Time: 0.571538
[05/26/2023-16:04:39] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 0.036608
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu (CaskGemmConvolution)
[05/26/2023-16:04:39] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:39] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu (CaskConvolution)
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 0.124343
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x5deb29b7a8e275f7 Time: 0.100645
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 0.109275
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 0.126976
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 0x5403ad713f811a18
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x5403ad713f811a18 Time: 0.128789
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xcb8a43f748d8a338 Time: 0.0808229
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 0.0925989
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xa31d27de74b895ff Time: 0.0472229
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x40a12e3938221818 Time: 0.114395
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x12dbf7d94ee3696d Time: 0.0797989
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xa9366041633a5135 Time: 0.122439
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xbb8c3889c7eacd30 Time: 0.230546
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xb0bf940d5e0f9f45 Time: 0.0375223
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x4727434768e46395 Time: 0.0465189
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x9d9fdb5fd9945f64 Time: 0.0463851
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x5aa723e0481da855 Time: 0.0702659
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x4efce38acc876f5c Time: 0.228352
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x8ad32616b1424be4 Time: 0.037592
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x01cf8ce2da913006 Time: 0.0702659
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 0.0985234
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 0.127538
[05/26/2023-16:04:39] [V] [TRT] Fastest Tactic: 0xb0bf940d5e0f9f45 Time: 0.0375223
[05/26/2023-16:04:39] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000001
[05/26/2023-16:04:39] [V] [TRT] *************** Autotuning format combination: Float(401408,1,7168,128) -> Float(100352,1,3584,128) ***************
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:39] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu (CaskConvolution)
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x3e2b881168d9689d Time: 0.0415394
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xf90060ce8193b811 Time: 0.122441
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xd9031472c05adf51 Time: 0.13256
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xbdfdef6b84f7ccc9 Time: 0.0738537
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x412c44dfeaf9161d Time: 0.0748251
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x27b316f52c109002
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x27b316f52c109002 Time: 0.0751177
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x7bc32c782b800c48 Time: 0.121115
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x7121ec1db3f80c67 Time: 0.0336247
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x5030121339a48bf3 Time: 0.133851
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x5953bec563d26434 Time: 0.0330898
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 0.0485669
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0a143be7a52f301a Time: 0.0412526
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x62835fce994f06dd Time: 0.0429714
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x94a7db94ba744c45 Time: 0.0428617
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x92ed3100c35fc43e Time: 0.0384069
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x32059de4888dfdda Time: 0.040704
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x4fd3c46622e98342 Time: 0.0556861
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x1da91d865428f237 Time: 0.0678766
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xa6448a1e79f1ca6f Time: 0.0598796
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x3f0c846d6379bc98 Time: 0.121856
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xf48db81f02eca9ee Time: 0.0672061
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: 0xca7eeb8d9143d738
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xca7eeb8d9143d738 Time: 0.135168
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xd15dd11d64344e83 Time: 0.0665737
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0x634e99502974e4da
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x634e99502974e4da Time: 0.123824
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x3e191488237fab8f Time: 0.0421966
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x8014228ec08b4d49 Time: 0.114174
[05/26/2023-16:04:39] [V] [TRT] Fastest Tactic: 0x5953bec563d26434 Time: 0.0330898
[05/26/2023-16:04:39] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5953bec563d26434
[05/26/2023-16:04:39] [V] [TRT] *************** Autotuning format combination: Float(100352,1:4,1792,32) -> Float(25088,1:4,896,32) ***************
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu (CudaDepthwiseConvolution)
[05/26/2023-16:04:39] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu (CaskGemmConvolution)
[05/26/2023-16:04:39] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:39] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu (CaskConvolution)
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 0.087552
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x999e005e3b016ea6 Time: 0.082944
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 0.0844069
[05/26/2023-16:04:39] [V] [TRT] Fastest Tactic: 0x999e005e3b016ea6 Time: 0.082944
[05/26/2023-16:04:39] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6
[05/26/2023-16:04:39] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:39] [V] [TRT] *************** Autotuning format combination: Float(802816,3136,56,1) -> Float(401408,784,28,1) ***************
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/downsample/downsample.0/Conv (CudaDepthwiseConvolution)
[05/26/2023-16:04:39] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/downsample/downsample.0/Conv (FusedConvActConvolution)
[05/26/2023-16:04:39] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/downsample/downsample.0/Conv (CudnnConvolution)
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0305445
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.0305234
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000002 Time: 0.13195
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000038 Time: 0.0305445
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000039 Time: 0.0305509
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000000003a Time: 0.132023
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000070 Time: 0.0305984
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000071 Time: 0.0305481
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000072 Time: 0.132096
[05/26/2023-16:04:39] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 0.0305234
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/downsample/downsample.0/Conv (CaskGemmConvolution)
[05/26/2023-16:04:39] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/downsample/downsample.0/Conv (CaskFlattenConvolution)
[05/26/2023-16:04:39] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/downsample/downsample.0/Conv (CaskConvolution)
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 0.0372297
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 0.0278918
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: 0xa8ef60e712f8ad24
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xa8ef60e712f8ad24 Time: 0.0344576
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xc3cf6e1d1c6aff27
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xc3cf6e1d1c6aff27 Time: 0.025341
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 0.0349897
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x9808072e706def96
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x9808072e706def96 Time: 0.0358766
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xcb8a43f748d8a338 Time: 0.0315255
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 0.0329225
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xa31d27de74b895ff Time: 0.0226534
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x40a12e3938221818 Time: 0.053248
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x7f0145cb49517338
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x7f0145cb49517338 Time: 0.0338213
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x12dbf7d94ee3696d Time: 0.0241859
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xe5603263b7f00303
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xe5603263b7f00303 Time: 0.0239665
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xa9366041633a5135 Time: 0.0413989
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xbb8c3889c7eacd30 Time: 0.0633448
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xb0bf940d5e0f9f45 Time: 0.0251124
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xc0b05b61d128e46e
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xc0b05b61d128e46e Time: 0.0220891
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x9d9fdb5fd9945f64 Time: 0.0340846
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x5aa723e0481da855 Time: 0.0222302
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x2ee10e11d6651675
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x2ee10e11d6651675 Time: 0.0408457
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x865894c4635db7fd
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x865894c4635db7fd Time: 0.0227755
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 0.0270141
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 0.0346112
[05/26/2023-16:04:39] [V] [TRT] Fastest Tactic: 0xc0b05b61d128e46e Time: 0.0220891
[05/26/2023-16:04:39] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc0b05b61d128e46e
[05/26/2023-16:04:39] [V] [TRT] *************** Autotuning format combination: Float(802816,1,14336,256) -> Float(401408,1,14336,512) ***************
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/downsample/downsample.0/Conv (CaskFlattenConvolution)
[05/26/2023-16:04:39] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/downsample/downsample.0/Conv (CaskConvolution)
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0x17173deba0b64484
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x17173deba0b64484 Time: 0.0337051
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x3e2b881168d9689d Time: 0.0230087
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xf90060ce8193b811 Time: 0.0318025
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xd9031472c05adf51 Time: 0.0339383
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xe47307053a42b3e4
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xe47307053a42b3e4 Time: 0.0314222
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x412c44dfeaf9161d Time: 0.0211011
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x27b316f52c109002
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x27b316f52c109002 Time: 0.0214204
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xae0c89d047932ba3
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xae0c89d047932ba3 Time: 0.0191086
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x7bc32c782b800c48 Time: 0.0314715
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x35f26f9c09557d86
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x35f26f9c09557d86 Time: 0.0202457
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x7121ec1db3f80c67 Time: 0.0241615
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x5030121339a48bf3 Time: 0.0342994
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 0.0229656
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0a143be7a52f301a Time: 0.0266636
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x62835fce994f06dd Time: 0.021713
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xc7feb33970feefa7
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xc7feb33970feefa7 Time: 0.0226325
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x1022069e6f8d9aeb
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x1022069e6f8d9aeb Time: 0.0202423
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x92ed3100c35fc43e Time: 0.0313637
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x4fd3c46622e98342 Time: 0.0383714
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x1da91d865428f237 Time: 0.0211624
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xa6448a1e79f1ca6f Time: 0.0273493
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x55d80c17b1cd982d
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x55d80c17b1cd982d Time: 0.0215033
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x3e191488237fab8f Time: 0.0234057
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x8014228ec08b4d49 Time: 0.0354862
[05/26/2023-16:04:39] [V] [TRT] Fastest Tactic: 0xae0c89d047932ba3 Time: 0.0191086
[05/26/2023-16:04:39] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xae0c89d047932ba3
[05/26/2023-16:04:39] [V] [TRT] *************** Autotuning format combination: Float(200704,1:4,3584,64) -> Float(100352,1:4,3584,128) ***************
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/downsample/downsample.0/Conv (CudaDepthwiseConvolution)
[05/26/2023-16:04:39] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/downsample/downsample.0/Conv (CaskGemmConvolution)
[05/26/2023-16:04:39] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/downsample/downsample.0/Conv (CaskFlattenConvolution)
[05/26/2023-16:04:39] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/downsample/downsample.0/Conv (CaskConvolution)
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x130df49cb195156b
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x130df49cb195156b Time: 0.022848
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 0.0236225
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 0.0231602
[05/26/2023-16:04:39] [V] [TRT] Fastest Tactic: 0x130df49cb195156b Time: 0.022848
[05/26/2023-16:04:39] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x130df49cb195156b
[05/26/2023-16:04:39] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:39] [V] [TRT] *************** Autotuning format combination: Float(100352,784,28,1), Float(401408,784,28,1) -> Float(401408,784,28,1) ***************
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu (CudnnConvolution)
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0269387
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.019616
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000002 Time: 0.0633585
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000004 Time: 0.910336
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000005 Time: 0.0797257
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000038 Time: 0.0270141
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000039 Time: 0.0196023
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000000003a Time: 0.0633905
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000000003c Time: 0.911067
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000000003d Time: 0.0803109
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000070 Time: 0.0269272
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000071 Time: 0.027843
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000072 Time: 0.0635368
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000074 Time: 0.909019
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000075 Time: 0.079872
[05/26/2023-16:04:39] [V] [TRT] Fastest Tactic: 0x0000000000000039 Time: 0.0196023
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu (CublasConvolution)
[05/26/2023-16:04:39] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu (CaskGemmConvolution)
[05/26/2023-16:04:39] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:39] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu (CaskConvolution)
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 0.0213368
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 0.0179571
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: 0xa8ef60e712f8ad24
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xa8ef60e712f8ad24 Time: 0.021039
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xc3cf6e1d1c6aff27
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xc3cf6e1d1c6aff27 Time: 0.0165613
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 0.0213571
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x9808072e706def96
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x9808072e706def96 Time: 0.020096
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xcb8a43f748d8a338 Time: 0.018664
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 0.0201783
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xa31d27de74b895ff Time: 0.0148891
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfff46c7893896eb1
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xfff46c7893896eb1 Time: 0.0423497
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x40a12e3938221818 Time: 0.0310437
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x7f0145cb49517338
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x7f0145cb49517338 Time: 0.0282331
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa419b3b68f2da07b
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xa419b3b68f2da07b Time: 0.0281844
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9cd5cdc35441c505
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x9cd5cdc35441c505 Time: 0.0153381
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fc87d7eb370bb7a
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x1fc87d7eb370bb7a Time: 0.0148197
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x12dbf7d94ee3696d Time: 0.017213
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xe5603263b7f00303
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xe5603263b7f00303 Time: 0.0170067
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x828d0ea88c66fce7
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x828d0ea88c66fce7 Time: 0.0162301
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9de226a0c44627c4
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x9de226a0c44627c4 Time: 0.0323246
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xa9366041633a5135 Time: 0.0248442
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x8e3884f0eaec3ecd
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x8e3884f0eaec3ecd Time: 0.0162702
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xbb8c3889c7eacd30 Time: 0.0441783
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xb0bf940d5e0f9f45 Time: 0.0154368
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xc0b05b61d128e46e
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xc0b05b61d128e46e Time: 0.0155941
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x9d9fdb5fd9945f64 Time: 0.020879
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x5aa723e0481da855 Time: 0.0159939
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x2ee10e11d6651675
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x2ee10e11d6651675 Time: 0.0330533
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x865894c4635db7fd
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x865894c4635db7fd Time: 0.0150171
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90f8f2915f87ed77
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x90f8f2915f87ed77 Time: 0.0151365
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 0.0174877
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 0.021295
[05/26/2023-16:04:39] [V] [TRT] Fastest Tactic: 0x1fc87d7eb370bb7a Time: 0.0148197
[05/26/2023-16:04:39] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1fc87d7eb370bb7a
[05/26/2023-16:04:39] [V] [TRT] *************** Autotuning format combination: Float(100352,1,3584,128), Float(401408,1,14336,512) -> Float(401408,1,14336,512) ***************
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu (CublasConvolution)
[05/26/2023-16:04:39] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:39] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu (CaskConvolution)
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0x17173deba0b64484
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x17173deba0b64484 Time: 0.0206054
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x3e2b881168d9689d Time: 0.0164206
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xf90060ce8193b811 Time: 0.019716
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xd9031472c05adf51 Time: 0.0208176
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xe47307053a42b3e4
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xe47307053a42b3e4 Time: 0.0195291
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x412c44dfeaf9161d Time: 0.0143173
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x27b316f52c109002
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x27b316f52c109002 Time: 0.0144667
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xae0c89d047932ba3
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xae0c89d047932ba3 Time: 0.0132958
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x7bc32c782b800c48 Time: 0.0195657
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x35f26f9c09557d86
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x35f26f9c09557d86 Time: 0.0146981
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xc7b3afceb5fb03c0
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xc7b3afceb5fb03c0 Time: 0.0140538
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x7121ec1db3f80c67 Time: 0.015024
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x5030121339a48bf3 Time: 0.0209633
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd9eb6ca56ddc3a22
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xd9eb6ca56ddc3a22 Time: 0.0150382
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xbc0bba0ff1a92939
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xbc0bba0ff1a92939 Time: 0.0289061
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 0.0149673
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0a143be7a52f301a Time: 0.0162265
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x62835fce994f06dd Time: 0.0144091
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xc7feb33970feefa7
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xc7feb33970feefa7 Time: 0.0159497
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x1022069e6f8d9aeb
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x1022069e6f8d9aeb Time: 0.0143863
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90898977fc8ce537
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x90898977fc8ce537 Time: 0.014058
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x92ed3100c35fc43e Time: 0.0188891
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x4fd3c46622e98342 Time: 0.0223608
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd55ee6fd0b56f808
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xd55ee6fd0b56f808 Time: 0.0140966
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x1da91d865428f237 Time: 0.0152832
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xa6448a1e79f1ca6f Time: 0.0171957
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fb90698107bb33a
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x1fb90698107bb33a Time: 0.0147707
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x55d80c17b1cd982d
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x55d80c17b1cd982d Time: 0.014203
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x3e191488237fab8f Time: 0.0165333
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x8014228ec08b4d49 Time: 0.0301979
[05/26/2023-16:04:39] [V] [TRT] Fastest Tactic: 0xae0c89d047932ba3 Time: 0.0132958
[05/26/2023-16:04:39] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xae0c89d047932ba3
[05/26/2023-16:04:39] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32), Float(100352,1:4,3584,128) -> Float(100352,1:4,3584,128) ***************
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu (CublasConvolution)
[05/26/2023-16:04:39] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu (CaskGemmConvolution)
[05/26/2023-16:04:39] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:39] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu (CaskConvolution)
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x9dece0dc37e90462
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x9dece0dc37e90462 Time: 0.0154336
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x130df49cb195156b
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x130df49cb195156b Time: 0.0155378
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 0.0159243
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 0.0159289
[05/26/2023-16:04:39] [V] [TRT] Fastest Tactic: 0x9dece0dc37e90462 Time: 0.0154336
[05/26/2023-16:04:39] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9dece0dc37e90462
[05/26/2023-16:04:39] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:39] [V] [TRT] *************** Autotuning format combination: Float(401408,784,28,1) -> Float(100352,784,28,1) ***************
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu (CudaDepthwiseConvolution)
[05/26/2023-16:04:39] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu (FusedConvActConvolution)
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000008ffff Time: 0.107739
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000009ffff Time: 0.0692358
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x00000000000bffff Time: 0.0300471
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x00000000000cffff Time: 0.0254964
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000011ffff Time: 0.0193646
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000018ffff Time: 0.0204506
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x00000000001bffff Time: 0.0346697
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x00000000001fffff Time: 0.0176874
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x00000000002effff Time: 0.0366811
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000035ffff Time: 0.0259093
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x00000000003cffff Time: 0.0223373
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x00000000003dffff Time: 0.0253714
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000040ffff Time: 0.01902
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000041ffff Time: 0.0242438
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000045ffff Time: 0.0222988
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000046ffff Time: 0.0202766
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x00000000004affff Time: 0.0208842
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x00000000004effff Time: 0.0186589
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000051ffff Time: 0.0224242
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000055ffff Time: 0.025405
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000059ffff Time: 0.0169564
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x00000000005effff Time: 0.0258682
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x00000000005fffff Time: 0.0193989
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000061ffff Time: 0.0354011
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000066ffff Time: 0.0235938
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x00000000006affff Time: 0.017664
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x00000000006bffff Time: 0.026499
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x00000000006fffff Time: 0.0181211
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000070ffff Time: 0.0229669
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000073ffff Time: 0.0280122
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000075ffff Time: 0.0254034
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x00000000007cffff Time: 0.0277943
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x00000000007dffff Time: 0.0207099
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x00000000007effff Time: 0.0284236
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000083ffff Time: 0.0277455
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x00000000008affff Time: 0.017443
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000091ffff Time: 0.0208934
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x00000000009dffff Time: 0.0191634
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000a3ffff Time: 0.0240823
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000a6ffff Time: 0.0386194
[05/26/2023-16:04:39] [V] [TRT] Fastest Tactic: 0x000000000059ffff Time: 0.0169564
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu (CudnnConvolution)
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0262583
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.0177006
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000002 Time: 0.119077
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000004 Time: 0.897317
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000005 Time: 0.102693
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000038 Time: 0.0262583
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000039 Time: 0.0176193
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000000003a Time: 0.119077
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000000003c Time: 0.895854
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000000003d Time: 0.102693
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000070 Time: 0.0262827
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000071 Time: 0.0276236
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000072 Time: 0.11915
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000074 Time: 0.897024
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000075 Time: 0.102569
[05/26/2023-16:04:39] [V] [TRT] Fastest Tactic: 0x0000000000000039 Time: 0.0176193
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu (CublasConvolution)
[05/26/2023-16:04:39] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu (CaskGemmConvolution)
[05/26/2023-16:04:39] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:39] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu (CaskConvolution)
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 0.059392
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 0.0492008
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: 0xa8ef60e712f8ad24
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xa8ef60e712f8ad24 Time: 0.0592945
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xc3cf6e1d1c6aff27
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xc3cf6e1d1c6aff27 Time: 0.0437394
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 0.0601234
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x9808072e706def96
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x9808072e706def96 Time: 0.0572846
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xcb8a43f748d8a338 Time: 0.0370411
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 0.0426057
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xa31d27de74b895ff Time: 0.0233502
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfff46c7893896eb1
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xfff46c7893896eb1 Time: 0.106569
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x40a12e3938221818 Time: 0.0509562
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x7f0145cb49517338
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x7f0145cb49517338 Time: 0.0523703
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa419b3b68f2da07b
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xa419b3b68f2da07b Time: 0.0514438
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9cd5cdc35441c505
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x9cd5cdc35441c505 Time: 0.0345234
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fc87d7eb370bb7a
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x1fc87d7eb370bb7a Time: 0.0243406
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x12dbf7d94ee3696d Time: 0.0393143
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xe5603263b7f00303
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xe5603263b7f00303 Time: 0.0390171
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x828d0ea88c66fce7
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x828d0ea88c66fce7 Time: 0.0275749
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9de226a0c44627c4
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x9de226a0c44627c4 Time: 0.0659749
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xa9366041633a5135 Time: 0.0555886
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x8e3884f0eaec3ecd
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x8e3884f0eaec3ecd Time: 0.0378514
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xbb8c3889c7eacd30 Time: 0.109861
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xb0bf940d5e0f9f45 Time: 0.0186463
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xc0b05b61d128e46e
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xc0b05b61d128e46e Time: 0.0351378
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x9d9fdb5fd9945f64 Time: 0.0225907
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x5aa723e0481da855 Time: 0.0352722
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x2ee10e11d6651675
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x2ee10e11d6651675 Time: 0.0677303
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x865894c4635db7fd
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x865894c4635db7fd Time: 0.0240846
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90f8f2915f87ed77
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x90f8f2915f87ed77 Time: 0.018924
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 0.0472
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 0.0594728
[05/26/2023-16:04:39] [V] [TRT] Fastest Tactic: 0xb0bf940d5e0f9f45 Time: 0.0186463
[05/26/2023-16:04:39] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: FusedConvActConvolution Tactic: 0x000000000059ffff
[05/26/2023-16:04:39] [V] [TRT] *************** Autotuning format combination: Float(401408,1,14336,512) -> Float(100352,1,3584,128) ***************
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu (CublasConvolution)
[05/26/2023-16:04:39] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:39] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu (CaskConvolution)
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0x17173deba0b64484
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x17173deba0b64484 Time: 0.0615375
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x3e2b881168d9689d Time: 0.0216294
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xf90060ce8193b811 Time: 0.0577829
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xd9031472c05adf51 Time: 0.0616351
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xe47307053a42b3e4
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xe47307053a42b3e4 Time: 0.056992
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x412c44dfeaf9161d Time: 0.03584
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x27b316f52c109002
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x27b316f52c109002 Time: 0.036472
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xae0c89d047932ba3
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xae0c89d047932ba3 Time: 0.0322121
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x7bc32c782b800c48 Time: 0.0570514
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x35f26f9c09557d86
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x35f26f9c09557d86 Time: 0.0334702
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xc7b3afceb5fb03c0
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xc7b3afceb5fb03c0 Time: 0.0332069
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x7121ec1db3f80c67 Time: 0.0171967
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x5030121339a48bf3 Time: 0.06256
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd9eb6ca56ddc3a22
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xd9eb6ca56ddc3a22 Time: 0.0235729
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xbc0bba0ff1a92939
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xbc0bba0ff1a92939 Time: 0.058525
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 0.024381
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0a143be7a52f301a Time: 0.0202766
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x62835fce994f06dd Time: 0.0219964
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xc7feb33970feefa7
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xc7feb33970feefa7 Time: 0.0211768
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x1022069e6f8d9aeb
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x1022069e6f8d9aeb Time: 0.0335872
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90898977fc8ce537
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x90898977fc8ce537 Time: 0.0213394
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x92ed3100c35fc43e Time: 0.0191954
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x4fd3c46622e98342 Time: 0.0268815
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd55ee6fd0b56f808
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xd55ee6fd0b56f808 Time: 0.033056
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x1da91d865428f237 Time: 0.0346734
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xa6448a1e79f1ca6f Time: 0.0300471
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fb90698107bb33a
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x1fb90698107bb33a Time: 0.0169691
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x55d80c17b1cd982d
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x55d80c17b1cd982d Time: 0.0216503
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x3e191488237fab8f Time: 0.0219638
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x8014228ec08b4d49 Time: 0.058368
[05/26/2023-16:04:39] [V] [TRT] Fastest Tactic: 0x1fb90698107bb33a Time: 0.0169691
[05/26/2023-16:04:39] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1fb90698107bb33a
[05/26/2023-16:04:39] [V] [TRT] *************** Autotuning format combination: Float(100352,1:4,3584,128) -> Float(25088,1:4,896,32) ***************
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu (CudaDepthwiseConvolution)
[05/26/2023-16:04:39] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu (CublasConvolution)
[05/26/2023-16:04:39] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu (CaskGemmConvolution)
[05/26/2023-16:04:39] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:39] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu (CaskConvolution)
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x9dece0dc37e90462
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x9dece0dc37e90462 Time: 0.0396434
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x130df49cb195156b
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x130df49cb195156b Time: 0.0398686
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 0.0419097
[05/26/2023-16:04:39] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 0.0406309
[05/26/2023-16:04:39] [V] [TRT] Fastest Tactic: 0x9dece0dc37e90462 Time: 0.0396434
[05/26/2023-16:04:39] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9dece0dc37e90462
[05/26/2023-16:04:39] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:39] [V] [TRT] *************** Autotuning format combination: Float(100352,784,28,1) -> Float(100352,784,28,1) ***************
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu (CudaDepthwiseConvolution)
[05/26/2023-16:04:39] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu (FusedConvActConvolution)
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000007ffff Time: 0.0538865
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x00000000000affff Time: 0.044184
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x00000000000effff Time: 0.0250621
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x00000000000fffff Time: 0.0378766
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000019ffff Time: 0.0267352
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x00000000001affff Time: 0.0253074
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000024ffff Time: 0.0320759
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000027ffff Time: 0.0259657
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x00000000002dffff Time: 0.0247619
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000036ffff Time: 0.0253318
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x00000000004cffff Time: 0.0332654
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000062ffff Time: 0.0406651
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x00000000006effff Time: 0.026115
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000077ffff Time: 0.0276495
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000086ffff Time: 0.0201691
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000089ffff Time: 0.0243497
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000097ffff Time: 0.0219749
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000098ffff Time: 0.0347849
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x00000000009fffff Time: 0.0256975
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000a2ffff Time: 0.0259467
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000a4ffff Time: 0.0284038
[05/26/2023-16:04:39] [V] [TRT] Fastest Tactic: 0x000000000086ffff Time: 0.0201691
[05/26/2023-16:04:39] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu (CudnnConvolution)
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.128
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.0352256
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000002 Time: 0.134217
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000004 Time: 0.252343
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000005 Time: 0.243273
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000006 Time: 0.0292864
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000038 Time: 0.127927
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x0000000000000039 Time: 0.0351963
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000000003a Time: 0.134071
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000000003c Time: 0.252782
[05/26/2023-16:04:39] [V] [TRT] Tactic: 0x000000000000003d Time: 0.243419
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x000000000000003e Time: 0.0293157
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000070 Time: 0.127927
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000071 Time: 0.130706
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000072 Time: 0.133925
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000074 Time: 0.252928
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000075 Time: 0.242981
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000076 Time: 0.0293157
[05/26/2023-16:04:40] [V] [TRT] Fastest Tactic: 0x0000000000000006 Time: 0.0292864
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu (CaskGemmConvolution)
[05/26/2023-16:04:40] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:40] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu (CaskConvolution)
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 0.123831
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x5deb29b7a8e275f7 Time: 0.102937
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 0.112594
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 0.12683
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 0x5403ad713f811a18
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x5403ad713f811a18 Time: 0.132242
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x94119b4c514b211a Time: 0.0233639
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xcb8a43f748d8a338 Time: 0.0808229
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 0.0925257
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xa31d27de74b895ff Time: 0.0471086
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x40a12e3938221818 Time: 0.113591
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x12dbf7d94ee3696d Time: 0.0798103
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xa9366041633a5135 Time: 0.122441
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xbb8c3889c7eacd30 Time: 0.2304
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xb0bf940d5e0f9f45 Time: 0.0374434
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x4727434768e46395 Time: 0.0464457
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x9d9fdb5fd9945f64 Time: 0.0465189
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x5aa723e0481da855 Time: 0.0702141
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x4efce38acc876f5c Time: 0.227621
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x8ad32616b1424be4 Time: 0.0372594
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x01cf8ce2da913006 Time: 0.0701196
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 0.0985234
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 0.130779
[05/26/2023-16:04:40] [V] [TRT] Fastest Tactic: 0x94119b4c514b211a Time: 0.0233639
[05/26/2023-16:04:40] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: FusedConvActConvolution Tactic: 0x000000000086ffff
[05/26/2023-16:04:40] [V] [TRT] *************** Autotuning format combination: Float(100352,1,3584,128) -> Float(100352,1,3584,128) ***************
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:40] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu (CaskConvolution)
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x3e2b881168d9689d Time: 0.0415817
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xf90060ce8193b811 Time: 0.122587
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xd9031472c05adf51 Time: 0.129536
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xbdfdef6b84f7ccc9 Time: 0.0738011
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x412c44dfeaf9161d Time: 0.0754834
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x27b316f52c109002
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x27b316f52c109002 Time: 0.0748983
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x7bc32c782b800c48 Time: 0.122149
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x7121ec1db3f80c67 Time: 0.0341723
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x5030121339a48bf3 Time: 0.13173
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x5953bec563d26434 Time: 0.0328558
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 0.048096
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0a143be7a52f301a Time: 0.0395051
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x62835fce994f06dd Time: 0.0432549
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x94a7db94ba744c45 Time: 0.0428503
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x92ed3100c35fc43e Time: 0.0384617
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x32059de4888dfdda Time: 0.0386194
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x4fd3c46622e98342 Time: 0.0555398
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x1da91d865428f237 Time: 0.0677303
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xa6448a1e79f1ca6f Time: 0.0566126
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x3f0c846d6379bc98 Time: 0.119385
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xf48db81f02eca9ee Time: 0.0667078
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: 0xca7eeb8d9143d738
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xca7eeb8d9143d738 Time: 0.132011
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xd15dd11d64344e83 Time: 0.0669013
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0x634e99502974e4da
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x634e99502974e4da Time: 0.12464
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x3e191488237fab8f Time: 0.0424149
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x8014228ec08b4d49 Time: 0.114688
[05/26/2023-16:04:40] [V] [TRT] Fastest Tactic: 0x5953bec563d26434 Time: 0.0328558
[05/26/2023-16:04:40] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5953bec563d26434
[05/26/2023-16:04:40] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32) -> Float(25088,1:4,896,32) ***************
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu (CudaDepthwiseConvolution)
[05/26/2023-16:04:40] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu (CaskGemmConvolution)
[05/26/2023-16:04:40] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:40] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu (CaskConvolution)
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 0.0879177
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x999e005e3b016ea6 Time: 0.0831634
[05/26/2023-16:04:40] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 0.0845646
[05/26/2023-16:04:40] [V] [TRT] Fastest Tactic: 0x999e005e3b016ea6 Time: 0.0831634
[05/26/2023-16:04:40] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6
[05/26/2023-16:04:40] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:40] [V] [TRT] *************** Autotuning format combination: Float(100352,784,28,1), Float(401408,784,28,1) -> Float(401408,784,28,1) ***************
[05/26/2023-16:04:40] [V] [TRT] *************** Autotuning format combination: Float(100352,1,3584,128), Float(401408,1,14336,512) -> Float(401408,1,14336,512) ***************
[05/26/2023-16:04:40] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32), Float(100352,1:4,3584,128) -> Float(100352,1:4,3584,128) ***************
[05/26/2023-16:04:40] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:40] [V] [TRT] *************** Autotuning format combination: Float(401408,784,28,1) -> Float(100352,784,28,1) ***************
[05/26/2023-16:04:40] [V] [TRT] *************** Autotuning format combination: Float(401408,1,14336,512) -> Float(100352,1,3584,128) ***************
[05/26/2023-16:04:40] [V] [TRT] *************** Autotuning format combination: Float(100352,1:4,3584,128) -> Float(25088,1:4,896,32) ***************
[05/26/2023-16:04:40] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:40] [V] [TRT] *************** Autotuning format combination: Float(100352,784,28,1) -> Float(100352,784,28,1) ***************
[05/26/2023-16:04:40] [V] [TRT] *************** Autotuning format combination: Float(100352,1,3584,128) -> Float(100352,1,3584,128) ***************
[05/26/2023-16:04:40] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32) -> Float(25088,1:4,896,32) ***************
[05/26/2023-16:04:40] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:40] [V] [TRT] *************** Autotuning format combination: Float(100352,784,28,1), Float(401408,784,28,1) -> Float(401408,784,28,1) ***************
[05/26/2023-16:04:40] [V] [TRT] *************** Autotuning format combination: Float(100352,1,3584,128), Float(401408,1,14336,512) -> Float(401408,1,14336,512) ***************
[05/26/2023-16:04:40] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32), Float(100352,1:4,3584,128) -> Float(100352,1:4,3584,128) ***************
[05/26/2023-16:04:40] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:40] [V] [TRT] *************** Autotuning format combination: Float(401408,784,28,1) -> Float(100352,784,28,1) ***************
[05/26/2023-16:04:40] [V] [TRT] *************** Autotuning format combination: Float(401408,1,14336,512) -> Float(100352,1,3584,128) ***************
[05/26/2023-16:04:40] [V] [TRT] *************** Autotuning format combination: Float(100352,1:4,3584,128) -> Float(25088,1:4,896,32) ***************
[05/26/2023-16:04:40] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:40] [V] [TRT] *************** Autotuning format combination: Float(100352,784,28,1) -> Float(100352,784,28,1) ***************
[05/26/2023-16:04:40] [V] [TRT] *************** Autotuning format combination: Float(100352,1,3584,128) -> Float(100352,1,3584,128) ***************
[05/26/2023-16:04:40] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32) -> Float(25088,1:4,896,32) ***************
[05/26/2023-16:04:40] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:40] [V] [TRT] *************** Autotuning format combination: Float(100352,784,28,1), Float(401408,784,28,1) -> Float(401408,784,28,1) ***************
[05/26/2023-16:04:40] [V] [TRT] *************** Autotuning format combination: Float(100352,1,3584,128), Float(401408,1,14336,512) -> Float(401408,1,14336,512) ***************
[05/26/2023-16:04:40] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32), Float(100352,1:4,3584,128) -> Float(100352,1:4,3584,128) ***************
[05/26/2023-16:04:40] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:40] [V] [TRT] *************** Autotuning format combination: Float(401408,784,28,1) -> Float(200704,784,28,1) ***************
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu (CudaDepthwiseConvolution)
[05/26/2023-16:04:40] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu (FusedConvActConvolution)
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x000000000008ffff Time: 0.110811
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x000000000009ffff Time: 0.0729097
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x00000000000bffff Time: 0.0371931
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x00000000000cffff Time: 0.0311067
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x000000000011ffff Time: 0.0359131
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x000000000018ffff Time: 0.0344942
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x00000000001bffff Time: 0.0416183
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x00000000001fffff Time: 0.0321536
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x00000000002effff Time: 0.0451954
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x000000000035ffff Time: 0.0341897
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x00000000003cffff Time: 0.0336165
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x00000000003dffff Time: 0.0322706
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x000000000040ffff Time: 0.0323355
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x000000000041ffff Time: 0.0318025
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x000000000045ffff Time: 0.0369337
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x000000000046ffff Time: 0.0360594
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x00000000004affff Time: 0.0308663
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x00000000004effff Time: 0.0285219
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x000000000051ffff Time: 0.0423863
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x000000000055ffff Time: 0.0323273
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x000000000059ffff Time: 0.0245051
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x00000000005effff Time: 0.0330606
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x00000000005fffff Time: 0.0358766
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x000000000061ffff Time: 0.0415086
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x000000000066ffff Time: 0.0355474
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x00000000006affff Time: 0.0320402
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x00000000006bffff Time: 0.0335579
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x00000000006fffff Time: 0.0269318
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x000000000070ffff Time: 0.0304777
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x000000000073ffff Time: 0.0369394
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x000000000075ffff Time: 0.0341696
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x00000000007cffff Time: 0.0425326
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x00000000007dffff Time: 0.0299886
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x00000000007effff Time: 0.0390949
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x000000000083ffff Time: 0.03584
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x00000000008affff Time: 0.0319781
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x000000000091ffff Time: 0.0307493
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x00000000009dffff Time: 0.0342894
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000a3ffff Time: 0.0326702
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000a6ffff Time: 0.0441943
[05/26/2023-16:04:40] [V] [TRT] Fastest Tactic: 0x000000000059ffff Time: 0.0245051
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu (CudnnConvolution)
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0362057
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.0191086
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000002 Time: 0.120466
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000004 Time: 1.75236
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000005 Time: 0.134437
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000038 Time: 0.0362057
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000039 Time: 0.0191086
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x000000000000003a Time: 0.120613
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x000000000000003c Time: 1.75236
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x000000000000003d Time: 0.134144
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000070 Time: 0.0361691
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000071 Time: 0.0342601
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000072 Time: 0.120613
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000074 Time: 1.7525
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000075 Time: 0.134437
[05/26/2023-16:04:40] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 0.0191086
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu (CublasConvolution)
[05/26/2023-16:04:40] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu (CaskGemmConvolution)
[05/26/2023-16:04:40] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:40] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu (CaskConvolution)
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 0.0595931
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 0.0492008
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: 0xa8ef60e712f8ad24
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xa8ef60e712f8ad24 Time: 0.0592823
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xc3cf6e1d1c6aff27
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xc3cf6e1d1c6aff27 Time: 0.0438126
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 0.0602545
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x9808072e706def96
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x9808072e706def96 Time: 0.0574903
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xcb8a43f748d8a338 Time: 0.0445943
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 0.0428251
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xa31d27de74b895ff Time: 0.0236062
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfff46c7893896eb1
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xfff46c7893896eb1 Time: 0.106496
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x40a12e3938221818 Time: 0.0642682
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x7f0145cb49517338
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x7f0145cb49517338 Time: 0.0520777
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa419b3b68f2da07b
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xa419b3b68f2da07b Time: 0.0512
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9cd5cdc35441c505
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x9cd5cdc35441c505 Time: 0.0345527
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fc87d7eb370bb7a
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x1fc87d7eb370bb7a Time: 0.0245722
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x12dbf7d94ee3696d Time: 0.0393086
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xe5603263b7f00303
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xe5603263b7f00303 Time: 0.0390583
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x828d0ea88c66fce7
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x828d0ea88c66fce7 Time: 0.0277722
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9de226a0c44627c4
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x9de226a0c44627c4 Time: 0.0656823
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xa9366041633a5135 Time: 0.0555383
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x8e3884f0eaec3ecd
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x8e3884f0eaec3ecd Time: 0.0378743
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xbb8c3889c7eacd30 Time: 0.109861
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xb0bf940d5e0f9f45 Time: 0.0270423
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xc0b05b61d128e46e
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xc0b05b61d128e46e Time: 0.0352466
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x9d9fdb5fd9945f64 Time: 0.0308955
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x5aa723e0481da855 Time: 0.0352914
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x2ee10e11d6651675
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x2ee10e11d6651675 Time: 0.067587
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x865894c4635db7fd
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x865894c4635db7fd Time: 0.024224
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90f8f2915f87ed77
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x90f8f2915f87ed77 Time: 0.0265265
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 0.0472194
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 0.0595383
[05/26/2023-16:04:40] [V] [TRT] Fastest Tactic: 0xa31d27de74b895ff Time: 0.0236062
[05/26/2023-16:04:40] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000001
[05/26/2023-16:04:40] [V] [TRT] *************** Autotuning format combination: Float(401408,1,14336,512) -> Float(200704,1,7168,256) ***************
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu (CublasConvolution)
[05/26/2023-16:04:40] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:40] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu (CaskConvolution)
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0x17173deba0b64484
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x17173deba0b64484 Time: 0.0614476
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x3e2b881168d9689d Time: 0.0220682
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xf90060ce8193b811 Time: 0.0576457
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xd9031472c05adf51 Time: 0.0614888
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xe47307053a42b3e4
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xe47307053a42b3e4 Time: 0.0569051
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x412c44dfeaf9161d Time: 0.0359794
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x27b316f52c109002
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x27b316f52c109002 Time: 0.0366366
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xae0c89d047932ba3
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xae0c89d047932ba3 Time: 0.0323877
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x7bc32c782b800c48 Time: 0.0568564
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x35f26f9c09557d86
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x35f26f9c09557d86 Time: 0.0330743
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xc7b3afceb5fb03c0
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xc7b3afceb5fb03c0 Time: 0.0331191
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x7121ec1db3f80c67 Time: 0.0240884
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x5030121339a48bf3 Time: 0.0623284
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd9eb6ca56ddc3a22
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xd9eb6ca56ddc3a22 Time: 0.0234893
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xbc0bba0ff1a92939
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xbc0bba0ff1a92939 Time: 0.0580754
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 0.0245013
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0a143be7a52f301a Time: 0.0288183
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x62835fce994f06dd Time: 0.022178
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xc7feb33970feefa7
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xc7feb33970feefa7 Time: 0.0216209
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x1022069e6f8d9aeb
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x1022069e6f8d9aeb Time: 0.033611
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90898977fc8ce537
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x90898977fc8ce537 Time: 0.0215014
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x92ed3100c35fc43e Time: 0.0304485
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x4fd3c46622e98342 Time: 0.0400457
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd55ee6fd0b56f808
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xd55ee6fd0b56f808 Time: 0.0327095
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x1da91d865428f237 Time: 0.0344933
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xa6448a1e79f1ca6f Time: 0.0295205
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fb90698107bb33a
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x1fb90698107bb33a Time: 0.0238392
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x55d80c17b1cd982d
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x55d80c17b1cd982d Time: 0.0217953
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x3e191488237fab8f Time: 0.0224209
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x8014228ec08b4d49 Time: 0.057344
[05/26/2023-16:04:40] [V] [TRT] Fastest Tactic: 0x90898977fc8ce537 Time: 0.0215014
[05/26/2023-16:04:40] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x90898977fc8ce537
[05/26/2023-16:04:40] [V] [TRT] *************** Autotuning format combination: Float(100352,1:4,3584,128) -> Float(50176,1:4,1792,64) ***************
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu (CudaDepthwiseConvolution)
[05/26/2023-16:04:40] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu (CublasConvolution)
[05/26/2023-16:04:40] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu (CaskGemmConvolution)
[05/26/2023-16:04:40] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:40] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu (CaskConvolution)
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x9dece0dc37e90462
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x9dece0dc37e90462 Time: 0.0396434
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x130df49cb195156b
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x130df49cb195156b Time: 0.0398263
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 0.0418377
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 0.0405211
[05/26/2023-16:04:40] [V] [TRT] Fastest Tactic: 0x9dece0dc37e90462 Time: 0.0396434
[05/26/2023-16:04:40] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9dece0dc37e90462
[05/26/2023-16:04:40] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:40] [V] [TRT] *************** Autotuning format combination: Float(200704,784,28,1) -> Float(50176,196,14,1) ***************
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu (CudaDepthwiseConvolution)
[05/26/2023-16:04:40] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu (FusedConvActConvolution)
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x000000000006ffff Time: 0.125952
[05/26/2023-16:04:40] [V] [TRT] Fastest Tactic: 0x000000000006ffff Time: 0.125952
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu (CudnnConvolution)
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.246345
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.0613912
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000002 Time: 0.230693
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000005 Time: 0.901851
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000038 Time: 0.246345
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000039 Time: 0.0613257
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x000000000000003a Time: 0.230249
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x000000000000003d Time: 0.901266
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000070 Time: 0.246345
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000071 Time: 0.244736
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000072 Time: 0.2304
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000075 Time: 0.901851
[05/26/2023-16:04:40] [V] [TRT] Fastest Tactic: 0x0000000000000039 Time: 0.0613257
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu (CaskGemmConvolution)
[05/26/2023-16:04:40] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:40] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu (CaskConvolution)
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 0.24181
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x5deb29b7a8e275f7 Time: 0.19573
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 0.212389
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 0.246784
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 0x5403ad713f811a18
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x5403ad713f811a18 Time: 0.250587
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xcb8a43f748d8a338 Time: 0.155941
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 0.181202
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xa31d27de74b895ff Time: 0.0907109
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x40a12e3938221818 Time: 0.222647
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x12dbf7d94ee3696d Time: 0.151529
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xa9366041633a5135 Time: 0.240791
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xbb8c3889c7eacd30 Time: 0.44544
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xb0bf940d5e0f9f45 Time: 0.0630674
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x4727434768e46395 Time: 0.0893897
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x9d9fdb5fd9945f64 Time: 0.0872594
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x5aa723e0481da855 Time: 0.133257
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x4efce38acc876f5c Time: 0.442075
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x8ad32616b1424be4 Time: 0.0615863
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x01cf8ce2da913006 Time: 0.133193
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 0.190706
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 0.247662
[05/26/2023-16:04:40] [V] [TRT] Fastest Tactic: 0x8ad32616b1424be4 Time: 0.0615863
[05/26/2023-16:04:40] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000039
[05/26/2023-16:04:40] [V] [TRT] *************** Autotuning format combination: Float(200704,1,7168,256) -> Float(50176,1,3584,256) ***************
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:40] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu (CaskConvolution)
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x3e2b881168d9689d Time: 0.0772389
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xf90060ce8193b811 Time: 0.238446
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xd9031472c05adf51 Time: 0.256
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xbdfdef6b84f7ccc9 Time: 0.141751
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x412c44dfeaf9161d Time: 0.143945
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x27b316f52c109002
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x27b316f52c109002 Time: 0.144091
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x7bc32c782b800c48 Time: 0.236398
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x7121ec1db3f80c67 Time: 0.0488183
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x5030121339a48bf3 Time: 0.258926
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x5953bec563d26434 Time: 0.0453611
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 0.0920869
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0a143be7a52f301a Time: 0.0656823
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x62835fce994f06dd Time: 0.0815543
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x94a7db94ba744c45 Time: 0.0808229
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x92ed3100c35fc43e Time: 0.0703634
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x32059de4888dfdda Time: 0.0763611
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x4fd3c46622e98342 Time: 0.106642
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x1da91d865428f237 Time: 0.128274
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xa6448a1e79f1ca6f Time: 0.114546
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x3f0c846d6379bc98 Time: 0.23435
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xf48db81f02eca9ee Time: 0.128
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: 0xca7eeb8d9143d738
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xca7eeb8d9143d738 Time: 0.260681
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xd15dd11d64344e83 Time: 0.127131
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0x634e99502974e4da
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x634e99502974e4da Time: 0.241664
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x3e191488237fab8f Time: 0.0784091
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x8014228ec08b4d49 Time: 0.213285
[05/26/2023-16:04:40] [V] [TRT] Fastest Tactic: 0x5953bec563d26434 Time: 0.0453611
[05/26/2023-16:04:40] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5953bec563d26434
[05/26/2023-16:04:40] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,1792,64) -> Float(12544,1:4,896,64) ***************
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu (CudaDepthwiseConvolution)
[05/26/2023-16:04:40] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu (CaskGemmConvolution)
[05/26/2023-16:04:40] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:40] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu (CaskConvolution)
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 0.17013
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x999e005e3b016ea6 Time: 0.160183
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 0.163109
[05/26/2023-16:04:40] [V] [TRT] Fastest Tactic: 0x999e005e3b016ea6 Time: 0.160183
[05/26/2023-16:04:40] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6
[05/26/2023-16:04:40] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:40] [V] [TRT] *************** Autotuning format combination: Float(401408,784,28,1) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/downsample/downsample.0/Conv (CudaDepthwiseConvolution)
[05/26/2023-16:04:40] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/downsample/downsample.0/Conv (FusedConvActConvolution)
[05/26/2023-16:04:40] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/downsample/downsample.0/Conv (CudnnConvolution)
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0366811
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.0367246
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000002 Time: 0.11381
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000038 Time: 0.0367177
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000039 Time: 0.0367909
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x000000000000003a Time: 0.113737
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000070 Time: 0.0367577
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000071 Time: 0.0366811
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000072 Time: 0.114249
[05/26/2023-16:04:40] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.0366811
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/downsample/downsample.0/Conv (CaskGemmConvolution)
[05/26/2023-16:04:40] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/downsample/downsample.0/Conv (CaskFlattenConvolution)
[05/26/2023-16:04:40] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/downsample/downsample.0/Conv (CaskConvolution)
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 0.0634392
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 0.0492983
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: 0xa8ef60e712f8ad24
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xa8ef60e712f8ad24 Time: 0.0641219
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xc3cf6e1d1c6aff27
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xc3cf6e1d1c6aff27 Time: 0.043768
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 0.0650484
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x9808072e706def96
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x9808072e706def96 Time: 0.0597349
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xcb8a43f748d8a338 Time: 0.0457874
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 0.0430297
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xa31d27de74b895ff Time: 0.0243238
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x40a12e3938221818 Time: 0.062688
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x7f0145cb49517338
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x7f0145cb49517338 Time: 0.0540312
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x12dbf7d94ee3696d Time: 0.0402286
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xe5603263b7f00303
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xe5603263b7f00303 Time: 0.0399726
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xa9366041633a5135 Time: 0.0561737
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xbb8c3889c7eacd30 Time: 0.110297
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xb0bf940d5e0f9f45 Time: 0.0310711
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xc0b05b61d128e46e
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xc0b05b61d128e46e Time: 0.0366766
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x9d9fdb5fd9945f64 Time: 0.034912
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x5aa723e0481da855 Time: 0.0367177
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x2ee10e11d6651675
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x2ee10e11d6651675 Time: 0.0680213
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x865894c4635db7fd
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x865894c4635db7fd Time: 0.0247954
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 0.04736
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 0.064512
[05/26/2023-16:04:40] [V] [TRT] Fastest Tactic: 0xa31d27de74b895ff Time: 0.0243238
[05/26/2023-16:04:40] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:40] [V] [TRT] *************** Autotuning format combination: Float(401408,1,14336,512) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/downsample/downsample.0/Conv (CaskFlattenConvolution)
[05/26/2023-16:04:40] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/downsample/downsample.0/Conv (CaskConvolution)
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0x17173deba0b64484
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x17173deba0b64484 Time: 0.06144
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x3e2b881168d9689d Time: 0.0222341
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xf90060ce8193b811 Time: 0.0575878
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xd9031472c05adf51 Time: 0.0615863
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xe47307053a42b3e4
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xe47307053a42b3e4 Time: 0.0569021
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x412c44dfeaf9161d Time: 0.0363349
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x27b316f52c109002
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x27b316f52c109002 Time: 0.0367543
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xae0c89d047932ba3
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xae0c89d047932ba3 Time: 0.0323886
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x7bc32c782b800c48 Time: 0.0567589
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x35f26f9c09557d86
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x35f26f9c09557d86 Time: 0.0337627
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x7121ec1db3f80c67 Time: 0.0299301
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x5030121339a48bf3 Time: 0.0623665
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 0.024752
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0a143be7a52f301a Time: 0.0334162
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x62835fce994f06dd Time: 0.0224875
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xc7feb33970feefa7
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xc7feb33970feefa7 Time: 0.0217966
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x1022069e6f8d9aeb
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x1022069e6f8d9aeb Time: 0.0336457
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x92ed3100c35fc43e Time: 0.0308663
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x4fd3c46622e98342 Time: 0.0414354
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x1da91d865428f237 Time: 0.0349358
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xa6448a1e79f1ca6f Time: 0.0301851
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x55d80c17b1cd982d
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x55d80c17b1cd982d Time: 0.0221447
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x3e191488237fab8f Time: 0.0224862
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x8014228ec08b4d49 Time: 0.0577341
[05/26/2023-16:04:40] [V] [TRT] Fastest Tactic: 0xc7feb33970feefa7 Time: 0.0217966
[05/26/2023-16:04:40] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc7feb33970feefa7
[05/26/2023-16:04:40] [V] [TRT] *************** Autotuning format combination: Float(100352,1:4,3584,128) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/downsample/downsample.0/Conv (CudaDepthwiseConvolution)
[05/26/2023-16:04:40] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/downsample/downsample.0/Conv (CaskGemmConvolution)
[05/26/2023-16:04:40] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/downsample/downsample.0/Conv (CaskFlattenConvolution)
[05/26/2023-16:04:40] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/downsample/downsample.0/Conv (CaskConvolution)
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x130df49cb195156b
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x130df49cb195156b Time: 0.0397977
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 0.0418743
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 0.0405577
[05/26/2023-16:04:40] [V] [TRT] Fastest Tactic: 0x130df49cb195156b Time: 0.0397977
[05/26/2023-16:04:40] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x130df49cb195156b
[05/26/2023-16:04:40] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:40] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1), Float(200704,196,14,1) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu (CudnnConvolution)
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.027648
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.0182674
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000002 Time: 0.0958903
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000004 Time: 1.05384
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000005 Time: 0.22133
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000038 Time: 0.0276236
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000039 Time: 0.0182674
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x000000000000003a Time: 0.0955977
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x000000000000003c Time: 1.05457
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x000000000000003d Time: 0.221623
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000070 Time: 0.027648
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000071 Time: 0.0275261
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000072 Time: 0.0958903
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000074 Time: 1.05428
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x0000000000000075 Time: 0.221038
[05/26/2023-16:04:40] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 0.0182674
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu (CublasConvolution)
[05/26/2023-16:04:40] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu (CaskGemmConvolution)
[05/26/2023-16:04:40] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:40] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:40] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu (CaskConvolution)
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 0.0338341
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 0.0286034
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: 0xa8ef60e712f8ad24
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xa8ef60e712f8ad24 Time: 0.0341138
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xc3cf6e1d1c6aff27
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0xc3cf6e1d1c6aff27 Time: 0.0258987
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 0.0346405
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x9808072e706def96
[05/26/2023-16:04:40] [V] [TRT] Tactic: 0x9808072e706def96 Time: 0.0326894
[05/26/2023-16:04:40] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xcb8a43f748d8a338 Time: 0.0254796
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 0.0247223
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xa31d27de74b895ff Time: 0.0153047
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfff46c7893896eb1
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xfff46c7893896eb1 Time: 0.06656
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x40a12e3938221818 Time: 0.0355182
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x7f0145cb49517338
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x7f0145cb49517338 Time: 0.0375897
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa419b3b68f2da07b
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xa419b3b68f2da07b Time: 0.0375589
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9cd5cdc35441c505
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x9cd5cdc35441c505 Time: 0.0220963
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fc87d7eb370bb7a
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x1fc87d7eb370bb7a Time: 0.0156325
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x12dbf7d94ee3696d Time: 0.0248838
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xe5603263b7f00303
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xe5603263b7f00303 Time: 0.0246171
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x828d0ea88c66fce7
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x828d0ea88c66fce7 Time: 0.016963
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9de226a0c44627c4
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x9de226a0c44627c4 Time: 0.0452023
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xa9366041633a5135 Time: 0.0315995
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x8e3884f0eaec3ecd
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x8e3884f0eaec3ecd Time: 0.0238933
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xbb8c3889c7eacd30 Time: 0.0690469
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xb0bf940d5e0f9f45 Time: 0.0173171
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xc0b05b61d128e46e
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xc0b05b61d128e46e Time: 0.0224202
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x9d9fdb5fd9945f64 Time: 0.0192771
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x5aa723e0481da855 Time: 0.022737
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x2ee10e11d6651675
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x2ee10e11d6651675 Time: 0.0462994
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x865894c4635db7fd
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x865894c4635db7fd Time: 0.0155063
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90f8f2915f87ed77
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x90f8f2915f87ed77 Time: 0.0171159
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 0.027648
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 0.0344174
[05/26/2023-16:04:41] [V] [TRT] Fastest Tactic: 0xa31d27de74b895ff Time: 0.0153047
[05/26/2023-16:04:41] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256), Float(200704,1,14336,1024) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:41] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu (CublasConvolution)
[05/26/2023-16:04:41] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:41] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:41] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:41] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu (CaskConvolution)
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0x17173deba0b64484
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x17173deba0b64484 Time: 0.0342601
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x3e2b881168d9689d Time: 0.0150112
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xf90060ce8193b811 Time: 0.0324462
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xd9031472c05adf51 Time: 0.0343186
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xe47307053a42b3e4
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xe47307053a42b3e4 Time: 0.0320951
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x412c44dfeaf9161d Time: 0.0217019
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x27b316f52c109002
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x27b316f52c109002 Time: 0.021922
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xae0c89d047932ba3
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xae0c89d047932ba3 Time: 0.0198217
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x7bc32c782b800c48 Time: 0.0321143
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x35f26f9c09557d86
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x35f26f9c09557d86 Time: 0.0214407
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xc7b3afceb5fb03c0
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xc7b3afceb5fb03c0 Time: 0.0207105
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x7121ec1db3f80c67 Time: 0.0173288
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x5030121339a48bf3 Time: 0.034816
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd9eb6ca56ddc3a22
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xd9eb6ca56ddc3a22 Time: 0.0149641
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xbc0bba0ff1a92939
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xbc0bba0ff1a92939 Time: 0.0408869
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 0.0152306
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x0a143be7a52f301a Time: 0.0192189
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x62835fce994f06dd Time: 0.0142375
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xc7feb33970feefa7
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xc7feb33970feefa7 Time: 0.0146286
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x1022069e6f8d9aeb
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x1022069e6f8d9aeb Time: 0.0210651
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90898977fc8ce537
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x90898977fc8ce537 Time: 0.0138913
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x92ed3100c35fc43e Time: 0.0180783
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x4fd3c46622e98342 Time: 0.0231412
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd55ee6fd0b56f808
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xd55ee6fd0b56f808 Time: 0.0208072
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x1da91d865428f237 Time: 0.0221505
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xa6448a1e79f1ca6f Time: 0.0181229
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fb90698107bb33a
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x1fb90698107bb33a Time: 0.0172048
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x55d80c17b1cd982d
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x55d80c17b1cd982d Time: 0.0140966
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x3e191488237fab8f Time: 0.0151141
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x8014228ec08b4d49 Time: 0.0411063
[05/26/2023-16:04:41] [V] [TRT] Fastest Tactic: 0x90898977fc8ce537 Time: 0.0138913
[05/26/2023-16:04:41] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x90898977fc8ce537
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64), Float(50176,1:4,3584,256) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:41] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu (CublasConvolution)
[05/26/2023-16:04:41] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:41] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu (CaskGemmConvolution)
[05/26/2023-16:04:41] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:41] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:41] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:41] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu (CaskConvolution)
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x9dece0dc37e90462
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x9dece0dc37e90462 Time: 0.0237192
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x130df49cb195156b
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x130df49cb195156b Time: 0.0238202
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 0.0249417
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 0.0244297
[05/26/2023-16:04:41] [V] [TRT] Fastest Tactic: 0x9dece0dc37e90462 Time: 0.0237192
[05/26/2023-16:04:41] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9dece0dc37e90462
[05/26/2023-16:04:41] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(200704,196,14,1) -> Float(50176,196,14,1) ***************
[05/26/2023-16:04:41] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu (CudaDepthwiseConvolution)
[05/26/2023-16:04:41] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:41] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu (FusedConvActConvolution)
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000008ffff Time: 0.206994
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000009ffff Time: 0.129463
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000000bffff Time: 0.0463703
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000000cffff Time: 0.0396114
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000011ffff Time: 0.0244556
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000018ffff Time: 0.0320768
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000001bffff Time: 0.058563
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000001fffff Time: 0.0188954
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000002effff Time: 0.0640244
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000035ffff Time: 0.0361429
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000003cffff Time: 0.0292571
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000003dffff Time: 0.0402754
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000040ffff Time: 0.0195657
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000041ffff Time: 0.0405577
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000045ffff Time: 0.0318903
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000046ffff Time: 0.0271124
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000004affff Time: 0.0287579
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000004effff Time: 0.0253806
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000051ffff Time: 0.0248663
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000055ffff Time: 0.0402549
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000059ffff Time: 0.0278187
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000005effff Time: 0.0359463
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000005fffff Time: 0.0244541
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000061ffff Time: 0.061312
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000066ffff Time: 0.0278187
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000006affff Time: 0.0189051
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000006bffff Time: 0.03752
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000006fffff Time: 0.0271604
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000070ffff Time: 0.0333138
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000073ffff Time: 0.043008
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000075ffff Time: 0.0416914
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000007cffff Time: 0.0393143
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000007dffff Time: 0.0292279
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000007effff Time: 0.0441783
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000083ffff Time: 0.0437874
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000008affff Time: 0.0197709
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000091ffff Time: 0.0287488
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000009dffff Time: 0.0248236
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x0000000000a3ffff Time: 0.0390446
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x0000000000a6ffff Time: 0.067779
[05/26/2023-16:04:41] [V] [TRT] Fastest Tactic: 0x00000000001fffff Time: 0.0188954
[05/26/2023-16:04:41] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu (CudnnConvolution)
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0418743
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.0205218
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x0000000000000002 Time: 0.158574
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x0000000000000004 Time: 1.05221
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x0000000000000005 Time: 0.223525
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x0000000000000038 Time: 0.0418743
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x0000000000000039 Time: 0.0195291
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000000003a Time: 0.158427
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000000003c Time: 1.05209
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000000003d Time: 0.223817
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x0000000000000070 Time: 0.0418743
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x0000000000000071 Time: 0.0408137
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x0000000000000072 Time: 0.158427
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x0000000000000074 Time: 1.05175
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x0000000000000075 Time: 0.223817
[05/26/2023-16:04:41] [V] [TRT] Fastest Tactic: 0x0000000000000039 Time: 0.0195291
[05/26/2023-16:04:41] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu (CublasConvolution)
[05/26/2023-16:04:41] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:41] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu (CaskGemmConvolution)
[05/26/2023-16:04:41] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:41] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:41] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:41] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu (CaskConvolution)
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 0.112933
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 0.0922857
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: 0xa8ef60e712f8ad24
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xa8ef60e712f8ad24 Time: 0.111936
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xc3cf6e1d1c6aff27
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xc3cf6e1d1c6aff27 Time: 0.0811863
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 0.113664
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x9808072e706def96
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x9808072e706def96 Time: 0.108837
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xcb8a43f748d8a338 Time: 0.0693394
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 0.0809691
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xa31d27de74b895ff Time: 0.0426057
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfff46c7893896eb1
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xfff46c7893896eb1 Time: 0.200265
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x40a12e3938221818 Time: 0.0964046
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x7f0145cb49517338
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x7f0145cb49517338 Time: 0.0912823
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa419b3b68f2da07b
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xa419b3b68f2da07b Time: 0.0903131
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9cd5cdc35441c505
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x9cd5cdc35441c505 Time: 0.0625615
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fc87d7eb370bb7a
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x1fc87d7eb370bb7a Time: 0.0445074
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x12dbf7d94ee3696d Time: 0.0706667
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xe5603263b7f00303
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xe5603263b7f00303 Time: 0.070173
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x828d0ea88c66fce7
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x828d0ea88c66fce7 Time: 0.0497356
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9de226a0c44627c4
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x9de226a0c44627c4 Time: 0.11965
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xa9366041633a5135 Time: 0.105993
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x8e3884f0eaec3ecd
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x8e3884f0eaec3ecd Time: 0.069155
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xbb8c3889c7eacd30 Time: 0.204069
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xb0bf940d5e0f9f45 Time: 0.0288192
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xc0b05b61d128e46e
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xc0b05b61d128e46e Time: 0.0630537
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x9d9fdb5fd9945f64 Time: 0.0402651
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x5aa723e0481da855 Time: 0.0631467
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x2ee10e11d6651675
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x2ee10e11d6651675 Time: 0.122293
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x865894c4635db7fd
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x865894c4635db7fd Time: 0.0436663
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90f8f2915f87ed77
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x90f8f2915f87ed77 Time: 0.0293157
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 0.0882103
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 0.1124
[05/26/2023-16:04:41] [V] [TRT] Fastest Tactic: 0xb0bf940d5e0f9f45 Time: 0.0288192
[05/26/2023-16:04:41] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: FusedConvActConvolution Tactic: 0x00000000001fffff
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(200704,1,14336,1024) -> Float(50176,1,3584,256) ***************
[05/26/2023-16:04:41] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu (CublasConvolution)
[05/26/2023-16:04:41] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:41] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:41] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:41] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu (CaskConvolution)
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0x17173deba0b64484
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x17173deba0b64484 Time: 0.117408
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x3e2b881168d9689d Time: 0.0375337
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xf90060ce8193b811 Time: 0.109184
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xd9031472c05adf51 Time: 0.116967
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xe47307053a42b3e4
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xe47307053a42b3e4 Time: 0.107739
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x412c44dfeaf9161d Time: 0.0666575
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x27b316f52c109002
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x27b316f52c109002 Time: 0.0672914
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xae0c89d047932ba3
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xae0c89d047932ba3 Time: 0.0590019
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x7bc32c782b800c48 Time: 0.107813
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x35f26f9c09557d86
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x35f26f9c09557d86 Time: 0.0606598
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xc7b3afceb5fb03c0
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xc7b3afceb5fb03c0 Time: 0.0604648
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x7121ec1db3f80c67 Time: 0.0239939
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x5030121339a48bf3 Time: 0.119077
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd9eb6ca56ddc3a22
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xd9eb6ca56ddc3a22 Time: 0.0424229
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xbc0bba0ff1a92939
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xbc0bba0ff1a92939 Time: 0.106496
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 0.044032
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x0a143be7a52f301a Time: 0.0310711
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x62835fce994f06dd Time: 0.0408869
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xc7feb33970feefa7
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xc7feb33970feefa7 Time: 0.0365737
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x1022069e6f8d9aeb
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x1022069e6f8d9aeb Time: 0.0608594
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90898977fc8ce537
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x90898977fc8ce537 Time: 0.0397897
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x92ed3100c35fc43e Time: 0.0333614
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x4fd3c46622e98342 Time: 0.0496884
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd55ee6fd0b56f808
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xd55ee6fd0b56f808 Time: 0.0603185
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x1da91d865428f237 Time: 0.0623573
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xa6448a1e79f1ca6f Time: 0.055491
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fb90698107bb33a
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x1fb90698107bb33a Time: 0.0231131
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x55d80c17b1cd982d
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x55d80c17b1cd982d Time: 0.0401189
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x3e191488237fab8f Time: 0.0380343
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x8014228ec08b4d49 Time: 0.103205
[05/26/2023-16:04:41] [V] [TRT] Fastest Tactic: 0x1fb90698107bb33a Time: 0.0231131
[05/26/2023-16:04:41] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1fb90698107bb33a
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,3584,256) -> Float(12544,1:4,896,64) ***************
[05/26/2023-16:04:41] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu (CudaDepthwiseConvolution)
[05/26/2023-16:04:41] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:41] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu (CublasConvolution)
[05/26/2023-16:04:41] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:41] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu (CaskGemmConvolution)
[05/26/2023-16:04:41] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:41] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:41] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:41] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu (CaskConvolution)
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x9dece0dc37e90462
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x9dece0dc37e90462 Time: 0.073728
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x130df49cb195156b
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x130df49cb195156b Time: 0.0738697
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 0.0781897
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 0.0753394
[05/26/2023-16:04:41] [V] [TRT] Fastest Tactic: 0x9dece0dc37e90462 Time: 0.073728
[05/26/2023-16:04:41] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9dece0dc37e90462
[05/26/2023-16:04:41] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1) -> Float(50176,196,14,1) ***************
[05/26/2023-16:04:41] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu (CudaDepthwiseConvolution)
[05/26/2023-16:04:41] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:41] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu (FusedConvActConvolution)
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000007ffff Time: 0.100059
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000000affff Time: 0.0792869
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000000effff Time: 0.0408503
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000000fffff Time: 0.067616
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000019ffff Time: 0.0367543
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000001affff Time: 0.0393497
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000024ffff Time: 0.0299301
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000027ffff Time: 0.0295497
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000002dffff Time: 0.0406411
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000036ffff Time: 0.0415817
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000004cffff Time: 0.057149
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000062ffff Time: 0.0741669
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000006effff Time: 0.0314514
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000077ffff Time: 0.0310711
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000086ffff Time: 0.0318903
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000089ffff Time: 0.0403749
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000097ffff Time: 0.0335762
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000098ffff Time: 0.0613912
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000009fffff Time: 0.0338213
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x0000000000a2ffff Time: 0.0289938
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x0000000000a4ffff Time: 0.0466286
[05/26/2023-16:04:41] [V] [TRT] Fastest Tactic: 0x0000000000a2ffff Time: 0.0289938
[05/26/2023-16:04:41] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu (CudnnConvolution)
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.246043
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.058755
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x0000000000000002 Time: 0.223232
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x0000000000000004 Time: 0.291401
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x0000000000000005 Time: 0.898633
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x0000000000000006 Time: 0.0552122
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x0000000000000038 Time: 0.246053
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x0000000000000039 Time: 0.0587611
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000000003a Time: 0.223525
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000000003c Time: 0.291986
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000000003d Time: 0.898779
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000000003e Time: 0.055203
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x0000000000000070 Time: 0.246199
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x0000000000000071 Time: 0.248101
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x0000000000000072 Time: 0.223817
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x0000000000000074 Time: 0.292279
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x0000000000000075 Time: 0.898345
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x0000000000000076 Time: 0.0551497
[05/26/2023-16:04:41] [V] [TRT] Fastest Tactic: 0x0000000000000076 Time: 0.0551497
[05/26/2023-16:04:41] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu (CaskGemmConvolution)
[05/26/2023-16:04:41] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:41] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:41] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:41] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu (CaskConvolution)
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 0.241371
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x5deb29b7a8e275f7 Time: 0.202313
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 0.222501
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 0.246345
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 0x5403ad713f811a18
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x5403ad713f811a18 Time: 0.259072
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x94119b4c514b211a Time: 0.0416549
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xcb8a43f748d8a338 Time: 0.155794
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 0.181248
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xa31d27de74b895ff Time: 0.0907703
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x40a12e3938221818 Time: 0.221623
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x12dbf7d94ee3696d Time: 0.151552
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xa9366041633a5135 Time: 0.24064
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xbb8c3889c7eacd30 Time: 0.445001
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xb0bf940d5e0f9f45 Time: 0.063133
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x4727434768e46395 Time: 0.0893806
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x9d9fdb5fd9945f64 Time: 0.0868777
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x5aa723e0481da855 Time: 0.133193
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x4efce38acc876f5c Time: 0.441051
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x8ad32616b1424be4 Time: 0.0616351
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x01cf8ce2da913006 Time: 0.133047
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 0.190757
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 0.255854
[05/26/2023-16:04:41] [V] [TRT] Fastest Tactic: 0x94119b4c514b211a Time: 0.0416549
[05/26/2023-16:04:41] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: FusedConvActConvolution Tactic: 0x0000000000a2ffff
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256) -> Float(50176,1,3584,256) ***************
[05/26/2023-16:04:41] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:41] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:41] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu (CaskConvolution)
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x3e2b881168d9689d Time: 0.0773851
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xf90060ce8193b811 Time: 0.238885
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xd9031472c05adf51 Time: 0.248686
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xbdfdef6b84f7ccc9 Time: 0.142117
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x412c44dfeaf9161d Time: 0.144677
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x27b316f52c109002
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x27b316f52c109002 Time: 0.143799
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x7bc32c782b800c48 Time: 0.238738
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x7121ec1db3f80c67 Time: 0.0489082
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x5030121339a48bf3 Time: 0.253221
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x5953bec563d26434 Time: 0.0454171
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 0.0912823
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x0a143be7a52f301a Time: 0.0654385
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x62835fce994f06dd Time: 0.0818469
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x94a7db94ba744c45 Time: 0.080896
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x92ed3100c35fc43e Time: 0.0718994
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x32059de4888dfdda Time: 0.0724891
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x4fd3c46622e98342 Time: 0.106642
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x1da91d865428f237 Time: 0.128343
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xa6448a1e79f1ca6f Time: 0.107886
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x3f0c846d6379bc98 Time: 0.234057
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xf48db81f02eca9ee Time: 0.127049
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: 0xca7eeb8d9143d738
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xca7eeb8d9143d738 Time: 0.252928
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xd15dd11d64344e83 Time: 0.127561
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0x634e99502974e4da
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x634e99502974e4da Time: 0.243273
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x3e191488237fab8f Time: 0.0793029
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x8014228ec08b4d49 Time: 0.215186
[05/26/2023-16:04:41] [V] [TRT] Fastest Tactic: 0x5953bec563d26434 Time: 0.0454171
[05/26/2023-16:04:41] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5953bec563d26434
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64) -> Float(12544,1:4,896,64) ***************
[05/26/2023-16:04:41] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu (CudaDepthwiseConvolution)
[05/26/2023-16:04:41] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:41] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu (CaskGemmConvolution)
[05/26/2023-16:04:41] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:41] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:41] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:41] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu (CaskConvolution)
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 0.169545
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x999e005e3b016ea6 Time: 0.159744
[05/26/2023-16:04:41] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 0.16267
[05/26/2023-16:04:41] [V] [TRT] Fastest Tactic: 0x999e005e3b016ea6 Time: 0.159744
[05/26/2023-16:04:41] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6
[05/26/2023-16:04:41] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1), Float(200704,196,14,1) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256), Float(200704,1,14336,1024) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64), Float(50176,1:4,3584,256) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:41] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(200704,196,14,1) -> Float(50176,196,14,1) ***************
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(200704,1,14336,1024) -> Float(50176,1,3584,256) ***************
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,3584,256) -> Float(12544,1:4,896,64) ***************
[05/26/2023-16:04:41] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1) -> Float(50176,196,14,1) ***************
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256) -> Float(50176,1,3584,256) ***************
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64) -> Float(12544,1:4,896,64) ***************
[05/26/2023-16:04:41] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1), Float(200704,196,14,1) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256), Float(200704,1,14336,1024) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64), Float(50176,1:4,3584,256) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:41] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(200704,196,14,1) -> Float(50176,196,14,1) ***************
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(200704,1,14336,1024) -> Float(50176,1,3584,256) ***************
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,3584,256) -> Float(12544,1:4,896,64) ***************
[05/26/2023-16:04:41] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1) -> Float(50176,196,14,1) ***************
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256) -> Float(50176,1,3584,256) ***************
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64) -> Float(12544,1:4,896,64) ***************
[05/26/2023-16:04:41] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1), Float(200704,196,14,1) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256), Float(200704,1,14336,1024) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64), Float(50176,1:4,3584,256) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:41] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(200704,196,14,1) -> Float(50176,196,14,1) ***************
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(200704,1,14336,1024) -> Float(50176,1,3584,256) ***************
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,3584,256) -> Float(12544,1:4,896,64) ***************
[05/26/2023-16:04:41] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1) -> Float(50176,196,14,1) ***************
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256) -> Float(50176,1,3584,256) ***************
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64) -> Float(12544,1:4,896,64) ***************
[05/26/2023-16:04:41] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1), Float(200704,196,14,1) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256), Float(200704,1,14336,1024) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64), Float(50176,1:4,3584,256) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:41] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(200704,196,14,1) -> Float(50176,196,14,1) ***************
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(200704,1,14336,1024) -> Float(50176,1,3584,256) ***************
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,3584,256) -> Float(12544,1:4,896,64) ***************
[05/26/2023-16:04:41] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1) -> Float(50176,196,14,1) ***************
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256) -> Float(50176,1,3584,256) ***************
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64) -> Float(12544,1:4,896,64) ***************
[05/26/2023-16:04:41] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1), Float(200704,196,14,1) -> Float(200704,196,14,1) ***************
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256), Float(200704,1,14336,1024) -> Float(200704,1,14336,1024) ***************
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64), Float(50176,1:4,3584,256) -> Float(50176,1:4,3584,256) ***************
[05/26/2023-16:04:41] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:41] [V] [TRT] *************** Autotuning format combination: Float(200704,196,14,1) -> Float(100352,196,14,1) ***************
[05/26/2023-16:04:41] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu (CudaDepthwiseConvolution)
[05/26/2023-16:04:41] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:41] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu (FusedConvActConvolution)
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000008ffff Time: 0.207872
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000009ffff Time: 0.13173
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000000bffff Time: 0.0536869
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000000cffff Time: 0.0409966
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000011ffff Time: 0.0338286
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000018ffff Time: 0.0424229
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000001bffff Time: 0.0635855
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000001fffff Time: 0.0293038
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000002effff Time: 0.0654385
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000035ffff Time: 0.0470023
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000003cffff Time: 0.0370834
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000003dffff Time: 0.0415451
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000040ffff Time: 0.0306615
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000041ffff Time: 0.0444343
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000045ffff Time: 0.0376754
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000046ffff Time: 0.0325001
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000004affff Time: 0.0353426
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000004effff Time: 0.0372423
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000051ffff Time: 0.0364251
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000055ffff Time: 0.0415817
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000059ffff Time: 0.0261364
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000005effff Time: 0.0444754
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000005fffff Time: 0.0338761
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000061ffff Time: 0.0634926
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x000000000066ffff Time: 0.0380686
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000006affff Time: 0.0292571
[05/26/2023-16:04:41] [V] [TRT] Tactic: 0x00000000006bffff Time: 0.0466
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x00000000006fffff Time: 0.0290779
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x000000000070ffff Time: 0.0379851
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x000000000073ffff Time: 0.0498743
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x000000000075ffff Time: 0.0439303
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x00000000007cffff Time: 0.0533455
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x00000000007dffff Time: 0.0368297
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x00000000007effff Time: 0.0491185
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x000000000083ffff Time: 0.0513143
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x00000000008affff Time: 0.0315685
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x000000000091ffff Time: 0.0354304
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x00000000009dffff Time: 0.0319013
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000a3ffff Time: 0.0411794
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000a6ffff Time: 0.0759954
[05/26/2023-16:04:42] [V] [TRT] Fastest Tactic: 0x000000000059ffff Time: 0.0261364
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu (CudnnConvolution)
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0446537
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.0218168
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000002 Time: 0.158866
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000004 Time: 2.07579
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000005 Time: 0.406414
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000038 Time: 0.044544
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000039 Time: 0.0218175
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x000000000000003a Time: 0.158866
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x000000000000003c Time: 2.07579
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x000000000000003d Time: 0.406089
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000070 Time: 0.0446171
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000071 Time: 0.0464823
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000072 Time: 0.159013
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000074 Time: 2.07565
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000075 Time: 0.406235
[05/26/2023-16:04:42] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 0.0218168
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu (CublasConvolution)
[05/26/2023-16:04:42] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu (CaskGemmConvolution)
[05/26/2023-16:04:42] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:42] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu (CaskConvolution)
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 0.113152
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 0.0920869
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: 0xa8ef60e712f8ad24
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xa8ef60e712f8ad24 Time: 0.111909
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xc3cf6e1d1c6aff27
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xc3cf6e1d1c6aff27 Time: 0.081136
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 0.113714
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x9808072e706def96
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x9808072e706def96 Time: 0.108983
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xcb8a43f748d8a338 Time: 0.0702202
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 0.0809691
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xa31d27de74b895ff Time: 0.0427143
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfff46c7893896eb1
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xfff46c7893896eb1 Time: 0.200411
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x40a12e3938221818 Time: 0.096256
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x7f0145cb49517338
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x7f0145cb49517338 Time: 0.0915017
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa419b3b68f2da07b
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xa419b3b68f2da07b Time: 0.0907703
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9cd5cdc35441c505
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x9cd5cdc35441c505 Time: 0.062464
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fc87d7eb370bb7a
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x1fc87d7eb370bb7a Time: 0.0447269
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x12dbf7d94ee3696d Time: 0.0704122
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xe5603263b7f00303
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xe5603263b7f00303 Time: 0.0700709
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x828d0ea88c66fce7
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x828d0ea88c66fce7 Time: 0.0498347
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9de226a0c44627c4
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x9de226a0c44627c4 Time: 0.119589
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xa9366041633a5135 Time: 0.105984
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x8e3884f0eaec3ecd
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x8e3884f0eaec3ecd Time: 0.0690469
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xbb8c3889c7eacd30 Time: 0.204654
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xb0bf940d5e0f9f45 Time: 0.0340763
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xc0b05b61d128e46e
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xc0b05b61d128e46e Time: 0.0630491
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x9d9fdb5fd9945f64 Time: 0.0406046
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x5aa723e0481da855 Time: 0.0631558
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x2ee10e11d6651675
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x2ee10e11d6651675 Time: 0.122149
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x865894c4635db7fd
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x865894c4635db7fd Time: 0.043776
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90f8f2915f87ed77
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x90f8f2915f87ed77 Time: 0.0345015
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 0.0881371
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 0.112347
[05/26/2023-16:04:42] [V] [TRT] Fastest Tactic: 0xb0bf940d5e0f9f45 Time: 0.0340763
[05/26/2023-16:04:42] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000001
[05/26/2023-16:04:42] [V] [TRT] *************** Autotuning format combination: Float(200704,1,14336,1024) -> Float(100352,1,7168,512) ***************
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu (CublasConvolution)
[05/26/2023-16:04:42] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:42] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu (CaskConvolution)
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0x17173deba0b64484
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x17173deba0b64484 Time: 0.117541
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x3e2b881168d9689d Time: 0.0376583
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xf90060ce8193b811 Time: 0.109495
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xd9031472c05adf51 Time: 0.117541
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xe47307053a42b3e4
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xe47307053a42b3e4 Time: 0.108192
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x412c44dfeaf9161d Time: 0.0667063
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x27b316f52c109002
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x27b316f52c109002 Time: 0.067389
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xae0c89d047932ba3
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xae0c89d047932ba3 Time: 0.0589044
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x7bc32c782b800c48 Time: 0.107899
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x35f26f9c09557d86
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x35f26f9c09557d86 Time: 0.0596358
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xc7b3afceb5fb03c0
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xc7b3afceb5fb03c0 Time: 0.0603185
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x7121ec1db3f80c67 Time: 0.0316379
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x5030121339a48bf3 Time: 0.119223
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd9eb6ca56ddc3a22
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xd9eb6ca56ddc3a22 Time: 0.0420937
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xbc0bba0ff1a92939
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xbc0bba0ff1a92939 Time: 0.104521
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 0.0441417
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0a143be7a52f301a Time: 0.0370331
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x62835fce994f06dd Time: 0.0411509
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xc7feb33970feefa7
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xc7feb33970feefa7 Time: 0.036864
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x1022069e6f8d9aeb
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x1022069e6f8d9aeb Time: 0.0608061
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90898977fc8ce537
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x90898977fc8ce537 Time: 0.0398994
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x92ed3100c35fc43e Time: 0.0334985
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x4fd3c46622e98342 Time: 0.0498834
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd55ee6fd0b56f808
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xd55ee6fd0b56f808 Time: 0.059392
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x1da91d865428f237 Time: 0.0619109
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xa6448a1e79f1ca6f Time: 0.0542232
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fb90698107bb33a
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x1fb90698107bb33a Time: 0.0306222
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x55d80c17b1cd982d
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x55d80c17b1cd982d Time: 0.0402686
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x3e191488237fab8f Time: 0.0382171
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x8014228ec08b4d49 Time: 0.102107
[05/26/2023-16:04:42] [V] [TRT] Fastest Tactic: 0x1fb90698107bb33a Time: 0.0306222
[05/26/2023-16:04:42] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1fb90698107bb33a
[05/26/2023-16:04:42] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,3584,256) -> Float(25088,1:4,1792,128) ***************
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu (CudaDepthwiseConvolution)
[05/26/2023-16:04:42] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu (CublasConvolution)
[05/26/2023-16:04:42] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu (CaskGemmConvolution)
[05/26/2023-16:04:42] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:42] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu (CaskConvolution)
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x9dece0dc37e90462
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x9dece0dc37e90462 Time: 0.0737851
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x130df49cb195156b
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x130df49cb195156b Time: 0.0740206
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 0.0781166
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 0.0753371
[05/26/2023-16:04:42] [V] [TRT] Fastest Tactic: 0x9dece0dc37e90462 Time: 0.0737851
[05/26/2023-16:04:42] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9dece0dc37e90462
[05/26/2023-16:04:42] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:42] [V] [TRT] *************** Autotuning format combination: Float(100352,196,14,1) -> Float(25088,49,7,1) ***************
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu (CudaDepthwiseConvolution)
[05/26/2023-16:04:42] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu (FusedConvActConvolution)
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x000000000006ffff Time: 0.32768
[05/26/2023-16:04:42] [V] [TRT] Fastest Tactic: 0x000000000006ffff Time: 0.32768
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu (CudnnConvolution)
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.282039
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.118638
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000002 Time: 0.504101
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000005 Time: 3.50515
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000038 Time: 0.282185
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000039 Time: 0.11976
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x000000000000003a Time: 0.508635
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x000000000000003d Time: 3.50647
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000070 Time: 0.285842
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000071 Time: 0.588507
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000072 Time: 0.512146
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000075 Time: 3.50603
[05/26/2023-16:04:42] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 0.118638
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu (CaskGemmConvolution)
[05/26/2023-16:04:42] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:42] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu (CaskConvolution)
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 0.583387
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x5deb29b7a8e275f7 Time: 0.400677
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 0.41984
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 0.488594
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 0x5403ad713f811a18
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x5403ad713f811a18 Time: 0.49323
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xcb8a43f748d8a338 Time: 0.310565
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 0.368055
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xa31d27de74b895ff Time: 0.184905
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x40a12e3938221818 Time: 0.428617
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x12dbf7d94ee3696d Time: 0.297106
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xa9366041633a5135 Time: 0.476453
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xbb8c3889c7eacd30 Time: 0.87669
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xb0bf940d5e0f9f45 Time: 0.132608
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x4727434768e46395 Time: 0.185947
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x9d9fdb5fd9945f64 Time: 0.177883
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x5aa723e0481da855 Time: 0.265371
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x4efce38acc876f5c Time: 0.868498
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x8ad32616b1424be4 Time: 0.132389
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x01cf8ce2da913006 Time: 0.265216
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 0.381952
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 0.487278
[05/26/2023-16:04:42] [V] [TRT] Fastest Tactic: 0x8ad32616b1424be4 Time: 0.132389
[05/26/2023-16:04:42] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000001
[05/26/2023-16:04:42] [V] [TRT] *************** Autotuning format combination: Float(100352,1,7168,512) -> Float(25088,1,3584,512) ***************
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:42] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu (CaskConvolution)
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x3e2b881168d9689d Time: 0.150089
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xf90060ce8193b811 Time: 0.47205
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xd9031472c05adf51 Time: 0.482011
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xbdfdef6b84f7ccc9 Time: 0.279003
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x412c44dfeaf9161d Time: 0.284672
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x27b316f52c109002
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x27b316f52c109002 Time: 0.283502
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x7bc32c782b800c48 Time: 0.46987
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x7121ec1db3f80c67 Time: 0.108837
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x5030121339a48bf3 Time: 0.489033
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x5953bec563d26434 Time: 0.101449
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 0.184466
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0a143be7a52f301a Time: 0.13707
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x62835fce994f06dd Time: 0.164133
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x94a7db94ba744c45 Time: 0.163255
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x92ed3100c35fc43e Time: 0.14453
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x32059de4888dfdda Time: 0.144091
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x4fd3c46622e98342 Time: 0.210944
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x1da91d865428f237 Time: 0.251026
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xa6448a1e79f1ca6f Time: 0.212553
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x3f0c846d6379bc98 Time: 0.478062
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xf48db81f02eca9ee Time: 0.248686
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: 0xca7eeb8d9143d738
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xca7eeb8d9143d738 Time: 0.491968
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xd15dd11d64344e83 Time: 0.250587
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0x634e99502974e4da
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x634e99502974e4da Time: 0.477737
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x3e191488237fab8f Time: 0.152576
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x8014228ec08b4d49 Time: 0.415451
[05/26/2023-16:04:42] [V] [TRT] Fastest Tactic: 0x5953bec563d26434 Time: 0.101449
[05/26/2023-16:04:42] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5953bec563d26434
[05/26/2023-16:04:42] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,1792,128) -> Float(6272,1:4,896,128) ***************
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu (CudaDepthwiseConvolution)
[05/26/2023-16:04:42] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu (CaskGemmConvolution)
[05/26/2023-16:04:42] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:42] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu (CaskConvolution)
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 0.334702
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x999e005e3b016ea6 Time: 0.314807
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 0.320805
[05/26/2023-16:04:42] [V] [TRT] Fastest Tactic: 0x999e005e3b016ea6 Time: 0.314807
[05/26/2023-16:04:42] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6
[05/26/2023-16:04:42] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:42] [V] [TRT] *************** Autotuning format combination: Float(200704,196,14,1) -> Float(100352,49,7,1) ***************
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/downsample/downsample.0/Conv (CudaDepthwiseConvolution)
[05/26/2023-16:04:42] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/downsample/downsample.0/Conv (FusedConvActConvolution)
[05/26/2023-16:04:42] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/downsample/downsample.0/Conv (CudnnConvolution)
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0748983
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.0682667
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000002 Time: 0.209627
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000038 Time: 0.0749554
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000039 Time: 0.0682667
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x000000000000003a Time: 0.209774
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000070 Time: 0.0747794
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000071 Time: 0.0682179
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000072 Time: 0.209627
[05/26/2023-16:04:42] [V] [TRT] Fastest Tactic: 0x0000000000000071 Time: 0.0682179
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/downsample/downsample.0/Conv (CaskGemmConvolution)
[05/26/2023-16:04:42] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/downsample/downsample.0/Conv (CaskFlattenConvolution)
[05/26/2023-16:04:42] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/downsample/downsample.0/Conv (CaskConvolution)
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 0.157842
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 0.0968411
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: 0xa8ef60e712f8ad24
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xa8ef60e712f8ad24 Time: 0.112859
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xc3cf6e1d1c6aff27
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xc3cf6e1d1c6aff27 Time: 0.0857234
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 0.114688
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x9808072e706def96
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x9808072e706def96 Time: 0.151552
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xcb8a43f748d8a338 Time: 0.0718263
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 0.0837257
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xa31d27de74b895ff Time: 0.0476171
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x40a12e3938221818 Time: 0.122368
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x7f0145cb49517338
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x7f0145cb49517338 Time: 0.0928914
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x12dbf7d94ee3696d Time: 0.0721646
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xe5603263b7f00303
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xe5603263b7f00303 Time: 0.0718263
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xa9366041633a5135 Time: 0.107154
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xbb8c3889c7eacd30 Time: 0.208018
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xb0bf940d5e0f9f45 Time: 0.0397531
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xc0b05b61d128e46e
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xc0b05b61d128e46e Time: 0.0672579
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x9d9fdb5fd9945f64 Time: 0.0630979
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x5aa723e0481da855 Time: 0.0674865
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x2ee10e11d6651675
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x2ee10e11d6651675 Time: 0.125415
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x865894c4635db7fd
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x865894c4635db7fd Time: 0.0477379
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 0.0920091
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 0.113298
[05/26/2023-16:04:42] [V] [TRT] Fastest Tactic: 0xb0bf940d5e0f9f45 Time: 0.0397531
[05/26/2023-16:04:42] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xb0bf940d5e0f9f45
[05/26/2023-16:04:42] [V] [TRT] *************** Autotuning format combination: Float(200704,1,14336,1024) -> Float(100352,1,14336,2048) ***************
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/downsample/downsample.0/Conv (CaskFlattenConvolution)
[05/26/2023-16:04:42] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/downsample/downsample.0/Conv (CaskConvolution)
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0x17173deba0b64484
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x17173deba0b64484 Time: 0.114761
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x3e2b881168d9689d Time: 0.0389486
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xf90060ce8193b811 Time: 0.113737
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xd9031472c05adf51 Time: 0.114615
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xe47307053a42b3e4
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xe47307053a42b3e4 Time: 0.111982
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x412c44dfeaf9161d Time: 0.0675398
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x27b316f52c109002
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x27b316f52c109002 Time: 0.0690956
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xae0c89d047932ba3
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xae0c89d047932ba3 Time: 0.0600929
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x7bc32c782b800c48 Time: 0.112432
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x35f26f9c09557d86
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x35f26f9c09557d86 Time: 0.0602667
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x7121ec1db3f80c67 Time: 0.0344064
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x5030121339a48bf3 Time: 0.116151
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 0.0467417
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0a143be7a52f301a Time: 0.0398674
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x62835fce994f06dd Time: 0.0433749
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xc7feb33970feefa7
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xc7feb33970feefa7 Time: 0.0379611
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x1022069e6f8d9aeb
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x1022069e6f8d9aeb Time: 0.0618804
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x92ed3100c35fc43e Time: 0.0558324
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x4fd3c46622e98342 Time: 0.0736434
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x1da91d865428f237 Time: 0.0626133
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xa6448a1e79f1ca6f Time: 0.0528091
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x55d80c17b1cd982d
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x55d80c17b1cd982d Time: 0.0428366
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x3e191488237fab8f Time: 0.0393771
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x8014228ec08b4d49 Time: 0.103506
[05/26/2023-16:04:42] [V] [TRT] Fastest Tactic: 0x7121ec1db3f80c67 Time: 0.0344064
[05/26/2023-16:04:42] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x7121ec1db3f80c67
[05/26/2023-16:04:42] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,3584,256) -> Float(25088,1:4,3584,512) ***************
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/downsample/downsample.0/Conv (CudaDepthwiseConvolution)
[05/26/2023-16:04:42] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/downsample/downsample.0/Conv (CaskGemmConvolution)
[05/26/2023-16:04:42] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/downsample/downsample.0/Conv (CaskFlattenConvolution)
[05/26/2023-16:04:42] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/downsample/downsample.0/Conv (CaskConvolution)
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x130df49cb195156b
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x130df49cb195156b Time: 0.075264
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 0.0791406
[05/26/2023-16:04:42] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 0.0764343
[05/26/2023-16:04:42] [V] [TRT] Fastest Tactic: 0x130df49cb195156b Time: 0.075264
[05/26/2023-16:04:42] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x130df49cb195156b
[05/26/2023-16:04:42] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:42] [V] [TRT] *************** Autotuning format combination: Float(25088,49,7,1), Float(100352,49,7,1) -> Float(100352,49,7,1) ***************
[05/26/2023-16:04:42] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu (CudnnConvolution)
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0346697
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.0236356
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000002 Time: 0.114249
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000004 skipped. Scratch requested: 2421293056, available: 2147483648
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000005 Time: 0.745765
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000038 Time: 0.034816
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000039 Time: 0.0236983
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x000000000000003a Time: 0.114176
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x000000000000003c skipped. Scratch requested: 2421293056, available: 2147483648
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x000000000000003d Time: 0.746057
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000070 Time: 0.0350501
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000071 Time: 0.0318318
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000072 Time: 0.114249
[05/26/2023-16:04:42] [V] [TRT] Tactic: 0x0000000000000074 skipped. Scratch requested: 2421293056, available: 2147483648
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x0000000000000075 Time: 0.745765
[05/26/2023-16:04:43] [I] [TRT] Some tactics do not have sufficient workspace memory to run. Increasing workspace size will enable more tactics, please check verbose output for requested sizes.
[05/26/2023-16:04:43] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 0.0236356
[05/26/2023-16:04:43] [V] [TRT] Setting workspace to 2421293056enables more tactics for profiling
[05/26/2023-16:04:43] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu (CublasConvolution)
[05/26/2023-16:04:43] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:43] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu (CaskGemmConvolution)
[05/26/2023-16:04:43] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:43] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:43] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:43] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu (CaskConvolution)
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 0.0619276
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 0.0506149
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: 0xa8ef60e712f8ad24
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xa8ef60e712f8ad24 Time: 0.0607878
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xc3cf6e1d1c6aff27
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xc3cf6e1d1c6aff27 Time: 0.045176
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 0.0617813
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x9808072e706def96
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x9808072e706def96 Time: 0.059587
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xcb8a43f748d8a338 Time: 0.0382183
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 0.0446171
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xa31d27de74b895ff Time: 0.02464
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfff46c7893896eb1
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xfff46c7893896eb1 Time: 0.125145
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x40a12e3938221818 Time: 0.0629181
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x7f0145cb49517338
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x7f0145cb49517338 Time: 0.0698758
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa419b3b68f2da07b
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xa419b3b68f2da07b Time: 0.0656335
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9cd5cdc35441c505
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x9cd5cdc35441c505 Time: 0.036168
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fc87d7eb370bb7a
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x1fc87d7eb370bb7a Time: 0.0255688
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x12dbf7d94ee3696d Time: 0.0408503
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xe5603263b7f00303
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xe5603263b7f00303 Time: 0.0405509
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x828d0ea88c66fce7
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x828d0ea88c66fce7 Time: 0.0289938
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9de226a0c44627c4
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x9de226a0c44627c4 Time: 0.0866011
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xa9366041633a5135 Time: 0.057056
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x8e3884f0eaec3ecd
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x8e3884f0eaec3ecd Time: 0.0398629
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xbb8c3889c7eacd30 Time: 0.129317
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xb0bf940d5e0f9f45 Time: 0.0204793
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xc0b05b61d128e46e
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xc0b05b61d128e46e Time: 0.0366331
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x9d9fdb5fd9945f64 Time: 0.0318309
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x5aa723e0481da855 Time: 0.03712
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x2ee10e11d6651675
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x2ee10e11d6651675 Time: 0.0907703
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x865894c4635db7fd
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x865894c4635db7fd Time: 0.0251893
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90f8f2915f87ed77
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x90f8f2915f87ed77 Time: 0.0206047
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 0.0484785
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 0.0609524
[05/26/2023-16:04:43] [V] [TRT] Fastest Tactic: 0xb0bf940d5e0f9f45 Time: 0.0204793
[05/26/2023-16:04:43] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xb0bf940d5e0f9f45
[05/26/2023-16:04:43] [V] [TRT] *************** Autotuning format combination: Float(25088,1,3584,512), Float(100352,1,14336,2048) -> Float(100352,1,14336,2048) ***************
[05/26/2023-16:04:43] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu (CublasConvolution)
[05/26/2023-16:04:43] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:43] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:43] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:43] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu (CaskConvolution)
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0x17173deba0b64484
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x17173deba0b64484 Time: 0.059779
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x3e2b881168d9689d Time: 0.0225698
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xf90060ce8193b811 Time: 0.0581242
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xd9031472c05adf51 Time: 0.0596846
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xe47307053a42b3e4
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xe47307053a42b3e4 Time: 0.057344
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x412c44dfeaf9161d Time: 0.0365714
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x27b316f52c109002
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x27b316f52c109002 Time: 0.0372297
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xae0c89d047932ba3
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xae0c89d047932ba3 Time: 0.0329143
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x7bc32c782b800c48 Time: 0.0576366
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x35f26f9c09557d86
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x35f26f9c09557d86 Time: 0.0334226
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xc7b3afceb5fb03c0
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xc7b3afceb5fb03c0 Time: 0.0340745
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x7121ec1db3f80c67 Time: 0.0188006
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x5030121339a48bf3 Time: 0.0604648
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd9eb6ca56ddc3a22
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xd9eb6ca56ddc3a22 Time: 0.0230988
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xbc0bba0ff1a92939
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xbc0bba0ff1a92939 Time: 0.0719726
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 0.0249173
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x0a143be7a52f301a Time: 0.0207935
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x62835fce994f06dd Time: 0.0226893
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xc7feb33970feefa7
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xc7feb33970feefa7 Time: 0.02211
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x1022069e6f8d9aeb
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x1022069e6f8d9aeb Time: 0.0343186
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90898977fc8ce537
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x90898977fc8ce537 Time: 0.0221048
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x92ed3100c35fc43e Time: 0.0305161
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x4fd3c46622e98342 Time: 0.0397531
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd55ee6fd0b56f808
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xd55ee6fd0b56f808 Time: 0.0329728
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x1da91d865428f237 Time: 0.0354011
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xa6448a1e79f1ca6f Time: 0.0290048
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fb90698107bb33a
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x1fb90698107bb33a Time: 0.0178179
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x55d80c17b1cd982d
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x55d80c17b1cd982d Time: 0.0223556
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x3e191488237fab8f Time: 0.0227768
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x8014228ec08b4d49 Time: 0.077824
[05/26/2023-16:04:43] [V] [TRT] Fastest Tactic: 0x1fb90698107bb33a Time: 0.0178179
[05/26/2023-16:04:43] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1fb90698107bb33a
[05/26/2023-16:04:43] [V] [TRT] *************** Autotuning format combination: Float(6272,1:4,896,128), Float(25088,1:4,3584,512) -> Float(25088,1:4,3584,512) ***************
[05/26/2023-16:04:43] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu (CublasConvolution)
[05/26/2023-16:04:43] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:43] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu (CaskGemmConvolution)
[05/26/2023-16:04:43] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:43] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:43] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:43] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu (CaskConvolution)
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x9dece0dc37e90462
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x9dece0dc37e90462 Time: 0.0404114
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x130df49cb195156b
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x130df49cb195156b Time: 0.0406674
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 0.0426846
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 0.041472
[05/26/2023-16:04:43] [V] [TRT] Fastest Tactic: 0x9dece0dc37e90462 Time: 0.0404114
[05/26/2023-16:04:43] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9dece0dc37e90462
[05/26/2023-16:04:43] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:43] [V] [TRT] *************** Autotuning format combination: Float(100352,49,7,1) -> Float(25088,49,7,1) ***************
[05/26/2023-16:04:43] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu (CudaDepthwiseConvolution)
[05/26/2023-16:04:43] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:43] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu (FusedConvActConvolution)
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x000000000008ffff Time: 0.407259
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x000000000009ffff Time: 0.252489
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x00000000000bffff Time: 0.0852846
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x00000000000cffff Time: 0.0725554
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x000000000011ffff Time: 0.0423623
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x000000000018ffff Time: 0.0551985
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x00000000001bffff Time: 0.110446
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x00000000001fffff Time: 0.0302775
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x00000000002effff Time: 0.12171
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x000000000035ffff Time: 0.0656396
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x00000000003cffff Time: 0.0462663
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x00000000003dffff Time: 0.073216
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x000000000040ffff Time: 0.0323109
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x000000000041ffff Time: 0.0668556
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x000000000045ffff Time: 0.0494446
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x000000000046ffff Time: 0.050176
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x00000000004affff Time: 0.0508099
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x00000000004effff Time: 0.0428869
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x000000000051ffff Time: 0.0413623
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x000000000055ffff Time: 0.0732891
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x000000000059ffff Time: 0.0514926
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x00000000005effff Time: 0.064323
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x00000000005fffff Time: 0.0423863
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x000000000061ffff Time: 0.117321
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x000000000066ffff Time: 0.040448
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x00000000006affff Time: 0.0303323
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x00000000006bffff Time: 0.0682667
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x00000000006fffff Time: 0.0484206
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x000000000070ffff Time: 0.0599238
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x000000000073ffff Time: 0.0770926
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x000000000075ffff Time: 0.0774583
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x00000000007cffff Time: 0.0650971
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x00000000007dffff Time: 0.0512
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x00000000007effff Time: 0.0797989
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x000000000083ffff Time: 0.0776777
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x00000000008affff Time: 0.0300471
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x000000000091ffff Time: 0.0508587
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x00000000009dffff Time: 0.043776
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x0000000000a3ffff Time: 0.072704
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x0000000000a6ffff Time: 0.128073
[05/26/2023-16:04:43] [V] [TRT] Fastest Tactic: 0x00000000008affff Time: 0.0300471
[05/26/2023-16:04:43] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu (CudnnConvolution)
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.077312
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.0199863
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x0000000000000002 Time: 0.248101
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x0000000000000004 skipped. Scratch requested: 2424832000, available: 2147483648
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x0000000000000005 Time: 0.790089
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x0000000000000038 Time: 0.0772389
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x0000000000000039 Time: 0.0199863
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x000000000000003a Time: 0.246345
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x000000000000003c skipped. Scratch requested: 2424832000, available: 2147483648
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x000000000000003d Time: 0.789358
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x0000000000000070 Time: 0.0772389
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x0000000000000071 Time: 0.0719977
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x0000000000000072 Time: 0.249966
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x0000000000000074 skipped. Scratch requested: 2424832000, available: 2147483648
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x0000000000000075 Time: 0.790235
[05/26/2023-16:04:43] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 0.0199863
[05/26/2023-16:04:43] [V] [TRT] Setting workspace to 2424832000enables more tactics for profiling
[05/26/2023-16:04:43] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu (CublasConvolution)
[05/26/2023-16:04:43] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:43] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu (CaskGemmConvolution)
[05/26/2023-16:04:43] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:43] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:43] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:43] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu (CaskConvolution)
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 0.218551
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 0.178615
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: 0xa8ef60e712f8ad24
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xa8ef60e712f8ad24 Time: 0.217088
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xc3cf6e1d1c6aff27
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xc3cf6e1d1c6aff27 Time: 0.156379
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 0.220594
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x9808072e706def96
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x9808072e706def96 Time: 0.210766
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xcb8a43f748d8a338 Time: 0.134802
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 0.161061
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xa31d27de74b895ff Time: 0.0795063
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfff46c7893896eb1
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xfff46c7893896eb1 Time: 0.38795
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x40a12e3938221818 Time: 0.182857
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x7f0145cb49517338
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x7f0145cb49517338 Time: 0.169691
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa419b3b68f2da07b
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xa419b3b68f2da07b Time: 0.168814
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9cd5cdc35441c505
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x9cd5cdc35441c505 Time: 0.118199
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fc87d7eb370bb7a
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x1fc87d7eb370bb7a Time: 0.0836023
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x12dbf7d94ee3696d Time: 0.133266
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xe5603263b7f00303
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xe5603263b7f00303 Time: 0.132535
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x828d0ea88c66fce7
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x828d0ea88c66fce7 Time: 0.0953509
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9de226a0c44627c4
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x9de226a0c44627c4 Time: 0.225445
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xa9366041633a5135 Time: 0.206523
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x8e3884f0eaec3ecd
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x8e3884f0eaec3ecd Time: 0.13195
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xbb8c3889c7eacd30 Time: 0.394825
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xb0bf940d5e0f9f45 Time: 0.0531992
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xc0b05b61d128e46e
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xc0b05b61d128e46e Time: 0.118784
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x9d9fdb5fd9945f64 Time: 0.0765806
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x5aa723e0481da855 Time: 0.118857
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x2ee10e11d6651675
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x2ee10e11d6651675 Time: 0.229522
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x865894c4635db7fd
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x865894c4635db7fd Time: 0.0825211
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90f8f2915f87ed77
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x90f8f2915f87ed77 Time: 0.0546133
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 0.17013
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 0.217088
[05/26/2023-16:04:43] [V] [TRT] Fastest Tactic: 0xb0bf940d5e0f9f45 Time: 0.0531992
[05/26/2023-16:04:43] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000001
[05/26/2023-16:04:43] [V] [TRT] *************** Autotuning format combination: Float(100352,1,14336,2048) -> Float(25088,1,3584,512) ***************
[05/26/2023-16:04:43] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu (CublasConvolution)
[05/26/2023-16:04:43] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:43] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:43] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:43] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu (CaskConvolution)
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0x17173deba0b64484
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x17173deba0b64484 Time: 0.219282
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x3e2b881168d9689d Time: 0.0691931
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xf90060ce8193b811 Time: 0.212421
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xd9031472c05adf51 Time: 0.217527
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xe47307053a42b3e4
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xe47307053a42b3e4 Time: 0.209481
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x412c44dfeaf9161d Time: 0.12656
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x27b316f52c109002
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x27b316f52c109002 Time: 0.12939
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xae0c89d047932ba3
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xae0c89d047932ba3 Time: 0.112347
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x7bc32c782b800c48 Time: 0.210066
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x35f26f9c09557d86
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x35f26f9c09557d86 Time: 0.110007
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xc7b3afceb5fb03c0
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xc7b3afceb5fb03c0 Time: 0.114711
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x7121ec1db3f80c67 Time: 0.0433371
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x5030121339a48bf3 Time: 0.221477
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd9eb6ca56ddc3a22
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xd9eb6ca56ddc3a22 Time: 0.0747726
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xbc0bba0ff1a92939
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xbc0bba0ff1a92939 Time: 0.197632
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 0.0830697
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x0a143be7a52f301a Time: 0.059392
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x62835fce994f06dd Time: 0.0727771
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xc7feb33970feefa7
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xc7feb33970feefa7 Time: 0.0677303
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x1022069e6f8d9aeb
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x1022069e6f8d9aeb Time: 0.114688
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90898977fc8ce537
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x90898977fc8ce537 Time: 0.0707535
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x92ed3100c35fc43e Time: 0.0630004
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x4fd3c46622e98342 Time: 0.0950857
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd55ee6fd0b56f808
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xd55ee6fd0b56f808 Time: 0.110811
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x1da91d865428f237 Time: 0.116014
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xa6448a1e79f1ca6f Time: 0.0989623
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fb90698107bb33a
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x1fb90698107bb33a Time: 0.0413623
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x55d80c17b1cd982d
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x55d80c17b1cd982d Time: 0.07168
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x3e191488237fab8f Time: 0.0700709
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x8014228ec08b4d49 Time: 0.193097
[05/26/2023-16:04:43] [V] [TRT] Fastest Tactic: 0x1fb90698107bb33a Time: 0.0413623
[05/26/2023-16:04:43] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1fb90698107bb33a
[05/26/2023-16:04:43] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,3584,512) -> Float(6272,1:4,896,128) ***************
[05/26/2023-16:04:43] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu (CudaDepthwiseConvolution)
[05/26/2023-16:04:43] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:43] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu (CublasConvolution)
[05/26/2023-16:04:43] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:43] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu (CaskGemmConvolution)
[05/26/2023-16:04:43] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:43] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:43] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:43] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu (CaskConvolution)
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x9dece0dc37e90462
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x9dece0dc37e90462 Time: 0.141678
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x130df49cb195156b
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x130df49cb195156b Time: 0.142117
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 0.150967
[05/26/2023-16:04:43] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 0.14501
[05/26/2023-16:04:43] [V] [TRT] Fastest Tactic: 0x9dece0dc37e90462 Time: 0.141678
[05/26/2023-16:04:43] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9dece0dc37e90462
[05/26/2023-16:04:43] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:43] [V] [TRT] *************** Autotuning format combination: Float(25088,49,7,1) -> Float(25088,49,7,1) ***************
[05/26/2023-16:04:43] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu (CudaDepthwiseConvolution)
[05/26/2023-16:04:43] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:43] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu (FusedConvActConvolution)
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x000000000007ffff Time: 0.273554
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x00000000000affff Time: 0.194853
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x00000000000effff Time: 0.098304
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x00000000000fffff Time: 0.174519
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x000000000019ffff Time: 0.0906331
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x00000000001affff Time: 0.0841303
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x000000000024ffff Time: 0.0597227
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x000000000027ffff Time: 0.0642194
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x00000000002dffff Time: 0.100059
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x000000000036ffff Time: 0.101888
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x00000000004cffff Time: 0.154039
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x000000000062ffff Time: 0.190757
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x00000000006effff Time: 0.0764343
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x000000000077ffff Time: 0.0664625
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x000000000086ffff Time: 0.0707048
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x000000000089ffff Time: 0.0999131
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x000000000097ffff Time: 0.0672914
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x000000000098ffff Time: 0.162085
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x00000000009fffff Time: 0.0852114
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x0000000000a2ffff Time: 0.0626103
[05/26/2023-16:04:43] [V] [TRT] Tactic: 0x0000000000a4ffff Time: 0.109934
[05/26/2023-16:04:43] [V] [TRT] Fastest Tactic: 0x000000000024ffff Time: 0.0597227
[05/26/2023-16:04:44] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu (CudnnConvolution)
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.603429
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.117687
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000000000002 Time: 0.493861
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000000000004 Time: 1.11748
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000000000005 Time: 3.50705
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000000000006 Time: 0.118272
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000000000038 Time: 0.603721
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000000000039 Time: 0.117687
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x000000000000003a Time: 0.493106
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x000000000000003c Time: 1.11733
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x000000000000003d Time: 3.50676
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x000000000000003e Time: 0.118215
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000000000070 Time: 0.612352
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000000000071 Time: 0.544183
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000000000072 Time: 0.497952
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000000000074 Time: 1.11865
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000000000075 Time: 3.50749
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000000000076 Time: 0.119442
[05/26/2023-16:04:44] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 0.117687
[05/26/2023-16:04:44] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu (CaskGemmConvolution)
[05/26/2023-16:04:44] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:44] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:44] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:44] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu (CaskConvolution)
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 0.537454
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x5deb29b7a8e275f7 Time: 0.420718
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 0.443977
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 0.488155
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 0x5403ad713f811a18
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x5403ad713f811a18 Time: 0.513609
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x94119b4c514b211a Time: 0.0809691
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0xcb8a43f748d8a338 Time: 0.310711
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 0.366885
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0xa31d27de74b895ff Time: 0.181102
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x40a12e3938221818 Time: 0.428032
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x12dbf7d94ee3696d Time: 0.29579
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0xa9366041633a5135 Time: 0.476014
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0xbb8c3889c7eacd30 Time: 0.875813
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0xb0bf940d5e0f9f45 Time: 0.132023
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x4727434768e46395 Time: 0.180517
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x9d9fdb5fd9945f64 Time: 0.173056
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x5aa723e0481da855 Time: 0.261888
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x4efce38acc876f5c Time: 0.867767
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x8ad32616b1424be4 Time: 0.13195
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x01cf8ce2da913006 Time: 0.261559
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 0.381659
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 0.508928
[05/26/2023-16:04:44] [V] [TRT] Fastest Tactic: 0x94119b4c514b211a Time: 0.0809691
[05/26/2023-16:04:44] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: FusedConvActConvolution Tactic: 0x000000000024ffff
[05/26/2023-16:04:44] [V] [TRT] *************** Autotuning format combination: Float(25088,1,3584,512) -> Float(25088,1,3584,512) ***************
[05/26/2023-16:04:44] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:44] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:44] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu (CaskConvolution)
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x3e2b881168d9689d Time: 0.150382
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0xf90060ce8193b811 Time: 0.472795
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0xd9031472c05adf51 Time: 0.483621
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0xbdfdef6b84f7ccc9 Time: 0.27765
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x412c44dfeaf9161d Time: 0.284233
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x27b316f52c109002
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x27b316f52c109002 Time: 0.283355
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x7bc32c782b800c48 Time: 0.477184
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x7121ec1db3f80c67 Time: 0.109202
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x5030121339a48bf3 Time: 0.488155
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x5953bec563d26434 Time: 0.101669
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 0.183881
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0a143be7a52f301a Time: 0.137143
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x62835fce994f06dd Time: 0.166203
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x94a7db94ba744c45 Time: 0.165303
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x92ed3100c35fc43e Time: 0.146432
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x32059de4888dfdda Time: 0.142528
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x4fd3c46622e98342 Time: 0.212215
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x1da91d865428f237 Time: 0.252517
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0xa6448a1e79f1ca6f Time: 0.210944
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x3f0c846d6379bc98 Time: 0.477915
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0xf48db81f02eca9ee Time: 0.250149
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: 0xca7eeb8d9143d738
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0xca7eeb8d9143d738 Time: 0.495031
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0xd15dd11d64344e83 Time: 0.252782
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0x634e99502974e4da
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x634e99502974e4da Time: 0.483182
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x3e191488237fab8f Time: 0.153307
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x8014228ec08b4d49 Time: 0.416768
[05/26/2023-16:04:44] [V] [TRT] Fastest Tactic: 0x5953bec563d26434 Time: 0.101669
[05/26/2023-16:04:44] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5953bec563d26434
[05/26/2023-16:04:44] [V] [TRT] *************** Autotuning format combination: Float(6272,1:4,896,128) -> Float(6272,1:4,896,128) ***************
[05/26/2023-16:04:44] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu (CudaDepthwiseConvolution)
[05/26/2023-16:04:44] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:44] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu (CaskGemmConvolution)
[05/26/2023-16:04:44] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:44] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu (CaskFlattenConvolution)
[05/26/2023-16:04:44] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:44] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu (CaskConvolution)
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 0.334702
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x999e005e3b016ea6 Time: 0.314953
[05/26/2023-16:04:44] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 0.320658
[05/26/2023-16:04:44] [V] [TRT] Fastest Tactic: 0x999e005e3b016ea6 Time: 0.314953
[05/26/2023-16:04:44] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6
[05/26/2023-16:04:44] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:44] [V] [TRT] *************** Autotuning format combination: Float(25088,49,7,1), Float(100352,49,7,1) -> Float(100352,49,7,1) ***************
[05/26/2023-16:04:44] [V] [TRT] *************** Autotuning format combination: Float(25088,1,3584,512), Float(100352,1,14336,2048) -> Float(100352,1,14336,2048) ***************
[05/26/2023-16:04:44] [V] [TRT] *************** Autotuning format combination: Float(6272,1:4,896,128), Float(25088,1:4,3584,512) -> Float(25088,1:4,3584,512) ***************
[05/26/2023-16:04:44] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:44] [V] [TRT] *************** Autotuning format combination: Float(100352,49,7,1) -> Float(25088,49,7,1) ***************
[05/26/2023-16:04:44] [V] [TRT] *************** Autotuning format combination: Float(100352,1,14336,2048) -> Float(25088,1,3584,512) ***************
[05/26/2023-16:04:44] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,3584,512) -> Float(6272,1:4,896,128) ***************
[05/26/2023-16:04:44] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:44] [V] [TRT] *************** Autotuning format combination: Float(25088,49,7,1) -> Float(25088,49,7,1) ***************
[05/26/2023-16:04:44] [V] [TRT] *************** Autotuning format combination: Float(25088,1,3584,512) -> Float(25088,1,3584,512) ***************
[05/26/2023-16:04:44] [V] [TRT] *************** Autotuning format combination: Float(6272,1:4,896,128) -> Float(6272,1:4,896,128) ***************
[05/26/2023-16:04:44] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:44] [V] [TRT] *************** Autotuning format combination: Float(25088,49,7,1), Float(100352,49,7,1) -> Float(100352,49,7,1) ***************
[05/26/2023-16:04:44] [V] [TRT] *************** Autotuning format combination: Float(25088,1,3584,512), Float(100352,1,14336,2048) -> Float(100352,1,14336,2048) ***************
[05/26/2023-16:04:44] [V] [TRT] *************** Autotuning format combination: Float(6272,1:4,896,128), Float(25088,1:4,3584,512) -> Float(25088,1:4,3584,512) ***************
[05/26/2023-16:04:44] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:44] [V] [TRT] *************** Autotuning format combination: Float(100352,49,7,1) -> Float(2048,1,1,1) ***************
[05/26/2023-16:04:44] [V] [TRT] --------------- Timing Runner: /avgpool/GlobalAveragePool (TiledPooling)
[05/26/2023-16:04:44] [V] [TRT] TiledPooling has no valid tactics for this config, skipping
[05/26/2023-16:04:44] [V] [TRT] --------------- Timing Runner: /avgpool/GlobalAveragePool (CudnnPooling)
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0xffffffffffffffff Time: 0.0037677
[05/26/2023-16:04:44] [V] [TRT] Fastest Tactic: 0xffffffffffffffff Time: 0.0037677
[05/26/2023-16:04:44] [V] [TRT] --------------- Timing Runner: /avgpool/GlobalAveragePool (CaskPooling)
[05/26/2023-16:04:44] [V] [TRT] /avgpool/GlobalAveragePool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kAVERAGE_tP1_tQ1_tR7_tS7_tU1_tV1_tUnroll2_tThreads49 Tactic: 0x31d30f1a58b7ea39
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x31d30f1a58b7ea39 Time: 0.00388856
[05/26/2023-16:04:44] [V] [TRT] /avgpool/GlobalAveragePool Set Tactic Name: sm50_xmma_pooling_fw_4d_FP32FP32NCHW_Average_FastDiv Tactic: 0x933eceba7b866d59
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x933eceba7b866d59 Time: 0.00380379
[05/26/2023-16:04:44] [V] [TRT] /avgpool/GlobalAveragePool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kAVERAGE_tP1_tQ1_tR7_tS7_tU1_tV1_tUnroll5_tThreads49 Tactic: 0x02269187420e4bc5
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x02269187420e4bc5 Time: 0.00415491
[05/26/2023-16:04:44] [V] [TRT] /avgpool/GlobalAveragePool Set Tactic Name: sm50_xmma_pooling_nd_NCDHW_kAVERAGE_kGENERIC_3D_POOLING_MODE_kFLOAT_0 Tactic: 0xba33c80addb15739
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0xba33c80addb15739 Time: 0.00627717
[05/26/2023-16:04:44] [V] [TRT] /avgpool/GlobalAveragePool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kAVERAGE_tP1_tQ1_tR7_tS7_tU1_tV1_tUnroll6_tThreads49 Tactic: 0xa4a96ea1892462c7
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0xa4a96ea1892462c7 Time: 0.00404927
[05/26/2023-16:04:44] [V] [TRT] /avgpool/GlobalAveragePool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kAVERAGE_tP1_tQ1_tR7_tS7_tU1_tV1_tUnroll1_tThreads49 Tactic: 0x975cf03c939dc33b
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x975cf03c939dc33b Time: 0.00401308
[05/26/2023-16:04:44] [V] [TRT] /avgpool/GlobalAveragePool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kAVERAGE_tP1_tQ1_tR7_tS7_tU1_tV1_tUnroll7_tThreads49 Tactic: 0x489ba15aaac78fba
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x489ba15aaac78fba Time: 0.00429069
[05/26/2023-16:04:44] [V] [TRT] /avgpool/GlobalAveragePool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kAVERAGE_tP1_tQ1_tR7_tS7_tU1_tV1_tUnroll3_tThreads49 Tactic: 0xdde1c0e17b540744
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0xdde1c0e17b540744 Time: 0.00400597
[05/26/2023-16:04:44] [V] [TRT] /avgpool/GlobalAveragePool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kAVERAGE_tP1_tQ1_tR7_tS7_tU1_tV1_tUnroll8_tThreads49 Tactic: 0xc342539bbc57213f
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0xc342539bbc57213f Time: 0.00390029
[05/26/2023-16:04:44] [V] [TRT] /avgpool/GlobalAveragePool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kAVERAGE_tP1_tQ1_tR7_tS7_tU1_tV1_tUnroll4_tThreads49 Tactic: 0xee145e7c61eda6b8
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0xee145e7c61eda6b8 Time: 0.00389646
[05/26/2023-16:04:44] [V] [TRT] Fastest Tactic: 0x933eceba7b866d59 Time: 0.00380379
[05/26/2023-16:04:44] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnPooling Tactic: 0xffffffffffffffff
[05/26/2023-16:04:44] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,3584,512) -> Float(512,1:4,512,512) ***************
[05/26/2023-16:04:44] [V] [TRT] --------------- Timing Runner: /avgpool/GlobalAveragePool (CaskPooling)
[05/26/2023-16:04:44] [V] [TRT] /avgpool/GlobalAveragePool Set Tactic Name: sm50_xmma_pooling_fw_4d_FP32FP32NHWC_Average_FastDiv_CAlign4 Tactic: 0xfab3e2ee1c085a9a
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0xfab3e2ee1c085a9a Time: 0.00364343
[05/26/2023-16:04:44] [V] [TRT] Fastest Tactic: 0xfab3e2ee1c085a9a Time: 0.00364343
[05/26/2023-16:04:44] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0xfab3e2ee1c085a9a
[05/26/2023-16:04:44] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:44] [V] [TRT] *************** Autotuning format combination: Float(2048,1,1,1) -> Float(1000,1,1,1) ***************
[05/26/2023-16:04:44] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CudaDepthwiseConvolution)
[05/26/2023-16:04:44] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:44] [V] [TRT] --------------- Timing Runner: /fc/Gemm (FusedConvActConvolution)
[05/26/2023-16:04:44] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:44] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CudnnConvolution)
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.123392
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.0164673
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000000000002 Time: 0.241079
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000000000004 skipped. Scratch requested: 4731502592, available: 2147483648
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000000000005 Time: 0.96373
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000000000038 Time: 0.123392
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000000000039 Time: 0.0164688
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x000000000000003a Time: 0.241957
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x000000000000003c skipped. Scratch requested: 4731502592, available: 2147483648
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x000000000000003d Time: 0.962853
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000000000070 Time: 0.123538
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000000000071 Time: 0.096672
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000000000072 Time: 0.241371
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000000000074 skipped. Scratch requested: 4731502592, available: 2147483648
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000000000075 Time: 0.963291
[05/26/2023-16:04:44] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 0.0164673
[05/26/2023-16:04:44] [V] [TRT] Setting workspace to 4731502592enables more tactics for profiling
[05/26/2023-16:04:44] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CublasConvolution)
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0161849
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.0159451
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000000000002 Time: 0.0161727
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000000000003 Time: 0.0159777
[05/26/2023-16:04:44] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 0.0159451
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param64x2x1_strided_copyx_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param64x2x1_strided_copyx_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param64x2x1_strided_copyx_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param64x2x1_strided_copyx_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param64x2x1_strided_copyx_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param64x2x1_strided_copyx_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param64x2x1_strided_copyx_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param64x2x1_strided_copyx_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param64x2x1_strided_copyx_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param64x2x1_strided_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param64x2x1_strided_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param64x2x1_strided_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param64x2x1_strided_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param64x2x1_strided_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param64x2x1_strided_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param64x2x1_strided_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param64x2x1_strided_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param64x2x1_strided_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_copyx_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_copyx_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_copyx_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_copyx_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_copyx_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_copyx_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_copyx_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_copyx_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_copyx_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param16x32x32_strided_copyx_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param16x32x32_strided_copyx_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param16x32x32_strided_copyx_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param16x32x32_strided_copyx_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param16x32x32_strided_copyx_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param16x32x32_strided_copyx_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param16x32x32_strided_copyx_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param16x32x32_strided_copyx_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param16x32x32_strided_copyx_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param8x32x32_strided_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param8x32x32_strided_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param8x32x32_strided_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param8x32x32_strided_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param8x32x32_strided_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param8x32x32_strided_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param8x32x32_strided_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param8x32x32_strided_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param8x32x32_strided_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm70_xmma_gemm_as_conv1x1_f32f32_f32_f32_tn_n_simt_small_batch_bias_relu
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param8x32x32_strided_copyx_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param8x32x32_strided_copyx_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param8x32x32_strided_copyx_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param8x32x32_strided_copyx_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param8x32x32_strided_copyx_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param8x32x32_strided_copyx_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param8x32x32_strided_copyx_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param8x32x32_strided_copyx_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param8x32x32_strided_copyx_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param16x32x32_strided_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param16x32x32_strided_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param16x32x32_strided_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param16x32x32_strided_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param16x32x32_strided_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param16x32x32_strided_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param16x32x32_strided_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param16x32x32_strided_unit_stride
[05/26/2023-16:04:44] [V] [TRT] Unrecognized MMA instruction source format or shape: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param16x32x32_strided_unit_stride
[05/26/2023-16:04:44] [V] [TRT] /fc/Gemm: 73 available tactics, 73 unparsable, 0 pruned, 73 remaining after tactic pruning.
[05/26/2023-16:04:44] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskGemmConvolution)
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param8x32x32_strided_unit_stride Tactic: 0x00000000000204b2 numSplitK: 1 numBuffers: 0 numKernels: 1
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x00000000000204b2 Time: 0.0152709
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm70_xmma_gemm_as_conv1x1_f32f32_f32_f32_tn_n_simt_small_batch_bias_relu Tactic: 0x00000000000205c0 numSplitK: 1 numBuffers: 0 numKernels: 1
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x00000000000205c0 Time: 0.0169712
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param8x32x32_strided_unit_stride Tactic: 0x00000002022004b2 numSplitK: 16 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x00000002022004b2 Time: 0.0424229
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param8x32x32_strided_unit_stride Tactic: 0x00000002021804b2 numSplitK: 12 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x00000002021804b2 Time: 0.0492983
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param8x32x32_strided_unit_stride Tactic: 0x00000002021004b2 numSplitK: 8 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x00000002021004b2 Time: 0.0304859
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param8x32x32_strided_unit_stride Tactic: 0x00000002020c04b2 numSplitK: 6 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x00000002020c04b2 Time: 0.0333824
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param8x32x32_strided_unit_stride Tactic: 0x00000002020a04b2 numSplitK: 5 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x00000002020a04b2 Time: 0.0303397
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param8x32x32_strided_unit_stride Tactic: 0x00000002020804b2 numSplitK: 4 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x00000002020804b2 Time: 0.019712
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param8x32x32_strided_unit_stride Tactic: 0x00000002020604b2 numSplitK: 3 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x00000002020604b2 Time: 0.0256
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param8x32x32_strided_unit_stride Tactic: 0x00000002020404b2 numSplitK: 2 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x00000002020404b2 Time: 0.0191269
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param8x32x32_strided_copyx_unit_stride Tactic: 0x0000000000020639 numSplitK: 1 numBuffers: 0 numKernels: 1
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000000020639 Time: 0.0164328
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_unit_stride Tactic: 0x0000000202200460 numSplitK: 16 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000202200460 Time: 0.0296375
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_unit_stride Tactic: 0x0000000202180460 numSplitK: 12 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000202180460 Time: 0.0346112
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_unit_stride Tactic: 0x0000000202100460 numSplitK: 8 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000202100460 Time: 0.0223817
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_unit_stride Tactic: 0x00000002020c0460 numSplitK: 6 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x00000002020c0460 Time: 0.0254781
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_unit_stride Tactic: 0x00000002020a0460 numSplitK: 5 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x00000002020a0460 Time: 0.023439
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_unit_stride Tactic: 0x0000000202080460 numSplitK: 4 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000202080460 Time: 0.0181394
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_unit_stride Tactic: 0x0000000202060460 numSplitK: 3 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000202060460 Time: 0.0217966
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_unit_stride Tactic: 0x0000000202040460 numSplitK: 2 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000202040460 Time: 0.0181029
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param16x32x32_strided_unit_stride Tactic: 0x000000000002068d numSplitK: 1 numBuffers: 0 numKernels: 1
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x000000000002068d Time: 0.0146725
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param16x32x32_strided_unit_stride Tactic: 0x000000020220068d numSplitK: 16 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x000000020220068d Time: 0.0295159
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param16x32x32_strided_unit_stride Tactic: 0x000000020218068d numSplitK: 12 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x000000020218068d Time: 0.034699
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param16x32x32_strided_unit_stride Tactic: 0x000000020210068d numSplitK: 8 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x000000020210068d Time: 0.0228206
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param16x32x32_strided_unit_stride Tactic: 0x00000002020c068d numSplitK: 6 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x00000002020c068d Time: 0.0256465
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param16x32x32_strided_unit_stride Tactic: 0x00000002020a068d numSplitK: 5 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x00000002020a068d Time: 0.0241295
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param16x32x32_strided_unit_stride Tactic: 0x000000020208068d numSplitK: 4 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x000000020208068d Time: 0.0196531
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param16x32x32_strided_unit_stride Tactic: 0x000000020206068d numSplitK: 3 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x000000020206068d Time: 0.0220056
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param16x32x32_strided_unit_stride Tactic: 0x000000020204068d numSplitK: 2 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x000000020204068d Time: 0.0191817
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_unit_stride Tactic: 0x0000000000020460 numSplitK: 1 numBuffers: 0 numKernels: 1
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000000020460 Time: 0.0144727
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param8x32x32_strided_copyx_unit_stride Tactic: 0x0000000202200639 numSplitK: 16 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000202200639 Time: 0.033909
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param8x32x32_strided_copyx_unit_stride Tactic: 0x0000000202180639 numSplitK: 12 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000202180639 Time: 0.0360663
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param8x32x32_strided_copyx_unit_stride Tactic: 0x0000000202100639 numSplitK: 8 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000202100639 Time: 0.0245661
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param8x32x32_strided_copyx_unit_stride Tactic: 0x00000002020c0639 numSplitK: 6 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x00000002020c0639 Time: 0.0276792
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param8x32x32_strided_copyx_unit_stride Tactic: 0x00000002020a0639 numSplitK: 5 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x00000002020a0639 Time: 0.0258126
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param8x32x32_strided_copyx_unit_stride Tactic: 0x0000000202080639 numSplitK: 4 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000202080639 Time: 0.0209254
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param8x32x32_strided_copyx_unit_stride Tactic: 0x0000000202060639 numSplitK: 3 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000202060639 Time: 0.0237662
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param8x32x32_strided_copyx_unit_stride Tactic: 0x0000000202040639 numSplitK: 2 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000202040639 Time: 0.019976
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param64x2x1_strided_unit_stride Tactic: 0x000000000002042a numSplitK: 1 numBuffers: 0 numKernels: 1
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x000000000002042a Time: 0.0748983
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param64x2x1_strided_unit_stride Tactic: 0x000000020220042a numSplitK: 16 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x000000020220042a Time: 0.0267215
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param64x2x1_strided_unit_stride Tactic: 0x000000020218042a numSplitK: 12 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x000000020218042a Time: 0.0265752
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param64x2x1_strided_unit_stride Tactic: 0x000000020210042a numSplitK: 8 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x000000020210042a Time: 0.0272808
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param64x2x1_strided_unit_stride Tactic: 0x00000002020c042a numSplitK: 6 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x00000002020c042a Time: 0.0315685
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param64x2x1_strided_unit_stride Tactic: 0x00000002020a042a numSplitK: 5 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x00000002020a042a Time: 0.0370103
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param64x2x1_strided_unit_stride Tactic: 0x000000020208042a numSplitK: 4 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x000000020208042a Time: 0.0262583
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param64x2x1_strided_unit_stride Tactic: 0x000000020206042a numSplitK: 3 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x000000020206042a Time: 0.0323877
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param64x2x1_strided_unit_stride Tactic: 0x000000020204042a numSplitK: 2 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x000000020204042a Time: 0.0433006
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_copyx_unit_stride Tactic: 0x000000000002044a numSplitK: 1 numBuffers: 0 numKernels: 1
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x000000000002044a Time: 0.0169219
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param64x2x1_strided_copyx_unit_stride Tactic: 0x0000000202200419 numSplitK: 16 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000202200419 Time: 0.0265006
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param64x2x1_strided_copyx_unit_stride Tactic: 0x0000000202180419 numSplitK: 12 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000202180419 Time: 0.0273463
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param64x2x1_strided_copyx_unit_stride Tactic: 0x0000000202100419 numSplitK: 8 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000202100419 Time: 0.0272632
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param64x2x1_strided_copyx_unit_stride Tactic: 0x00000002020c0419 numSplitK: 6 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x00000002020c0419 Time: 0.0337975
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param64x2x1_strided_copyx_unit_stride Tactic: 0x00000002020a0419 numSplitK: 5 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x00000002020a0419 Time: 0.0385829
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param64x2x1_strided_copyx_unit_stride Tactic: 0x0000000202080419 numSplitK: 4 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000202080419 Time: 0.0297938
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param64x2x1_strided_copyx_unit_stride Tactic: 0x0000000202060419 numSplitK: 3 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000202060419 Time: 0.0374491
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param64x2x1_strided_copyx_unit_stride Tactic: 0x0000000202040419 numSplitK: 2 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000202040419 Time: 0.0498895
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param16x32x32_strided_copyx_unit_stride Tactic: 0x000000000002045e numSplitK: 1 numBuffers: 0 numKernels: 1
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x000000000002045e Time: 0.0163647
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param16x32x32_strided_copyx_unit_stride Tactic: 0x000000020220045e numSplitK: 16 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x000000020220045e Time: 0.0373417
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param16x32x32_strided_copyx_unit_stride Tactic: 0x000000020218045e numSplitK: 12 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x000000020218045e Time: 0.040248
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param16x32x32_strided_copyx_unit_stride Tactic: 0x000000020210045e numSplitK: 8 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x000000020210045e Time: 0.0268808
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param16x32x32_strided_copyx_unit_stride Tactic: 0x00000002020c045e numSplitK: 6 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x00000002020c045e Time: 0.0295387
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param16x32x32_strided_copyx_unit_stride Tactic: 0x00000002020a045e numSplitK: 5 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x00000002020a045e Time: 0.0275398
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param16x32x32_strided_copyx_unit_stride Tactic: 0x000000020208045e numSplitK: 4 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x000000020208045e Time: 0.0210227
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param16x32x32_strided_copyx_unit_stride Tactic: 0x000000020206045e numSplitK: 3 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x000000020206045e Time: 0.0249509
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param16x32x32_strided_copyx_unit_stride Tactic: 0x000000020204045e numSplitK: 2 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x000000020204045e Time: 0.0202623
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param64x2x1_strided_copyx_unit_stride Tactic: 0x0000000000020419 numSplitK: 1 numBuffers: 0 numKernels: 1
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x0000000000020419 Time: 0.0865417
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_copyx_unit_stride Tactic: 0x000000020220044a numSplitK: 16 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x000000020220044a Time: 0.0327433
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_copyx_unit_stride Tactic: 0x000000020218044a numSplitK: 12 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x000000020218044a Time: 0.0355557
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_copyx_unit_stride Tactic: 0x000000020210044a numSplitK: 8 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x000000020210044a Time: 0.0246872
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_copyx_unit_stride Tactic: 0x00000002020c044a numSplitK: 6 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x00000002020c044a Time: 0.0260952
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_copyx_unit_stride Tactic: 0x00000002020a044a numSplitK: 5 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x00000002020a044a Time: 0.0240221
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_copyx_unit_stride Tactic: 0x000000020208044a numSplitK: 4 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x000000020208044a Time: 0.0207138
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_copyx_unit_stride Tactic: 0x000000020206044a numSplitK: 3 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x000000020206044a Time: 0.0232457
[05/26/2023-16:04:44] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_copyx_unit_stride Tactic: 0x000000020204044a numSplitK: 2 numBuffers: 1 numKernels: 2
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x000000020204044a Time: 0.0199891
[05/26/2023-16:04:44] [V] [TRT] Fastest Tactic: 0x0000000000020460 Time: 0.0144727
[05/26/2023-16:04:44] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskFlattenConvolution)
[05/26/2023-16:04:44] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:44] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskConvolution)
[05/26/2023-16:04:44] [V] [TRT] /fc/Gemm Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 0.210944
[05/26/2023-16:04:44] [V] [TRT] /fc/Gemm Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 0.182272
[05/26/2023-16:04:44] [V] [TRT] /fc/Gemm Set Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: 0xa8ef60e712f8ad24
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0xa8ef60e712f8ad24 Time: 0.217527
[05/26/2023-16:04:44] [V] [TRT] /fc/Gemm Set Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xc3cf6e1d1c6aff27
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0xc3cf6e1d1c6aff27 Time: 0.159744
[05/26/2023-16:04:44] [V] [TRT] /fc/Gemm Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 0.221038
[05/26/2023-16:04:44] [V] [TRT] /fc/Gemm Set Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x9808072e706def96
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x9808072e706def96 Time: 0.203483
[05/26/2023-16:04:44] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0xcb8a43f748d8a338 Time: 0.135168
[05/26/2023-16:04:44] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 0.157842
[05/26/2023-16:04:44] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0xa31d27de74b895ff Time: 0.0836023
[05/26/2023-16:04:44] [V] [TRT] /fc/Gemm Set Tactic Name: sm70_xmma_fprop_conv1x1_f32f32_f32_f32_nchwkcrs_nchw_simt_small_batch_bias_relu Tactic: 0xaa0953b1a73b0b9b
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0xaa0953b1a73b0b9b Time: 0.0168879
[05/26/2023-16:04:44] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfff46c7893896eb1
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0xfff46c7893896eb1 Time: 0.388096
[05/26/2023-16:04:44] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x40a12e3938221818 Time: 0.18325
[05/26/2023-16:04:44] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x7f0145cb49517338
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0x7f0145cb49517338 Time: 0.171593
[05/26/2023-16:04:44] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa419b3b68f2da07b
[05/26/2023-16:04:44] [V] [TRT] Tactic: 0xa419b3b68f2da07b Time: 0.170715
[05/26/2023-16:04:44] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9cd5cdc35441c505
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x9cd5cdc35441c505 Time: 0.121417
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fc87d7eb370bb7a
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x1fc87d7eb370bb7a Time: 0.0869669
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x12dbf7d94ee3696d Time: 0.135959
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xe5603263b7f00303
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0xe5603263b7f00303 Time: 0.135387
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x828d0ea88c66fce7
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x828d0ea88c66fce7 Time: 0.0996937
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9de226a0c44627c4
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x9de226a0c44627c4 Time: 0.228206
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0xa9366041633a5135 Time: 0.205678
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x8e3884f0eaec3ecd
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x8e3884f0eaec3ecd Time: 0.133559
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0xbb8c3889c7eacd30 Time: 0.396581
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0xb0bf940d5e0f9f45 Time: 0.0610987
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xc0b05b61d128e46e
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0xc0b05b61d128e46e Time: 0.121637
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x9d9fdb5fd9945f64 Time: 0.0817006
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x5aa723e0481da855 Time: 0.121755
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x2ee10e11d6651675
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x2ee10e11d6651675 Time: 0.232009
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x865894c4635db7fd
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x865894c4635db7fd Time: 0.0857234
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90f8f2915f87ed77
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x90f8f2915f87ed77 Time: 0.0625128
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 0.173056
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 0.217966
[05/26/2023-16:04:45] [V] [TRT] Fastest Tactic: 0xaa0953b1a73b0b9b Time: 0.0168879
[05/26/2023-16:04:45] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskGemmConvolution Tactic: 0x0000000000020460
[05/26/2023-16:04:45] [V] [TRT] *************** Autotuning format combination: Float(2048,1,2048,2048) -> Float(1000,1,1000,1000) ***************
[05/26/2023-16:04:45] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CublasConvolution)
[05/26/2023-16:04:45] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:45] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskFlattenConvolution)
[05/26/2023-16:04:45] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:45] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskConvolution)
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0x17173deba0b64484
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x17173deba0b64484 Time: 0.218697
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x3e2b881168d9689d Time: 0.0703147
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0xf90060ce8193b811 Time: 0.215666
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0xd9031472c05adf51 Time: 0.218405
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xe47307053a42b3e4
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0xe47307053a42b3e4 Time: 0.212992
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x412c44dfeaf9161d Time: 0.128242
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x27b316f52c109002
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x27b316f52c109002 Time: 0.130779
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xae0c89d047932ba3
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0xae0c89d047932ba3 Time: 0.113225
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x7bc32c782b800c48 Time: 0.213285
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x35f26f9c09557d86
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x35f26f9c09557d86 Time: 0.11125
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xc7b3afceb5fb03c0
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0xc7b3afceb5fb03c0 Time: 0.115931
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x7121ec1db3f80c67 Time: 0.0552472
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x5030121339a48bf3 Time: 0.221623
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd9eb6ca56ddc3a22
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0xd9eb6ca56ddc3a22 Time: 0.077824
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xbc0bba0ff1a92939
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0xbc0bba0ff1a92939 Time: 0.194706
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 0.0836754
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x0a143be7a52f301a Time: 0.0652434
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x62835fce994f06dd Time: 0.0777737
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xc7feb33970feefa7
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0xc7feb33970feefa7 Time: 0.0689006
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x1022069e6f8d9aeb
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x1022069e6f8d9aeb Time: 0.115639
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90898977fc8ce537
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x90898977fc8ce537 Time: 0.07576
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x92ed3100c35fc43e Time: 0.0671939
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x4fd3c46622e98342 Time: 0.0949394
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd55ee6fd0b56f808
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0xd55ee6fd0b56f808 Time: 0.111835
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x1da91d865428f237 Time: 0.115797
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0xa6448a1e79f1ca6f Time: 0.0955977
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fb90698107bb33a
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x1fb90698107bb33a Time: 0.0532404
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x55d80c17b1cd982d
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x55d80c17b1cd982d Time: 0.0762149
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x3e191488237fab8f Time: 0.07168
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x8014228ec08b4d49 Time: 0.190757
[05/26/2023-16:04:45] [V] [TRT] Fastest Tactic: 0x1fb90698107bb33a Time: 0.0532404
[05/26/2023-16:04:45] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1fb90698107bb33a
[05/26/2023-16:04:45] [V] [TRT] *************** Autotuning format combination: Float(512,1:4,512,512) -> Float(250,1:4,250,250) ***************
[05/26/2023-16:04:45] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CudaDepthwiseConvolution)
[05/26/2023-16:04:45] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:45] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CublasConvolution)
[05/26/2023-16:04:45] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm: 52 available tactics, 2 unparsable, 25 pruned, 27 remaining after tactic pruning.
[05/26/2023-16:04:45] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskGemmConvolution)
[05/26/2023-16:04:45] [V] [TRT] Set Tactic Name: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage6_warpsize2x2x1_tensor16x8x8 Tactic: 0x0000000000020386 numSplitK: 1 numBuffers: 0 numKernels: 1
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x0000000000020386 Time: 0.0176036
[05/26/2023-16:04:45] [V] [TRT] Set Tactic Name: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x2x1_tensor16x8x8 Tactic: 0x00000000000204ec numSplitK: 1 numBuffers: 0 numKernels: 1
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x00000000000204ec Time: 0.0247954
[05/26/2023-16:04:45] [V] [TRT] Set Tactic Name: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage6_warpsize2x2x1_tensor16x8x8 Tactic: 0x0000000002200386 numSplitK: 16 numBuffers: 1 numKernels: 1
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x0000000002200386 Time: 0.0267947
[05/26/2023-16:04:45] [V] [TRT] Set Tactic Name: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x2x1_tensor16x8x8 Tactic: 0x00000000182004ec numSplitK: 16 numBuffers: 12 numKernels: 1
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x00000000182004ec Time: 0.0380091
[05/26/2023-16:04:45] [V] [TRT] Set Tactic Name: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage6_warpsize2x2x1_tensor16x8x8 Tactic: 0x0000000008200386 numSplitK: 16 numBuffers: 4 numKernels: 1
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x0000000008200386 Time: 0.0268488
[05/26/2023-16:04:45] [V] [TRT] Set Tactic Name: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage6_warpsize2x2x1_tensor16x8x8 Tactic: 0x0000000010200386 numSplitK: 16 numBuffers: 8 numKernels: 1
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x0000000010200386 Time: 0.0278019
[05/26/2023-16:04:45] [V] [TRT] Set Tactic Name: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage6_warpsize2x2x1_tensor16x8x8 Tactic: 0x0000000018200386 numSplitK: 16 numBuffers: 12 numKernels: 1
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x0000000018200386 Time: 0.0288055
[05/26/2023-16:04:45] [V] [TRT] Set Tactic Name: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x2x1_tensor16x8x8 Tactic: 0x00000000020804ec numSplitK: 4 numBuffers: 1 numKernels: 1
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x00000000020804ec Time: 0.020689
[05/26/2023-16:04:45] [V] [TRT] Set Tactic Name: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x2x1_tensor16x8x8 Tactic: 0x00000000040804ec numSplitK: 4 numBuffers: 2 numKernels: 1
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x00000000040804ec Time: 0.0209398
[05/26/2023-16:04:45] [V] [TRT] Set Tactic Name: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x2x1_tensor16x8x8 Tactic: 0x00000000060804ec numSplitK: 4 numBuffers: 3 numKernels: 1
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x00000000060804ec Time: 0.021504
[05/26/2023-16:04:45] [V] [TRT] Set Tactic Name: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x2x1_tensor16x8x8 Tactic: 0x00000000021004ec numSplitK: 8 numBuffers: 1 numKernels: 1
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x00000000021004ec Time: 0.0247665
[05/26/2023-16:04:45] [V] [TRT] Set Tactic Name: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x2x1_tensor16x8x8 Tactic: 0x00000000041004ec numSplitK: 8 numBuffers: 2 numKernels: 1
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x00000000041004ec Time: 0.0253501
[05/26/2023-16:04:45] [V] [TRT] Set Tactic Name: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x2x1_tensor16x8x8 Tactic: 0x00000000081004ec numSplitK: 8 numBuffers: 4 numKernels: 1
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x00000000081004ec Time: 0.0266789
[05/26/2023-16:04:45] [V] [TRT] Set Tactic Name: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x2x1_tensor16x8x8 Tactic: 0x000000000c1004ec numSplitK: 8 numBuffers: 6 numKernels: 1
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x000000000c1004ec Time: 0.027507
[05/26/2023-16:04:45] [V] [TRT] Set Tactic Name: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x2x1_tensor16x8x8 Tactic: 0x00000000021804ec numSplitK: 12 numBuffers: 1 numKernels: 1
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x00000000021804ec Time: 0.0299301
[05/26/2023-16:04:45] [V] [TRT] Set Tactic Name: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x2x1_tensor16x8x8 Tactic: 0x00000000081804ec numSplitK: 12 numBuffers: 4 numKernels: 1
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x00000000081804ec Time: 0.0308855
[05/26/2023-16:04:45] [V] [TRT] Set Tactic Name: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x2x1_tensor16x8x8 Tactic: 0x000000000c1804ec numSplitK: 12 numBuffers: 6 numKernels: 1
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x000000000c1804ec Time: 0.0319195
[05/26/2023-16:04:45] [V] [TRT] Set Tactic Name: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x2x1_tensor16x8x8 Tactic: 0x00000000101804ec numSplitK: 12 numBuffers: 8 numKernels: 1
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x00000000101804ec Time: 0.0326135
[05/26/2023-16:04:45] [V] [TRT] Set Tactic Name: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x2x1_tensor16x8x8 Tactic: 0x00000000022004ec numSplitK: 16 numBuffers: 1 numKernels: 1
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x00000000022004ec Time: 0.0325925
[05/26/2023-16:04:45] [V] [TRT] Set Tactic Name: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x2x1_tensor16x8x8 Tactic: 0x00000000082004ec numSplitK: 16 numBuffers: 4 numKernels: 1
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x00000000082004ec Time: 0.0328795
[05/26/2023-16:04:45] [V] [TRT] Set Tactic Name: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x2x1_tensor16x8x8 Tactic: 0x00000000102004ec numSplitK: 16 numBuffers: 8 numKernels: 1
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x00000000102004ec Time: 0.0359863
[05/26/2023-16:04:45] [V] [TRT] Set Tactic Name: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage6_warpsize2x2x1_tensor16x8x8 Tactic: 0x000000000c180386 numSplitK: 12 numBuffers: 6 numKernels: 1
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x000000000c180386 Time: 0.0251611
[05/26/2023-16:04:45] [V] [TRT] Set Tactic Name: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage6_warpsize2x2x1_tensor16x8x8 Tactic: 0x0000000002040386 numSplitK: 2 numBuffers: 1 numKernels: 1
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x0000000002040386 Time: 0.0181531
[05/26/2023-16:04:45] [V] [TRT] Set Tactic Name: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage6_warpsize2x2x1_tensor16x8x8 Tactic: 0x0000000002080386 numSplitK: 4 numBuffers: 1 numKernels: 1
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x0000000002080386 Time: 0.0195137
[05/26/2023-16:04:45] [V] [TRT] Set Tactic Name: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage6_warpsize2x2x1_tensor16x8x8 Tactic: 0x0000000004080386 numSplitK: 4 numBuffers: 2 numKernels: 1
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x0000000004080386 Time: 0.0194743
[05/26/2023-16:04:45] [V] [TRT] Set Tactic Name: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage6_warpsize2x2x1_tensor16x8x8 Tactic: 0x0000000006080386 numSplitK: 4 numBuffers: 3 numKernels: 1
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x0000000006080386 Time: 0.0196389
[05/26/2023-16:04:45] [V] [TRT] Set Tactic Name: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage6_warpsize2x2x1_tensor16x8x8 Tactic: 0x00000000020c0386 numSplitK: 6 numBuffers: 1 numKernels: 1
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x00000000020c0386 Time: 0.0212082
[05/26/2023-16:04:45] [V] [TRT] Fastest Tactic: 0x0000000000020386 Time: 0.0176036
[05/26/2023-16:04:45] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskFlattenConvolution)
[05/26/2023-16:04:45] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[05/26/2023-16:04:45] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskConvolution)
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x9dece0dc37e90462
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x9dece0dc37e90462 Time: 0.142775
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x130df49cb195156b
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x130df49cb195156b Time: 0.143067
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 0.151698
[05/26/2023-16:04:45] [V] [TRT] /fc/Gemm Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 0.145993
[05/26/2023-16:04:45] [V] [TRT] Fastest Tactic: 0x9dece0dc37e90462 Time: 0.142775
[05/26/2023-16:04:45] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskGemmConvolution Tactic: 0x0000000000020386
[05/26/2023-16:04:45] [V] [TRT] =============== Computing costs for 
[05/26/2023-16:04:45] [V] [TRT] *************** Autotuning format combination: Float(1000,1,1,1) -> Float(1000,1) ***************
[05/26/2023-16:04:45] [V] [TRT] --------------- Timing Runner: reshape_after_/fc/Gemm (Shuffle)
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00276053
[05/26/2023-16:04:45] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.0084281
[05/26/2023-16:04:45] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00276053
[05/26/2023-16:04:45] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000
[05/26/2023-16:04:45] [V] [TRT] Adding reformat layer: Reformatted Input Tensor 0 to /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu (/layer3/layer3.0/relu/Relu_output_0) from Float(200704,784,28,1) to Float(200704,1,7168,256)
[05/26/2023-16:04:45] [V] [TRT] Adding reformat layer: Reformatted Input Tensor 0 to /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu (/layer3/layer3.0/relu_1/Relu_output_0) from Float(50176,1,3584,256) to Float(50176,196,14,1)
[05/26/2023-16:04:45] [V] [TRT] Adding reformat layer: Reformatted Input Tensor 0 to /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu (/layer4/layer4.0/relu/Relu_output_0) from Float(100352,196,14,1) to Float(100352,1,7168,512)
[05/26/2023-16:04:45] [V] [TRT] Adding reformat layer: Reformatted Input Tensor 0 to /layer4/layer4.0/downsample/downsample.0/Conv (/layer3/layer3.5/relu_2/Relu_output_0) from Float(200704,196,14,1) to Float(200704,1,14336,1024)
[05/26/2023-16:04:45] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu (/layer4/layer4.0/relu_2/Relu_output_0) from Float(100352,1,14336,2048) to Float(100352,49,7,1)
[05/26/2023-16:04:45] [V] [TRT] Formats and tactics selection completed in 8.98104 seconds.
[05/26/2023-16:04:45] [V] [TRT] After reformat layers: 62 layers
[05/26/2023-16:04:45] [V] [TRT] Total number of blocks in pre-optimized block assignment: 62
[05/26/2023-16:04:45] [I] [TRT] Total Activation Memory: 2195360256
[05/26/2023-16:04:45] [I] [TRT] Detected 1 inputs and 1 output network tensors.
[05/26/2023-16:04:45] [V] [TRT] /conv1/Conv + /relu/Relu Set Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7
[05/26/2023-16:04:45] [V] [TRT] /maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll4_tThreads255 Tactic: 0xfcb5fcaa68fff7ac
[05/26/2023-16:04:45] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:45] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a
[05/26/2023-16:04:45] [V] [TRT] /layer1/layer1.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fc87d7eb370bb7a
[05/26/2023-16:04:45] [V] [TRT] /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x865894c4635db7fd
[05/26/2023-16:04:45] [V] [TRT] /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:45] [V] [TRT] /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/relu_1/Relu Set Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a
[05/26/2023-16:04:45] [V] [TRT] /layer1/layer1.1/conv3/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x865894c4635db7fd
[05/26/2023-16:04:45] [V] [TRT] /layer1/layer1.2/conv1/Conv + /layer1/layer1.2/relu/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:45] [V] [TRT] /layer1/layer1.2/conv2/Conv + /layer1/layer1.2/relu_1/Relu Set Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a
[05/26/2023-16:04:45] [V] [TRT] /layer1/layer1.2/conv3/Conv + /layer1/layer1.2/Add + /layer1/layer1.2/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x865894c4635db7fd
[05/26/2023-16:04:45] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xc0b05b61d128e46e
[05/26/2023-16:04:45] [V] [TRT] /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fc87d7eb370bb7a
[05/26/2023-16:04:45] [V] [TRT] /layer2/layer2.1/conv3/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fc87d7eb370bb7a
[05/26/2023-16:04:45] [V] [TRT] /layer2/layer2.2/conv3/Conv + /layer2/layer2.2/Add + /layer2/layer2.2/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fc87d7eb370bb7a
[05/26/2023-16:04:45] [V] [TRT] /layer2/layer2.3/conv3/Conv + /layer2/layer2.3/Add + /layer2/layer2.3/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fc87d7eb370bb7a
[05/26/2023-16:04:45] [V] [TRT] /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434
[05/26/2023-16:04:45] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:45] [V] [TRT] /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:45] [V] [TRT] /layer3/layer3.1/conv3/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:45] [V] [TRT] /layer3/layer3.2/conv3/Conv + /layer3/layer3.2/Add + /layer3/layer3.2/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:45] [V] [TRT] /layer3/layer3.3/conv3/Conv + /layer3/layer3.3/Add + /layer3/layer3.3/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:45] [V] [TRT] /layer3/layer3.4/conv3/Conv + /layer3/layer3.4/Add + /layer3/layer3.4/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:45] [V] [TRT] /layer3/layer3.5/conv3/Conv + /layer3/layer3.5/Add + /layer3/layer3.5/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[05/26/2023-16:04:45] [V] [TRT] /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434
[05/26/2023-16:04:45] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67
[05/26/2023-16:04:45] [V] [TRT] /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fb90698107bb33a
[05/26/2023-16:04:45] [V] [TRT] /layer4/layer4.1/conv3/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45
[05/26/2023-16:04:45] [V] [TRT] /layer4/layer4.2/conv3/Conv + /layer4/layer4.2/Add + /layer4/layer4.2/relu_2/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45
[05/26/2023-16:04:45] [V] [TRT] Set Tactic Name: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_unit_stride Tactic: 0x0000000000020460 numSplitK: 1 numBuffers: 0 numKernels: 1
[05/26/2023-16:04:45] [V] [TRT] Layer: /conv1/Conv + /relu/Relu Host Persistent: 4224 Device Persistent: 75776 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /maxpool/MaxPool Host Persistent: 1344 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Host Persistent: 3264 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Host Persistent: 512 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer1/layer1.0/downsample/downsample.0/Conv Host Persistent: 3264 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Host Persistent: 3264 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Host Persistent: 3264 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/relu_1/Relu Host Persistent: 512 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer1/layer1.1/conv3/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/relu_2/Relu Host Persistent: 3264 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer1/layer1.2/conv1/Conv + /layer1/layer1.2/relu/Relu Host Persistent: 3264 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer1/layer1.2/conv2/Conv + /layer1/layer1.2/relu_1/Relu Host Persistent: 512 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer1/layer1.2/conv3/Conv + /layer1/layer1.2/Add + /layer1/layer1.2/relu_2/Relu Host Persistent: 3264 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu Host Persistent: 32 Device Persistent: 0 Scratch Memory: 4194304
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu Host Persistent: 32 Device Persistent: 0 Scratch Memory: 5808640
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer2/layer2.0/downsample/downsample.0/Conv Host Persistent: 3264 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Host Persistent: 3264 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu Host Persistent: 2192 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu Host Persistent: 2192 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer2/layer2.1/conv3/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/relu_2/Relu Host Persistent: 3264 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer2/layer2.2/conv1/Conv + /layer2/layer2.2/relu/Relu Host Persistent: 2192 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer2/layer2.2/conv2/Conv + /layer2/layer2.2/relu_1/Relu Host Persistent: 2192 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer2/layer2.2/conv3/Conv + /layer2/layer2.2/Add + /layer2/layer2.2/relu_2/Relu Host Persistent: 3264 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer2/layer2.3/conv1/Conv + /layer2/layer2.3/relu/Relu Host Persistent: 2192 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer2/layer2.3/conv2/Conv + /layer2/layer2.3/relu_1/Relu Host Persistent: 2192 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer2/layer2.3/conv3/Conv + /layer2/layer2.3/Add + /layer2/layer2.3/relu_2/Relu Host Persistent: 3264 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu Host Persistent: 32 Device Persistent: 0 Scratch Memory: 2048
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Host Persistent: 3264 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer3/layer3.0/downsample/downsample.0/Conv Host Persistent: 3264 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Host Persistent: 3264 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu Host Persistent: 2192 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu Host Persistent: 2192 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer3/layer3.1/conv3/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/relu_2/Relu Host Persistent: 3264 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer3/layer3.2/conv1/Conv + /layer3/layer3.2/relu/Relu Host Persistent: 2192 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer3/layer3.2/conv2/Conv + /layer3/layer3.2/relu_1/Relu Host Persistent: 2192 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer3/layer3.2/conv3/Conv + /layer3/layer3.2/Add + /layer3/layer3.2/relu_2/Relu Host Persistent: 3264 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer3/layer3.3/conv1/Conv + /layer3/layer3.3/relu/Relu Host Persistent: 2192 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer3/layer3.3/conv2/Conv + /layer3/layer3.3/relu_1/Relu Host Persistent: 2192 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer3/layer3.3/conv3/Conv + /layer3/layer3.3/Add + /layer3/layer3.3/relu_2/Relu Host Persistent: 3264 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer3/layer3.4/conv1/Conv + /layer3/layer3.4/relu/Relu Host Persistent: 2192 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer3/layer3.4/conv2/Conv + /layer3/layer3.4/relu_1/Relu Host Persistent: 2192 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer3/layer3.4/conv3/Conv + /layer3/layer3.4/Add + /layer3/layer3.4/relu_2/Relu Host Persistent: 3264 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer3/layer3.5/conv1/Conv + /layer3/layer3.5/relu/Relu Host Persistent: 2192 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer3/layer3.5/conv2/Conv + /layer3/layer3.5/relu_1/Relu Host Persistent: 2192 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer3/layer3.5/conv3/Conv + /layer3/layer3.5/Add + /layer3/layer3.5/relu_2/Relu Host Persistent: 3264 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu Host Persistent: 32 Device Persistent: 0 Scratch Memory: 134217728
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Host Persistent: 3264 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer4/layer4.0/downsample/downsample.0/Conv Host Persistent: 3264 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Host Persistent: 3264 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu Host Persistent: 32 Device Persistent: 0 Scratch Memory: 4194304
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu Host Persistent: 2192 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer4/layer4.1/conv3/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/relu_2/Relu Host Persistent: 3264 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer4/layer4.2/conv1/Conv + /layer4/layer4.2/relu/Relu Host Persistent: 32 Device Persistent: 0 Scratch Memory: 4194304
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer4/layer4.2/conv2/Conv + /layer4/layer4.2/relu_1/Relu Host Persistent: 2192 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /layer4/layer4.2/conv3/Conv + /layer4/layer4.2/Add + /layer4/layer4.2/relu_2/Relu Host Persistent: 3264 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /avgpool/GlobalAveragePool Host Persistent: 48 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Layer: /fc/Gemm Host Persistent: 6048 Device Persistent: 0 Scratch Memory: 0
[05/26/2023-16:04:45] [V] [TRT] Skipped printing memory information for 6 layers with 0 memory size i.e. Host Persistent + Device Persistent + Scratch Memory == 0.
[05/26/2023-16:04:45] [I] [TRT] Total Host Persistent Memory: 134448
[05/26/2023-16:04:45] [I] [TRT] Total Device Persistent Memory: 75776
[05/26/2023-16:04:45] [I] [TRT] Total Scratch Memory: 134217728
[05/26/2023-16:04:45] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 9 MiB, GPU 1169 MiB
[05/26/2023-16:04:45] [I] [TRT] [BlockAssignment] Started assigning block shifts. This will take 67 steps to complete.
[05/26/2023-16:04:45] [V] [TRT] STILL ALIVE: Started step 51 of 67
[05/26/2023-16:04:45] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 0.48619ms to assign 4 blocks to 67 nodes requiring 139436032 bytes.
[05/26/2023-16:04:45] [V] [TRT] Total number of blocks in optimized block assignment: 4
[05/26/2023-16:04:45] [I] [TRT] Total Activation Memory: 139436032
[05/26/2023-16:04:45] [V] [TRT] Finalize: /conv1/Conv + /relu/Relu Set kernel index: 0
[05/26/2023-16:04:45] [V] [TRT] Finalize: /maxpool/MaxPool Set kernel index: 1
[05/26/2023-16:04:45] [V] [TRT] Finalize: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu Set kernel index: 2
[05/26/2023-16:04:45] [V] [TRT] Finalize: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu Set kernel index: 3
[05/26/2023-16:04:45] [V] [TRT] Finalize: /layer1/layer1.0/downsample/downsample.0/Conv Set kernel index: 4
[05/26/2023-16:04:45] [V] [TRT] Finalize: /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu Set kernel index: 5
[05/26/2023-16:04:45] [V] [TRT] Finalize: /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu Set kernel index: 2
[05/26/2023-16:04:45] [V] [TRT] Finalize: /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/relu_1/Relu Set kernel index: 3
[05/26/2023-16:04:45] [V] [TRT] Finalize: /layer1/layer1.1/conv3/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/relu_2/Relu Set kernel index: 5
[05/26/2023-16:04:45] [V] [TRT] Finalize: /layer1/layer1.2/conv1/Conv + /layer1/layer1.2/relu/Relu Set kernel index: 2
[05/26/2023-16:04:45] [V] [TRT] Finalize: /layer1/layer1.2/conv2/Conv + /layer1/layer1.2/relu_1/Relu Set kernel index: 3
[05/26/2023-16:04:45] [V] [TRT] Finalize: /layer1/layer1.2/conv3/Conv + /layer1/layer1.2/Add + /layer1/layer1.2/relu_2/Relu Set kernel index: 5
[05/26/2023-16:04:45] [V] [TRT] Finalize: /layer2/layer2.0/downsample/downsample.0/Conv Set kernel index: 6
[05/26/2023-16:04:45] [V] [TRT] Finalize: /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu Set kernel index: 4
[05/26/2023-16:04:45] [V] [TRT] Finalize: /layer2/layer2.1/conv3/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/relu_2/Relu Set kernel index: 4
[05/26/2023-16:04:45] [V] [TRT] Finalize: /layer2/layer2.2/conv3/Conv + /layer2/layer2.2/Add + /layer2/layer2.2/relu_2/Relu Set kernel index: 4
[05/26/2023-16:04:45] [V] [TRT] Finalize: /layer2/layer2.3/conv3/Conv + /layer2/layer2.3/Add + /layer2/layer2.3/relu_2/Relu Set kernel index: 4
[05/26/2023-16:04:45] [V] [TRT] Finalize: /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu Set kernel index: 7
[05/26/2023-16:04:45] [V] [TRT] Finalize: /layer3/layer3.0/downsample/downsample.0/Conv Set kernel index: 2
[05/26/2023-16:04:45] [V] [TRT] Finalize: /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu Set kernel index: 2
[05/26/2023-16:04:45] [V] [TRT] Finalize: /layer3/layer3.1/conv3/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/relu_2/Relu Set kernel index: 2
[05/26/2023-16:04:45] [V] [TRT] Finalize: /layer3/layer3.2/conv3/Conv + /layer3/layer3.2/Add + /layer3/layer3.2/relu_2/Relu Set kernel index: 2
[05/26/2023-16:04:45] [V] [TRT] Finalize: /layer3/layer3.3/conv3/Conv + /layer3/layer3.3/Add + /layer3/layer3.3/relu_2/Relu Set kernel index: 2
[05/26/2023-16:04:45] [V] [TRT] Finalize: /layer3/layer3.4/conv3/Conv + /layer3/layer3.4/Add + /layer3/layer3.4/relu_2/Relu Set kernel index: 2
[05/26/2023-16:04:45] [V] [TRT] Finalize: /layer3/layer3.5/conv3/Conv + /layer3/layer3.5/Add + /layer3/layer3.5/relu_2/Relu Set kernel index: 2
[05/26/2023-16:04:45] [V] [TRT] Finalize: /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu Set kernel index: 7
[05/26/2023-16:04:45] [V] [TRT] Finalize: /layer4/layer4.0/downsample/downsample.0/Conv Set kernel index: 8
[05/26/2023-16:04:45] [V] [TRT] Finalize: /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu Set kernel index: 9
[05/26/2023-16:04:45] [V] [TRT] Finalize: /layer4/layer4.1/conv3/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/relu_2/Relu Set kernel index: 10
[05/26/2023-16:04:45] [V] [TRT] Finalize: /layer4/layer4.2/conv3/Conv + /layer4/layer4.2/Add + /layer4/layer4.2/relu_2/Relu Set kernel index: 10
[05/26/2023-16:04:45] [V] [TRT] Finalize: /fc/Gemm Set kernel index: 11
[05/26/2023-16:04:45] [V] [TRT] Total number of generated kernels selected for the engine: 12
[05/26/2023-16:04:45] [V] [TRT] Kernel: 0 CASK_STATIC
[05/26/2023-16:04:45] [V] [TRT] Kernel: 1 CASK_STATIC
[05/26/2023-16:04:45] [V] [TRT] Kernel: 2 CASK_STATIC
[05/26/2023-16:04:45] [V] [TRT] Kernel: 3 CASK_STATIC
[05/26/2023-16:04:45] [V] [TRT] Kernel: 4 CASK_STATIC
[05/26/2023-16:04:45] [V] [TRT] Kernel: 5 CASK_STATIC
[05/26/2023-16:04:45] [V] [TRT] Kernel: 6 CASK_STATIC
[05/26/2023-16:04:45] [V] [TRT] Kernel: 7 CASK_STATIC
[05/26/2023-16:04:45] [V] [TRT] Kernel: 8 CASK_STATIC
[05/26/2023-16:04:45] [V] [TRT] Kernel: 9 CASK_STATIC
[05/26/2023-16:04:45] [V] [TRT] Kernel: 10 CASK_STATIC
[05/26/2023-16:04:45] [V] [TRT] Kernel: 11 CASK_STATIC
[05/26/2023-16:04:45] [V] [TRT] Disabling unused tactic source: CUBLAS, CUBLAS_LT
[05/26/2023-16:04:45] [V] [TRT] Disabling unused tactic source: JIT_CONVOLUTIONS
[05/26/2023-16:04:45] [V] [TRT] Trying to load shared library libcudnn.so.8
[05/26/2023-16:04:45] [V] [TRT] Loaded shared library libcudnn.so.8
[05/26/2023-16:04:45] [V] [TRT] Using cuDNN as plugin tactic source
[05/26/2023-16:04:45] [V] [TRT] Using cuDNN as core library tactic source
[05/26/2023-16:04:45] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +10, now: CPU 2471, GPU 1341 (MiB)
[05/26/2023-16:04:45] [V] [TRT] Engine generation completed in 9.50705 seconds.
[05/26/2023-16:04:45] [V] [TRT] Deleting timing cache: 188 entries, served 499 hits since creation.
[05/26/2023-16:04:45] [V] [TRT] Engine Layer Information:
Layer(CaskConvolution): /conv1/Conv + /relu/Relu, Tactic: 0x5deb29b7a8e275f7, input0 (Float[1,3,224,224]) -> /relu/Relu_output_0 (Float[1,64,112,112])
Layer(CaskPooling): /maxpool/MaxPool, Tactic: 0xfcb5fcaa68fff7ac, /relu/Relu_output_0 (Float[1,64,112,112]) -> /maxpool/MaxPool_output_0 (Float[1,64,56,56])
Layer(CaskConvolution): /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu, Tactic: 0xa31d27de74b895ff, /maxpool/MaxPool_output_0 (Float[1,64,56,56]) -> /layer1/layer1.0/relu/Relu_output_0 (Float[1,64,56,56])
Layer(CaskConvolution): /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu, Tactic: 0x94119b4c514b211a, /layer1/layer1.0/relu/Relu_output_0 (Float[1,64,56,56]) -> /layer1/layer1.0/relu_1/Relu_output_0 (Float[1,64,56,56])
Layer(CaskConvolution): /layer1/layer1.0/downsample/downsample.0/Conv, Tactic: 0x1fc87d7eb370bb7a, /maxpool/MaxPool_output_0 (Float[1,64,56,56]) -> /layer1/layer1.0/downsample/downsample.0/Conv_output_0 (Float[1,256,56,56])
Layer(CaskConvolution): /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu, Tactic: 0x865894c4635db7fd, /layer1/layer1.0/relu_1/Relu_output_0 (Float[1,64,56,56]), /layer1/layer1.0/downsample/downsample.0/Conv_output_0 (Float[1,256,56,56]) -> /layer1/layer1.0/relu_2/Relu_output_0 (Float[1,256,56,56])
Layer(CaskConvolution): /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu, Tactic: 0xa31d27de74b895ff, /layer1/layer1.0/relu_2/Relu_output_0 (Float[1,256,56,56]) -> /layer1/layer1.1/relu/Relu_output_0 (Float[1,64,56,56])
Layer(CaskConvolution): /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/relu_1/Relu, Tactic: 0x94119b4c514b211a, /layer1/layer1.1/relu/Relu_output_0 (Float[1,64,56,56]) -> /layer1/layer1.1/relu_1/Relu_output_0 (Float[1,64,56,56])
Layer(CaskConvolution): /layer1/layer1.1/conv3/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/relu_2/Relu, Tactic: 0x865894c4635db7fd, /layer1/layer1.1/relu_1/Relu_output_0 (Float[1,64,56,56]), /layer1/layer1.0/relu_2/Relu_output_0 (Float[1,256,56,56]) -> /layer1/layer1.1/relu_2/Relu_output_0 (Float[1,256,56,56])
Layer(CaskConvolution): /layer1/layer1.2/conv1/Conv + /layer1/layer1.2/relu/Relu, Tactic: 0xa31d27de74b895ff, /layer1/layer1.1/relu_2/Relu_output_0 (Float[1,256,56,56]) -> /layer1/layer1.2/relu/Relu_output_0 (Float[1,64,56,56])
Layer(CaskConvolution): /layer1/layer1.2/conv2/Conv + /layer1/layer1.2/relu_1/Relu, Tactic: 0x94119b4c514b211a, /layer1/layer1.2/relu/Relu_output_0 (Float[1,64,56,56]) -> /layer1/layer1.2/relu_1/Relu_output_0 (Float[1,64,56,56])
Layer(CaskConvolution): /layer1/layer1.2/conv3/Conv + /layer1/layer1.2/Add + /layer1/layer1.2/relu_2/Relu, Tactic: 0x865894c4635db7fd, /layer1/layer1.2/relu_1/Relu_output_0 (Float[1,64,56,56]), /layer1/layer1.1/relu_2/Relu_output_0 (Float[1,256,56,56]) -> /layer1/layer1.2/relu_2/Relu_output_0 (Float[1,256,56,56])
Layer(CudnnConvolution): /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu, Tactic: 0x0000000000000001, /layer1/layer1.2/relu_2/Relu_output_0 (Float[1,256,56,56]) -> /layer2/layer2.0/relu/Relu_output_0 (Float[1,128,56,56])
Layer(CudnnConvolution): /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu, Tactic: 0x0000000000000001, /layer2/layer2.0/relu/Relu_output_0 (Float[1,128,56,56]) -> /layer2/layer2.0/relu_1/Relu_output_0 (Float[1,128,28,28])
Layer(CaskConvolution): /layer2/layer2.0/downsample/downsample.0/Conv, Tactic: 0xc0b05b61d128e46e, /layer1/layer1.2/relu_2/Relu_output_0 (Float[1,256,56,56]) -> /layer2/layer2.0/downsample/downsample.0/Conv_output_0 (Float[1,512,28,28])
Layer(CaskConvolution): /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu, Tactic: 0x1fc87d7eb370bb7a, /layer2/layer2.0/relu_1/Relu_output_0 (Float[1,128,28,28]), /layer2/layer2.0/downsample/downsample.0/Conv_output_0 (Float[1,512,28,28]) -> /layer2/layer2.0/relu_2/Relu_output_0 (Float[1,512,28,28])
Layer(FusedConvActConvolution): /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu, Tactic: 0x000000000059ffff, /layer2/layer2.0/relu_2/Relu_output_0 (Float[1,512,28,28]) -> /layer2/layer2.1/relu/Relu_output_0 (Float[1,128,28,28])
Layer(FusedConvActConvolution): /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu, Tactic: 0x000000000086ffff, /layer2/layer2.1/relu/Relu_output_0 (Float[1,128,28,28]) -> /layer2/layer2.1/relu_1/Relu_output_0 (Float[1,128,28,28])
Layer(CaskConvolution): /layer2/layer2.1/conv3/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/relu_2/Relu, Tactic: 0x1fc87d7eb370bb7a, /layer2/layer2.1/relu_1/Relu_output_0 (Float[1,128,28,28]), /layer2/layer2.0/relu_2/Relu_output_0 (Float[1,512,28,28]) -> /layer2/layer2.1/relu_2/Relu_output_0 (Float[1,512,28,28])
Layer(FusedConvActConvolution): /layer2/layer2.2/conv1/Conv + /layer2/layer2.2/relu/Relu, Tactic: 0x000000000059ffff, /layer2/layer2.1/relu_2/Relu_output_0 (Float[1,512,28,28]) -> /layer2/layer2.2/relu/Relu_output_0 (Float[1,128,28,28])
Layer(FusedConvActConvolution): /layer2/layer2.2/conv2/Conv + /layer2/layer2.2/relu_1/Relu, Tactic: 0x000000000086ffff, /layer2/layer2.2/relu/Relu_output_0 (Float[1,128,28,28]) -> /layer2/layer2.2/relu_1/Relu_output_0 (Float[1,128,28,28])
Layer(CaskConvolution): /layer2/layer2.2/conv3/Conv + /layer2/layer2.2/Add + /layer2/layer2.2/relu_2/Relu, Tactic: 0x1fc87d7eb370bb7a, /layer2/layer2.2/relu_1/Relu_output_0 (Float[1,128,28,28]), /layer2/layer2.1/relu_2/Relu_output_0 (Float[1,512,28,28]) -> /layer2/layer2.2/relu_2/Relu_output_0 (Float[1,512,28,28])
Layer(FusedConvActConvolution): /layer2/layer2.3/conv1/Conv + /layer2/layer2.3/relu/Relu, Tactic: 0x000000000059ffff, /layer2/layer2.2/relu_2/Relu_output_0 (Float[1,512,28,28]) -> /layer2/layer2.3/relu/Relu_output_0 (Float[1,128,28,28])
Layer(FusedConvActConvolution): /layer2/layer2.3/conv2/Conv + /layer2/layer2.3/relu_1/Relu, Tactic: 0x000000000086ffff, /layer2/layer2.3/relu/Relu_output_0 (Float[1,128,28,28]) -> /layer2/layer2.3/relu_1/Relu_output_0 (Float[1,128,28,28])
Layer(CaskConvolution): /layer2/layer2.3/conv3/Conv + /layer2/layer2.3/Add + /layer2/layer2.3/relu_2/Relu, Tactic: 0x1fc87d7eb370bb7a, /layer2/layer2.3/relu_1/Relu_output_0 (Float[1,128,28,28]), /layer2/layer2.2/relu_2/Relu_output_0 (Float[1,512,28,28]) -> /layer2/layer2.3/relu_2/Relu_output_0 (Float[1,512,28,28])
Layer(CudnnConvolution): /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu, Tactic: 0x0000000000000001, /layer2/layer2.3/relu_2/Relu_output_0 (Float[1,512,28,28]) -> /layer3/layer3.0/relu/Relu_output_0 (Float[1,256,28,28])
Layer(Reformat): Reformatting CopyNode for Input Tensor 0 to /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu, Tactic: 0x00000000000003ea, /layer3/layer3.0/relu/Relu_output_0 (Float[1,256,28,28]) -> Reformatted Input Tensor 0 to /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu (Float[1,256,28,28])
Layer(CaskConvolution): /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu, Tactic: 0x5953bec563d26434, Reformatted Input Tensor 0 to /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu (Float[1,256,28,28]) -> /layer3/layer3.0/relu_1/Relu_output_0 (Float[1,256,14,14])
Layer(CaskConvolution): /layer3/layer3.0/downsample/downsample.0/Conv, Tactic: 0xa31d27de74b895ff, /layer2/layer2.3/relu_2/Relu_output_0 (Float[1,512,28,28]) -> /layer3/layer3.0/downsample/downsample.0/Conv_output_0 (Float[1,1024,14,14])
Layer(Reformat): Reformatting CopyNode for Input Tensor 0 to /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu, Tactic: 0x0000000000000000, /layer3/layer3.0/relu_1/Relu_output_0 (Float[1,256,14,14]) -> Reformatted Input Tensor 0 to /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu (Float[1,256,14,14])
Layer(CaskConvolution): /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu, Tactic: 0xa31d27de74b895ff, Reformatted Input Tensor 0 to /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu (Float[1,256,14,14]), /layer3/layer3.0/downsample/downsample.0/Conv_output_0 (Float[1,1024,14,14]) -> /layer3/layer3.0/relu_2/Relu_output_0 (Float[1,1024,14,14])
Layer(FusedConvActConvolution): /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu, Tactic: 0x00000000001fffff, /layer3/layer3.0/relu_2/Relu_output_0 (Float[1,1024,14,14]) -> /layer3/layer3.1/relu/Relu_output_0 (Float[1,256,14,14])
Layer(FusedConvActConvolution): /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu, Tactic: 0x0000000000a2ffff, /layer3/layer3.1/relu/Relu_output_0 (Float[1,256,14,14]) -> /layer3/layer3.1/relu_1/Relu_output_0 (Float[1,256,14,14])
Layer(CaskConvolution): /layer3/layer3.1/conv3/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/relu_2/Relu, Tactic: 0xa31d27de74b895ff, /layer3/layer3.1/relu_1/Relu_output_0 (Float[1,256,14,14]), /layer3/layer3.0/relu_2/Relu_output_0 (Float[1,1024,14,14]) -> /layer3/layer3.1/relu_2/Relu_output_0 (Float[1,1024,14,14])
Layer(FusedConvActConvolution): /layer3/layer3.2/conv1/Conv + /layer3/layer3.2/relu/Relu, Tactic: 0x00000000001fffff, /layer3/layer3.1/relu_2/Relu_output_0 (Float[1,1024,14,14]) -> /layer3/layer3.2/relu/Relu_output_0 (Float[1,256,14,14])
Layer(FusedConvActConvolution): /layer3/layer3.2/conv2/Conv + /layer3/layer3.2/relu_1/Relu, Tactic: 0x0000000000a2ffff, /layer3/layer3.2/relu/Relu_output_0 (Float[1,256,14,14]) -> /layer3/layer3.2/relu_1/Relu_output_0 (Float[1,256,14,14])
Layer(CaskConvolution): /layer3/layer3.2/conv3/Conv + /layer3/layer3.2/Add + /layer3/layer3.2/relu_2/Relu, Tactic: 0xa31d27de74b895ff, /layer3/layer3.2/relu_1/Relu_output_0 (Float[1,256,14,14]), /layer3/layer3.1/relu_2/Relu_output_0 (Float[1,1024,14,14]) -> /layer3/layer3.2/relu_2/Relu_output_0 (Float[1,1024,14,14])
Layer(FusedConvActConvolution): /layer3/layer3.3/conv1/Conv + /layer3/layer3.3/relu/Relu, Tactic: 0x00000000001fffff, /layer3/layer3.2/relu_2/Relu_output_0 (Float[1,1024,14,14]) -> /layer3/layer3.3/relu/Relu_output_0 (Float[1,256,14,14])
Layer(FusedConvActConvolution): /layer3/layer3.3/conv2/Conv + /layer3/layer3.3/relu_1/Relu, Tactic: 0x0000000000a2ffff, /layer3/layer3.3/relu/Relu_output_0 (Float[1,256,14,14]) -> /layer3/layer3.3/relu_1/Relu_output_0 (Float[1,256,14,14])
Layer(CaskConvolution): /layer3/layer3.3/conv3/Conv + /layer3/layer3.3/Add + /layer3/layer3.3/relu_2/Relu, Tactic: 0xa31d27de74b895ff, /layer3/layer3.3/relu_1/Relu_output_0 (Float[1,256,14,14]), /layer3/layer3.2/relu_2/Relu_output_0 (Float[1,1024,14,14]) -> /layer3/layer3.3/relu_2/Relu_output_0 (Float[1,1024,14,14])
Layer(FusedConvActConvolution): /layer3/layer3.4/conv1/Conv + /layer3/layer3.4/relu/Relu, Tactic: 0x00000000001fffff, /layer3/layer3.3/relu_2/Relu_output_0 (Float[1,1024,14,14]) -> /layer3/layer3.4/relu/Relu_output_0 (Float[1,256,14,14])
Layer(FusedConvActConvolution): /layer3/layer3.4/conv2/Conv + /layer3/layer3.4/relu_1/Relu, Tactic: 0x0000000000a2ffff, /layer3/layer3.4/relu/Relu_output_0 (Float[1,256,14,14]) -> /layer3/layer3.4/relu_1/Relu_output_0 (Float[1,256,14,14])
Layer(CaskConvolution): /layer3/layer3.4/conv3/Conv + /layer3/layer3.4/Add + /layer3/layer3.4/relu_2/Relu, Tactic: 0xa31d27de74b895ff, /layer3/layer3.4/relu_1/Relu_output_0 (Float[1,256,14,14]), /layer3/layer3.3/relu_2/Relu_output_0 (Float[1,1024,14,14]) -> /layer3/layer3.4/relu_2/Relu_output_0 (Float[1,1024,14,14])
Layer(FusedConvActConvolution): /layer3/layer3.5/conv1/Conv + /layer3/layer3.5/relu/Relu, Tactic: 0x00000000001fffff, /layer3/layer3.4/relu_2/Relu_output_0 (Float[1,1024,14,14]) -> /layer3/layer3.5/relu/Relu_output_0 (Float[1,256,14,14])
Layer(FusedConvActConvolution): /layer3/layer3.5/conv2/Conv + /layer3/layer3.5/relu_1/Relu, Tactic: 0x0000000000a2ffff, /layer3/layer3.5/relu/Relu_output_0 (Float[1,256,14,14]) -> /layer3/layer3.5/relu_1/Relu_output_0 (Float[1,256,14,14])
Layer(CaskConvolution): /layer3/layer3.5/conv3/Conv + /layer3/layer3.5/Add + /layer3/layer3.5/relu_2/Relu, Tactic: 0xa31d27de74b895ff, /layer3/layer3.5/relu_1/Relu_output_0 (Float[1,256,14,14]), /layer3/layer3.4/relu_2/Relu_output_0 (Float[1,1024,14,14]) -> /layer3/layer3.5/relu_2/Relu_output_0 (Float[1,1024,14,14])
Layer(CudnnConvolution): /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu, Tactic: 0x0000000000000001, /layer3/layer3.5/relu_2/Relu_output_0 (Float[1,1024,14,14]) -> /layer4/layer4.0/relu/Relu_output_0 (Float[1,512,14,14])
Layer(Reformat): Reformatting CopyNode for Input Tensor 0 to /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu, Tactic: 0x0000000000000000, /layer4/layer4.0/relu/Relu_output_0 (Float[1,512,14,14]) -> Reformatted Input Tensor 0 to /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu (Float[1,512,14,14])
Layer(CaskConvolution): /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu, Tactic: 0x5953bec563d26434, Reformatted Input Tensor 0 to /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu (Float[1,512,14,14]) -> /layer4/layer4.0/relu_1/Relu_output_0 (Float[1,512,7,7])
Layer(Reformat): Reformatting CopyNode for Input Tensor 0 to /layer4/layer4.0/downsample/downsample.0/Conv, Tactic: 0x0000000000000000, /layer3/layer3.5/relu_2/Relu_output_0 (Float[1,1024,14,14]) -> Reformatted Input Tensor 0 to /layer4/layer4.0/downsample/downsample.0/Conv (Float[1,1024,14,14])
Layer(CaskConvolution): /layer4/layer4.0/downsample/downsample.0/Conv, Tactic: 0x7121ec1db3f80c67, Reformatted Input Tensor 0 to /layer4/layer4.0/downsample/downsample.0/Conv (Float[1,1024,14,14]) -> /layer4/layer4.0/downsample/downsample.0/Conv_output_0 (Float[1,2048,7,7])
Layer(CaskConvolution): /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu, Tactic: 0x1fb90698107bb33a, /layer4/layer4.0/relu_1/Relu_output_0 (Float[1,512,7,7]), /layer4/layer4.0/downsample/downsample.0/Conv_output_0 (Float[1,2048,7,7]) -> Reformatted Output Tensor 0 to /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu (Float[1,2048,7,7])
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu, Tactic: 0x00000000000003e8, Reformatted Output Tensor 0 to /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu (Float[1,2048,7,7]) -> /layer4/layer4.0/relu_2/Relu_output_0 (Float[1,2048,7,7])
Layer(CudnnConvolution): /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu, Tactic: 0x0000000000000001, /layer4/layer4.0/relu_2/Relu_output_0 (Float[1,2048,7,7]) -> /layer4/layer4.1/relu/Relu_output_0 (Float[1,512,7,7])
Layer(FusedConvActConvolution): /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu, Tactic: 0x000000000024ffff, /layer4/layer4.1/relu/Relu_output_0 (Float[1,512,7,7]) -> /layer4/layer4.1/relu_1/Relu_output_0 (Float[1,512,7,7])
Layer(CaskConvolution): /layer4/layer4.1/conv3/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/relu_2/Relu, Tactic: 0xb0bf940d5e0f9f45, /layer4/layer4.1/relu_1/Relu_output_0 (Float[1,512,7,7]), /layer4/layer4.0/relu_2/Relu_output_0 (Float[1,2048,7,7]) -> /layer4/layer4.1/relu_2/Relu_output_0 (Float[1,2048,7,7])
Layer(CudnnConvolution): /layer4/layer4.2/conv1/Conv + /layer4/layer4.2/relu/Relu, Tactic: 0x0000000000000001, /layer4/layer4.1/relu_2/Relu_output_0 (Float[1,2048,7,7]) -> /layer4/layer4.2/relu/Relu_output_0 (Float[1,512,7,7])
Layer(FusedConvActConvolution): /layer4/layer4.2/conv2/Conv + /layer4/layer4.2/relu_1/Relu, Tactic: 0x000000000024ffff, /layer4/layer4.2/relu/Relu_output_0 (Float[1,512,7,7]) -> /layer4/layer4.2/relu_1/Relu_output_0 (Float[1,512,7,7])
Layer(CaskConvolution): /layer4/layer4.2/conv3/Conv + /layer4/layer4.2/Add + /layer4/layer4.2/relu_2/Relu, Tactic: 0xb0bf940d5e0f9f45, /layer4/layer4.2/relu_1/Relu_output_0 (Float[1,512,7,7]), /layer4/layer4.1/relu_2/Relu_output_0 (Float[1,2048,7,7]) -> /layer4/layer4.2/relu_2/Relu_output_0 (Float[1,2048,7,7])
Layer(CudnnPooling): /avgpool/GlobalAveragePool, Tactic: 0xffffffffffffffff, /layer4/layer4.2/relu_2/Relu_output_0 (Float[1,2048,7,7]) -> /avgpool/GlobalAveragePool_output_0 (Float[1,2048,1,1])
Layer(CaskGemmConvolution): /fc/Gemm, Tactic: 0x0000000000020460, /avgpool/GlobalAveragePool_output_0 (Float[1,2048,1,1]) -> /fc/Gemm_out_tensor (Float[1,1000,1,1])
Layer(NoOp): reshape_after_/fc/Gemm, Tactic: 0x0000000000000000, /fc/Gemm_out_tensor (Float[1,1000,1,1]) -> output0 (Float[1,1000])
[05/26/2023-16:04:45] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +98, now: CPU 0, GPU 98 (MiB)
[05/26/2023-16:04:45] [I] Engine built in 11.3411 sec.
[05/26/2023-16:04:45] [I] [TRT] Loaded engine size: 98 MiB
[05/26/2023-16:04:45] [V] [TRT] Trying to load shared library libcudnn.so.8
[05/26/2023-16:04:45] [V] [TRT] Loaded shared library libcudnn.so.8
[05/26/2023-16:04:45] [V] [TRT] Using cuDNN as plugin tactic source
[05/26/2023-16:04:45] [V] [TRT] Using cuDNN as core library tactic source
[05/26/2023-16:04:45] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +10, now: CPU 2044, GPU 1217 (MiB)
[05/26/2023-16:04:45] [V] [TRT] Deserialization required 12273 microseconds.
[05/26/2023-16:04:45] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +97, now: CPU 0, GPU 97 (MiB)
[05/26/2023-16:04:45] [I] Engine deserialized in 0.0124342 sec.
[05/26/2023-16:04:45] [V] [TRT] Trying to load shared library libcudnn.so.8
[05/26/2023-16:04:45] [V] [TRT] Loaded shared library libcudnn.so.8
[05/26/2023-16:04:45] [V] [TRT] Using cuDNN as plugin tactic source
[05/26/2023-16:04:45] [V] [TRT] Using cuDNN as core library tactic source
[05/26/2023-16:04:45] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 2044, GPU 1217 (MiB)
[05/26/2023-16:04:45] [V] [TRT] Total per-runner device persistent memory is 75776
[05/26/2023-16:04:45] [V] [TRT] Total per-runner host persistent memory is 134448
[05/26/2023-16:04:45] [V] [TRT] Allocated activation device memory of size 139436032
[05/26/2023-16:04:45] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +133, now: CPU 0, GPU 230 (MiB)
[05/26/2023-16:04:45] [I] Setting persistentCacheLimit to 0 bytes.
[05/26/2023-16:04:45] [V] Using enqueueV3.
[05/26/2023-16:04:45] [I] Using random values for input input0
[05/26/2023-16:04:45] [I] Created input binding for input0 with dimensions 1x3x224x224
[05/26/2023-16:04:45] [I] Using random values for output output0
[05/26/2023-16:04:45] [I] Created output binding for output0 with dimensions 1x1000
[05/26/2023-16:04:45] [I] Layer Information:
[05/26/2023-16:04:45] [I] Layers:
Name: /conv1/Conv + /relu/Relu, LayerType: CaskConvolution, Inputs: [ { Name: input0, Location: Device, Dimensions: [1,3,224,224], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /relu/Relu_output_0, Location: Device, Dimensions: [1,64,112,112], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [7,7], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [3,3], PostPadding: [3,3], Stride: [2,2], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Float", "Count": 9408}, Bias: {"Type": "Float", "Count": 64}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: ampere_scudnn_128x64_relu_xregs_large_nn_v1, TacticValue: 0x5deb29b7a8e275f7
Name: /maxpool/MaxPool, LayerType: CaskPooling, Inputs: [ { Name: /relu/Relu_output_0, Location: Device, Dimensions: [1,64,112,112], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /maxpool/MaxPool_output_0, Location: Device, Dimensions: [1,64,56,56], Format/Datatype: Row major linear FP32 }], ParameterType: Pooling, PoolingType: MAX, WindowSize: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], BlendFactor: 0, AverageCountExcludesPadding: 1, TacticName: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll4_tThreads255, TacticValue: 0xfcb5fcaa68fff7ac
Name: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /maxpool/MaxPool_output_0, Location: Device, Dimensions: [1,64,56,56], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer1/layer1.0/relu/Relu_output_0, Location: Device, Dimensions: [1,64,56,56], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Float", "Count": 4096}, Bias: {"Type": "Float", "Count": 64}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4, TacticValue: 0xa31d27de74b895ff
Name: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer1/layer1.0/relu/Relu_output_0, Location: Device, Dimensions: [1,64,56,56], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer1/layer1.0/relu_1/Relu_output_0, Location: Device, Dimensions: [1,64,56,56], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Float", "Count": 36864}, Bias: {"Type": "Float", "Count": 64}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1, TacticValue: 0x94119b4c514b211a
Name: /layer1/layer1.0/downsample/downsample.0/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /maxpool/MaxPool_output_0, Location: Device, Dimensions: [1,64,56,56], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer1/layer1.0/downsample/downsample.0/Conv_output_0, Location: Device, Dimensions: [1,256,56,56], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Float", "Count": 16384}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4, TacticValue: 0x1fc87d7eb370bb7a
Name: /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer1/layer1.0/relu_1/Relu_output_0, Location: Device, Dimensions: [1,64,56,56], Format/Datatype: Row major linear FP32 }, { Name: /layer1/layer1.0/downsample/downsample.0/Conv_output_0, Location: Device, Dimensions: [1,256,56,56], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer1/layer1.0/relu_2/Relu_output_0, Location: Device, Dimensions: [1,256,56,56], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Float", "Count": 16384}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4, TacticValue: 0x865894c4635db7fd
Name: /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer1/layer1.0/relu_2/Relu_output_0, Location: Device, Dimensions: [1,256,56,56], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer1/layer1.1/relu/Relu_output_0, Location: Device, Dimensions: [1,64,56,56], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Float", "Count": 16384}, Bias: {"Type": "Float", "Count": 64}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4, TacticValue: 0xa31d27de74b895ff
Name: /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/relu_1/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer1/layer1.1/relu/Relu_output_0, Location: Device, Dimensions: [1,64,56,56], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer1/layer1.1/relu_1/Relu_output_0, Location: Device, Dimensions: [1,64,56,56], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Float", "Count": 36864}, Bias: {"Type": "Float", "Count": 64}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1, TacticValue: 0x94119b4c514b211a
Name: /layer1/layer1.1/conv3/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/relu_2/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer1/layer1.1/relu_1/Relu_output_0, Location: Device, Dimensions: [1,64,56,56], Format/Datatype: Row major linear FP32 }, { Name: /layer1/layer1.0/relu_2/Relu_output_0, Location: Device, Dimensions: [1,256,56,56], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer1/layer1.1/relu_2/Relu_output_0, Location: Device, Dimensions: [1,256,56,56], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Float", "Count": 16384}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4, TacticValue: 0x865894c4635db7fd
Name: /layer1/layer1.2/conv1/Conv + /layer1/layer1.2/relu/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer1/layer1.1/relu_2/Relu_output_0, Location: Device, Dimensions: [1,256,56,56], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer1/layer1.2/relu/Relu_output_0, Location: Device, Dimensions: [1,64,56,56], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Float", "Count": 16384}, Bias: {"Type": "Float", "Count": 64}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4, TacticValue: 0xa31d27de74b895ff
Name: /layer1/layer1.2/conv2/Conv + /layer1/layer1.2/relu_1/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer1/layer1.2/relu/Relu_output_0, Location: Device, Dimensions: [1,64,56,56], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer1/layer1.2/relu_1/Relu_output_0, Location: Device, Dimensions: [1,64,56,56], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Float", "Count": 36864}, Bias: {"Type": "Float", "Count": 64}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1, TacticValue: 0x94119b4c514b211a
Name: /layer1/layer1.2/conv3/Conv + /layer1/layer1.2/Add + /layer1/layer1.2/relu_2/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer1/layer1.2/relu_1/Relu_output_0, Location: Device, Dimensions: [1,64,56,56], Format/Datatype: Row major linear FP32 }, { Name: /layer1/layer1.1/relu_2/Relu_output_0, Location: Device, Dimensions: [1,256,56,56], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer1/layer1.2/relu_2/Relu_output_0, Location: Device, Dimensions: [1,256,56,56], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Float", "Count": 16384}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4, TacticValue: 0x865894c4635db7fd
Name: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu, LayerType: CudnnConvolution, Inputs: [ { Name: /layer1/layer1.2/relu_2/Relu_output_0, Location: Device, Dimensions: [1,256,56,56], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer2/layer2.0/relu/Relu_output_0, Location: Device, Dimensions: [1,128,56,56], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Float", "Count": 32768}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, TacticValue: 0x0000000000000001
Name: /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu, LayerType: CudnnConvolution, Inputs: [ { Name: /layer2/layer2.0/relu/Relu_output_0, Location: Device, Dimensions: [1,128,56,56], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer2/layer2.0/relu_1/Relu_output_0, Location: Device, Dimensions: [1,128,28,28], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Float", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, TacticValue: 0x0000000000000001
Name: /layer2/layer2.0/downsample/downsample.0/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /layer1/layer1.2/relu_2/Relu_output_0, Location: Device, Dimensions: [1,256,56,56], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer2/layer2.0/downsample/downsample.0/Conv_output_0, Location: Device, Dimensions: [1,512,28,28], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [2,2], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Float", "Count": 131072}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4, TacticValue: 0xc0b05b61d128e46e
Name: /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer2/layer2.0/relu_1/Relu_output_0, Location: Device, Dimensions: [1,128,28,28], Format/Datatype: Row major linear FP32 }, { Name: /layer2/layer2.0/downsample/downsample.0/Conv_output_0, Location: Device, Dimensions: [1,512,28,28], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer2/layer2.0/relu_2/Relu_output_0, Location: Device, Dimensions: [1,512,28,28], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Float", "Count": 65536}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4, TacticValue: 0x1fc87d7eb370bb7a
Name: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu, LayerType: FusedConvActConvolution, Inputs: [ { Name: /layer2/layer2.0/relu_2/Relu_output_0, Location: Device, Dimensions: [1,512,28,28], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer2/layer2.1/relu/Relu_output_0, Location: Device, Dimensions: [1,128,28,28], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Float", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, ConvolutionTacticIndex: 89, TacticValue: 0x000000000059ffff
Name: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu, LayerType: FusedConvActConvolution, Inputs: [ { Name: /layer2/layer2.1/relu/Relu_output_0, Location: Device, Dimensions: [1,128,28,28], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer2/layer2.1/relu_1/Relu_output_0, Location: Device, Dimensions: [1,128,28,28], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Float", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, ConvolutionTacticIndex: 134, TacticValue: 0x000000000086ffff
Name: /layer2/layer2.1/conv3/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/relu_2/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer2/layer2.1/relu_1/Relu_output_0, Location: Device, Dimensions: [1,128,28,28], Format/Datatype: Row major linear FP32 }, { Name: /layer2/layer2.0/relu_2/Relu_output_0, Location: Device, Dimensions: [1,512,28,28], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer2/layer2.1/relu_2/Relu_output_0, Location: Device, Dimensions: [1,512,28,28], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Float", "Count": 65536}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4, TacticValue: 0x1fc87d7eb370bb7a
Name: /layer2/layer2.2/conv1/Conv + /layer2/layer2.2/relu/Relu, LayerType: FusedConvActConvolution, Inputs: [ { Name: /layer2/layer2.1/relu_2/Relu_output_0, Location: Device, Dimensions: [1,512,28,28], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer2/layer2.2/relu/Relu_output_0, Location: Device, Dimensions: [1,128,28,28], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Float", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, ConvolutionTacticIndex: 89, TacticValue: 0x000000000059ffff
Name: /layer2/layer2.2/conv2/Conv + /layer2/layer2.2/relu_1/Relu, LayerType: FusedConvActConvolution, Inputs: [ { Name: /layer2/layer2.2/relu/Relu_output_0, Location: Device, Dimensions: [1,128,28,28], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer2/layer2.2/relu_1/Relu_output_0, Location: Device, Dimensions: [1,128,28,28], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Float", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, ConvolutionTacticIndex: 134, TacticValue: 0x000000000086ffff
Name: /layer2/layer2.2/conv3/Conv + /layer2/layer2.2/Add + /layer2/layer2.2/relu_2/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer2/layer2.2/relu_1/Relu_output_0, Location: Device, Dimensions: [1,128,28,28], Format/Datatype: Row major linear FP32 }, { Name: /layer2/layer2.1/relu_2/Relu_output_0, Location: Device, Dimensions: [1,512,28,28], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer2/layer2.2/relu_2/Relu_output_0, Location: Device, Dimensions: [1,512,28,28], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Float", "Count": 65536}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4, TacticValue: 0x1fc87d7eb370bb7a
Name: /layer2/layer2.3/conv1/Conv + /layer2/layer2.3/relu/Relu, LayerType: FusedConvActConvolution, Inputs: [ { Name: /layer2/layer2.2/relu_2/Relu_output_0, Location: Device, Dimensions: [1,512,28,28], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer2/layer2.3/relu/Relu_output_0, Location: Device, Dimensions: [1,128,28,28], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Float", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, ConvolutionTacticIndex: 89, TacticValue: 0x000000000059ffff
Name: /layer2/layer2.3/conv2/Conv + /layer2/layer2.3/relu_1/Relu, LayerType: FusedConvActConvolution, Inputs: [ { Name: /layer2/layer2.3/relu/Relu_output_0, Location: Device, Dimensions: [1,128,28,28], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer2/layer2.3/relu_1/Relu_output_0, Location: Device, Dimensions: [1,128,28,28], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Float", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, ConvolutionTacticIndex: 134, TacticValue: 0x000000000086ffff
Name: /layer2/layer2.3/conv3/Conv + /layer2/layer2.3/Add + /layer2/layer2.3/relu_2/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer2/layer2.3/relu_1/Relu_output_0, Location: Device, Dimensions: [1,128,28,28], Format/Datatype: Row major linear FP32 }, { Name: /layer2/layer2.2/relu_2/Relu_output_0, Location: Device, Dimensions: [1,512,28,28], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer2/layer2.3/relu_2/Relu_output_0, Location: Device, Dimensions: [1,512,28,28], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Float", "Count": 65536}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4, TacticValue: 0x1fc87d7eb370bb7a
Name: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu, LayerType: CudnnConvolution, Inputs: [ { Name: /layer2/layer2.3/relu_2/Relu_output_0, Location: Device, Dimensions: [1,512,28,28], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer3/layer3.0/relu/Relu_output_0, Location: Device, Dimensions: [1,256,28,28], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Float", "Count": 131072}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, TacticValue: 0x0000000000000001
Name: Reformatting CopyNode for Input Tensor 0 to /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu, LayerType: Reformat, Inputs: [ { Name: /layer3/layer3.0/relu/Relu_output_0, Location: Device, Dimensions: [1,256,28,28], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: Reformatted Input Tensor 0 to /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu, Location: Device, Dimensions: [1,256,28,28], Format/Datatype: Channel major FP32 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x00000000000003ea
Name: /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu, LayerType: CaskConvolution, Inputs: [ { Name: Reformatted Input Tensor 0 to /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu, Location: Device, Dimensions: [1,256,28,28], Format/Datatype: Channel major FP32 format }], Outputs: [ { Name: /layer3/layer3.0/relu_1/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Channel major FP32 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Float", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4, TacticValue: 0x5953bec563d26434
Name: /layer3/layer3.0/downsample/downsample.0/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /layer2/layer2.3/relu_2/Relu_output_0, Location: Device, Dimensions: [1,512,28,28], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer3/layer3.0/downsample/downsample.0/Conv_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [2,2], Dilation: [1,1], OutMaps: 1024, Groups: 1, Weights: {"Type": "Float", "Count": 524288}, Bias: {"Type": "Float", "Count": 1024}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4, TacticValue: 0xa31d27de74b895ff
Name: Reformatting CopyNode for Input Tensor 0 to /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu, LayerType: Reformat, Inputs: [ { Name: /layer3/layer3.0/relu_1/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Channel major FP32 format }], Outputs: [ { Name: Reformatted Input Tensor 0 to /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu, LayerType: CaskConvolution, Inputs: [ { Name: Reformatted Input Tensor 0 to /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }, { Name: /layer3/layer3.0/downsample/downsample.0/Conv_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer3/layer3.0/relu_2/Relu_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 1024, Groups: 1, Weights: {"Type": "Float", "Count": 262144}, Bias: {"Type": "Float", "Count": 1024}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4, TacticValue: 0xa31d27de74b895ff
Name: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu, LayerType: FusedConvActConvolution, Inputs: [ { Name: /layer3/layer3.0/relu_2/Relu_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer3/layer3.1/relu/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Float", "Count": 262144}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, ConvolutionTacticIndex: 31, TacticValue: 0x00000000001fffff
Name: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu, LayerType: FusedConvActConvolution, Inputs: [ { Name: /layer3/layer3.1/relu/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer3/layer3.1/relu_1/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Float", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, ConvolutionTacticIndex: 162, TacticValue: 0x0000000000a2ffff
Name: /layer3/layer3.1/conv3/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/relu_2/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer3/layer3.1/relu_1/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }, { Name: /layer3/layer3.0/relu_2/Relu_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer3/layer3.1/relu_2/Relu_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 1024, Groups: 1, Weights: {"Type": "Float", "Count": 262144}, Bias: {"Type": "Float", "Count": 1024}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4, TacticValue: 0xa31d27de74b895ff
Name: /layer3/layer3.2/conv1/Conv + /layer3/layer3.2/relu/Relu, LayerType: FusedConvActConvolution, Inputs: [ { Name: /layer3/layer3.1/relu_2/Relu_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer3/layer3.2/relu/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Float", "Count": 262144}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, ConvolutionTacticIndex: 31, TacticValue: 0x00000000001fffff
Name: /layer3/layer3.2/conv2/Conv + /layer3/layer3.2/relu_1/Relu, LayerType: FusedConvActConvolution, Inputs: [ { Name: /layer3/layer3.2/relu/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer3/layer3.2/relu_1/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Float", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, ConvolutionTacticIndex: 162, TacticValue: 0x0000000000a2ffff
Name: /layer3/layer3.2/conv3/Conv + /layer3/layer3.2/Add + /layer3/layer3.2/relu_2/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer3/layer3.2/relu_1/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }, { Name: /layer3/layer3.1/relu_2/Relu_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer3/layer3.2/relu_2/Relu_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 1024, Groups: 1, Weights: {"Type": "Float", "Count": 262144}, Bias: {"Type": "Float", "Count": 1024}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4, TacticValue: 0xa31d27de74b895ff
Name: /layer3/layer3.3/conv1/Conv + /layer3/layer3.3/relu/Relu, LayerType: FusedConvActConvolution, Inputs: [ { Name: /layer3/layer3.2/relu_2/Relu_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer3/layer3.3/relu/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Float", "Count": 262144}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, ConvolutionTacticIndex: 31, TacticValue: 0x00000000001fffff
Name: /layer3/layer3.3/conv2/Conv + /layer3/layer3.3/relu_1/Relu, LayerType: FusedConvActConvolution, Inputs: [ { Name: /layer3/layer3.3/relu/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer3/layer3.3/relu_1/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Float", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, ConvolutionTacticIndex: 162, TacticValue: 0x0000000000a2ffff
Name: /layer3/layer3.3/conv3/Conv + /layer3/layer3.3/Add + /layer3/layer3.3/relu_2/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer3/layer3.3/relu_1/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }, { Name: /layer3/layer3.2/relu_2/Relu_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer3/layer3.3/relu_2/Relu_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 1024, Groups: 1, Weights: {"Type": "Float", "Count": 262144}, Bias: {"Type": "Float", "Count": 1024}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4, TacticValue: 0xa31d27de74b895ff
Name: /layer3/layer3.4/conv1/Conv + /layer3/layer3.4/relu/Relu, LayerType: FusedConvActConvolution, Inputs: [ { Name: /layer3/layer3.3/relu_2/Relu_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer3/layer3.4/relu/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Float", "Count": 262144}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, ConvolutionTacticIndex: 31, TacticValue: 0x00000000001fffff
Name: /layer3/layer3.4/conv2/Conv + /layer3/layer3.4/relu_1/Relu, LayerType: FusedConvActConvolution, Inputs: [ { Name: /layer3/layer3.4/relu/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer3/layer3.4/relu_1/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Float", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, ConvolutionTacticIndex: 162, TacticValue: 0x0000000000a2ffff
Name: /layer3/layer3.4/conv3/Conv + /layer3/layer3.4/Add + /layer3/layer3.4/relu_2/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer3/layer3.4/relu_1/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }, { Name: /layer3/layer3.3/relu_2/Relu_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer3/layer3.4/relu_2/Relu_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 1024, Groups: 1, Weights: {"Type": "Float", "Count": 262144}, Bias: {"Type": "Float", "Count": 1024}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4, TacticValue: 0xa31d27de74b895ff
Name: /layer3/layer3.5/conv1/Conv + /layer3/layer3.5/relu/Relu, LayerType: FusedConvActConvolution, Inputs: [ { Name: /layer3/layer3.4/relu_2/Relu_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer3/layer3.5/relu/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Float", "Count": 262144}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, ConvolutionTacticIndex: 31, TacticValue: 0x00000000001fffff
Name: /layer3/layer3.5/conv2/Conv + /layer3/layer3.5/relu_1/Relu, LayerType: FusedConvActConvolution, Inputs: [ { Name: /layer3/layer3.5/relu/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer3/layer3.5/relu_1/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Float", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, ConvolutionTacticIndex: 162, TacticValue: 0x0000000000a2ffff
Name: /layer3/layer3.5/conv3/Conv + /layer3/layer3.5/Add + /layer3/layer3.5/relu_2/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer3/layer3.5/relu_1/Relu_output_0, Location: Device, Dimensions: [1,256,14,14], Format/Datatype: Row major linear FP32 }, { Name: /layer3/layer3.4/relu_2/Relu_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer3/layer3.5/relu_2/Relu_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 1024, Groups: 1, Weights: {"Type": "Float", "Count": 262144}, Bias: {"Type": "Float", "Count": 1024}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4, TacticValue: 0xa31d27de74b895ff
Name: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu, LayerType: CudnnConvolution, Inputs: [ { Name: /layer3/layer3.5/relu_2/Relu_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer4/layer4.0/relu/Relu_output_0, Location: Device, Dimensions: [1,512,14,14], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Float", "Count": 524288}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, TacticValue: 0x0000000000000001
Name: Reformatting CopyNode for Input Tensor 0 to /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu, LayerType: Reformat, Inputs: [ { Name: /layer4/layer4.0/relu/Relu_output_0, Location: Device, Dimensions: [1,512,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: Reformatted Input Tensor 0 to /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu, Location: Device, Dimensions: [1,512,14,14], Format/Datatype: Channel major FP32 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu, LayerType: CaskConvolution, Inputs: [ { Name: Reformatted Input Tensor 0 to /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu, Location: Device, Dimensions: [1,512,14,14], Format/Datatype: Channel major FP32 format }], Outputs: [ { Name: /layer4/layer4.0/relu_1/Relu_output_0, Location: Device, Dimensions: [1,512,7,7], Format/Datatype: Channel major FP32 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Float", "Count": 2359296}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4, TacticValue: 0x5953bec563d26434
Name: Reformatting CopyNode for Input Tensor 0 to /layer4/layer4.0/downsample/downsample.0/Conv, LayerType: Reformat, Inputs: [ { Name: /layer3/layer3.5/relu_2/Relu_output_0, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: Reformatted Input Tensor 0 to /layer4/layer4.0/downsample/downsample.0/Conv, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Channel major FP32 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: /layer4/layer4.0/downsample/downsample.0/Conv, LayerType: CaskConvolution, Inputs: [ { Name: Reformatted Input Tensor 0 to /layer4/layer4.0/downsample/downsample.0/Conv, Location: Device, Dimensions: [1,1024,14,14], Format/Datatype: Channel major FP32 format }], Outputs: [ { Name: /layer4/layer4.0/downsample/downsample.0/Conv_output_0, Location: Device, Dimensions: [1,2048,7,7], Format/Datatype: Channel major FP32 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [2,2], Dilation: [1,1], OutMaps: 2048, Groups: 1, Weights: {"Type": "Float", "Count": 2097152}, Bias: {"Type": "Float", "Count": 2048}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4, TacticValue: 0x7121ec1db3f80c67
Name: /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer4/layer4.0/relu_1/Relu_output_0, Location: Device, Dimensions: [1,512,7,7], Format/Datatype: Channel major FP32 format }, { Name: /layer4/layer4.0/downsample/downsample.0/Conv_output_0, Location: Device, Dimensions: [1,2048,7,7], Format/Datatype: Channel major FP32 format }], Outputs: [ { Name: Reformatted Output Tensor 0 to /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu, Location: Device, Dimensions: [1,2048,7,7], Format/Datatype: Channel major FP32 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 2048, Groups: 1, Weights: {"Type": "Float", "Count": 1048576}, Bias: {"Type": "Float", "Count": 2048}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4, TacticValue: 0x1fb90698107bb33a
Name: Reformatting CopyNode for Output Tensor 0 to /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu, Location: Device, Dimensions: [1,2048,7,7], Format/Datatype: Channel major FP32 format }], Outputs: [ { Name: /layer4/layer4.0/relu_2/Relu_output_0, Location: Device, Dimensions: [1,2048,7,7], Format/Datatype: Row major linear FP32 }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x00000000000003e8
Name: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu, LayerType: CudnnConvolution, Inputs: [ { Name: /layer4/layer4.0/relu_2/Relu_output_0, Location: Device, Dimensions: [1,2048,7,7], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer4/layer4.1/relu/Relu_output_0, Location: Device, Dimensions: [1,512,7,7], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Float", "Count": 1048576}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, TacticValue: 0x0000000000000001
Name: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu, LayerType: FusedConvActConvolution, Inputs: [ { Name: /layer4/layer4.1/relu/Relu_output_0, Location: Device, Dimensions: [1,512,7,7], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer4/layer4.1/relu_1/Relu_output_0, Location: Device, Dimensions: [1,512,7,7], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Float", "Count": 2359296}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, ConvolutionTacticIndex: 36, TacticValue: 0x000000000024ffff
Name: /layer4/layer4.1/conv3/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/relu_2/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer4/layer4.1/relu_1/Relu_output_0, Location: Device, Dimensions: [1,512,7,7], Format/Datatype: Row major linear FP32 }, { Name: /layer4/layer4.0/relu_2/Relu_output_0, Location: Device, Dimensions: [1,2048,7,7], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer4/layer4.1/relu_2/Relu_output_0, Location: Device, Dimensions: [1,2048,7,7], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 2048, Groups: 1, Weights: {"Type": "Float", "Count": 1048576}, Bias: {"Type": "Float", "Count": 2048}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4, TacticValue: 0xb0bf940d5e0f9f45
Name: /layer4/layer4.2/conv1/Conv + /layer4/layer4.2/relu/Relu, LayerType: CudnnConvolution, Inputs: [ { Name: /layer4/layer4.1/relu_2/Relu_output_0, Location: Device, Dimensions: [1,2048,7,7], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer4/layer4.2/relu/Relu_output_0, Location: Device, Dimensions: [1,512,7,7], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Float", "Count": 1048576}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, TacticValue: 0x0000000000000001
Name: /layer4/layer4.2/conv2/Conv + /layer4/layer4.2/relu_1/Relu, LayerType: FusedConvActConvolution, Inputs: [ { Name: /layer4/layer4.2/relu/Relu_output_0, Location: Device, Dimensions: [1,512,7,7], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer4/layer4.2/relu_1/Relu_output_0, Location: Device, Dimensions: [1,512,7,7], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Float", "Count": 2359296}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, ConvolutionTacticIndex: 36, TacticValue: 0x000000000024ffff
Name: /layer4/layer4.2/conv3/Conv + /layer4/layer4.2/Add + /layer4/layer4.2/relu_2/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /layer4/layer4.2/relu_1/Relu_output_0, Location: Device, Dimensions: [1,512,7,7], Format/Datatype: Row major linear FP32 }, { Name: /layer4/layer4.1/relu_2/Relu_output_0, Location: Device, Dimensions: [1,2048,7,7], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /layer4/layer4.2/relu_2/Relu_output_0, Location: Device, Dimensions: [1,2048,7,7], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 2048, Groups: 1, Weights: {"Type": "Float", "Count": 1048576}, Bias: {"Type": "Float", "Count": 2048}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4, TacticValue: 0xb0bf940d5e0f9f45
Name: /avgpool/GlobalAveragePool, LayerType: CudnnPooling, Inputs: [ { Name: /layer4/layer4.2/relu_2/Relu_output_0, Location: Device, Dimensions: [1,2048,7,7], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /avgpool/GlobalAveragePool_output_0, Location: Device, Dimensions: [1,2048,1,1], Format/Datatype: Row major linear FP32 }], ParameterType: Pooling, PoolingType: AVERAGE, WindowSize: [7,7], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], BlendFactor: 0, AverageCountExcludesPadding: 1, TacticValue: 0xffffffffffffffff
Name: /fc/Gemm, LayerType: CaskGemmConvolution, Inputs: [ { Name: /avgpool/GlobalAveragePool_output_0, Location: Device, Dimensions: [1,2048,1,1], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /fc/Gemm_out_tensor, Location: Device, Dimensions: [1,1000,1,1], Format/Datatype: Row major linear FP32 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 1000, Groups: 1, Weights: {"Type": "Float", "Count": 2048000}, Bias: {"Type": "Float", "Count": 1000}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm50_xmma_cublas_gemvx_f32f32_f32_f32_tn_n_int32_unit_n_launch_param4x32x32_strided_unit_stride, TacticValue: 0x0000000000020460
Name: reshape_after_/fc/Gemm, LayerType: NoOp, Inputs: [ { Name: /fc/Gemm_out_tensor, Location: Device, Dimensions: [1,1000,1,1], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: output0, Location: Device, Dimensions: [1,1000], Format/Datatype: Row major linear FP32 }], TacticValue: 0x0000000000000000

Bindings:
input0
output0
[05/26/2023-16:04:45] [I] Starting inference
[05/26/2023-16:04:48] [I] The e2e network timing is not reported since it is inaccurate due to the extra synchronizations when the profiler is enabled.
[05/26/2023-16:04:48] [I] To show e2e network timing report, add --separateProfileRun to profile layer timing in a separate run or remove --dumpProfile to disable the profiler.
[05/26/2023-16:04:48] [I] Output Tensors:
[05/26/2023-16:04:48] [I] output0: (1x1000)
[05/26/2023-16:04:48] [I] -3.61514 20.5864 11.0214 10.9955 0.469871 10.2569 -20.8846 11.1736 1.4042 6.45995 14.3445 0.91221 -17.3223 11.1886 -9.33976 -0.266301 -2.66785 -1.2273 7.27567 2.4215 -8.39138 22.7788 0.532748 4.39701 1.21249 3.16729 11.9165 -15.8489 2.50388 -7.07671 5.01986 -4.69475 -15.8461 14.8773 2.79506 -1.85218 -16.1026 3.37021 -7.81903 -14.4179 -0.0636368 -13.7821 -12.3901 10.4264 2.53839 -14.1177 10.0553 -11.608 -8.34573 -7.89687 3.07244 -7.78747 -1.68135 -7.66083 -18.5036 3.79383 10.5975 3.26009 -2.25623 5.14645 -7.15223 -4.93529 -6.81551 7.07172 -3.66583 5.99756 16.2912 -2.21476 1.93074 -18.4484 -5.57929 -7.60227 -6.47907 18.0926 -3.20715 -13.0869 -1.00028 -0.999909 -24.3498 -2.12765 -5.6794 5.29786 30.7989 9.22271 -5.75207 -22.9014 -4.66807 13.4157 -18.3165 5.18209 17.0783 6.45506 -2.0124 -17.5931 0.194456 -6.63053 12.3886 -1.34305 -19.5295 4.87452 1.50056 -12.0702 -1.10992 7.56352 -0.799791 3.02006 4.14015 4.95957 1.88931 -1.66887 6.66415 -17.869 -0.598216 7.6126 6.02388 31.2631 13.2972 6.0194 6.25454 -8.42365 -6.58558 -2.6349 -6.47468 0.361755 -7.39555 25.0752 14.5524 -0.356799 9.83181 -18.4363 -6.19991 6.90779 0.186953 -11.6013 5.69616 13.2493 24.8979 -3.22029 -19.9073 -1.21067 -3.46276 -14.2335 0.97074 -18.0945 1.07656 -3.29546 6.71238 -6.35752 3.81764 -1.35331 -7.73418 -5.51072 -1.52701 -13.3458 14.3228 -27.6773 12.2701 10.2538 -5.40543 13.8329 -1.77111 12.7961 14.9134 21.2064 -11.3925 -18.7573 -1.5922 17.3023 -23.0049 2.30465 25.3807 6.19375 10.0725 4.82165 -0.601671 6.88886 -12.3479 3.28114 -14.3472 -25.1817 -1.12925 17.3039 1.20441 -13.8823 -8.49497 -7.29709 9.20095 -0.651281 -9.64182 6.73666 -8.93101 0.0563836 4.52992 21.2885 14.0785 -11.3652 -2.36762 7.77468 -4.82847 10.1022 2.95122 5.37818 -1.30352 -0.467226 6.14346 -9.32022 5.56379 -6.43406 -10.9417 19.2365 5.40593 8.35933 -4.78789 -0.0180093 -3.1675 3.53283 -0.220597 2.16622 -34.8053 -5.46039 5.48391 23.9419 -19.7291 6.44776 -2.79577 -0.32222 3.29026 2.10438 7.35679 3.86183 22.8203 7.21656 5.89943 7.91322 -3.31704 -4.61533 8.45997 2.26265 -9.42314 6.41494 8.29631 8.86901 -1.76397 3.84321 -2.162 -4.03659 -20.4056 -2.40136 10.2492 13.4674 -8.17147 -1.6994 15.5995 9.04113 -3.67875 -3.57743 -17.4031 -28.9528 -9.57329 -23.9697 -2.63503 -15.6086 9.79158 -4.25316 -11.2407 -10.9114 0.583649 -5.47373 -5.46966 18.129 5.14323 15.9895 26.0679 -18.3737 -3.82246 19.8687 -4.8577 6.27197 -8.2189 17.3278 12.0871 -3.17672 -13.475 -16.5719 20.3844 -2.0114 -5.32602 -11.7288 15.6531 -14.5491 9.28674 -3.84718 -0.737823 11.7775 3.35493 -3.91071 12.7301 -5.80079 12.6496 0.533115 -0.998853 -2.75647 -9.62879 -5.92556 -7.06605 10.756 0.0232991 -2.92029 -8.58807 1.12943 -1.49939 8.57768 -25.6641 -11.5327 -10.3914 4.2292 3.06723 4.17305 3.54616 -2.68512 10.1834 -8.59272 -11.7881 -23.9614 6.15331 0.337373 -4.02687 -1.88258 -2.75279 -7.43761 22.0502 -28.0706 -13.5135 -17.0251 4.4169 -2.98445 2.00621 -10.8582 -13.4742 26.1424 -0.148607 6.90982 -19.7207 -4.94349 -1.42505 -11.1037 10.7987 -2.94576 12.5014 7.82407 10.2485 -2.37356 10.855 1.82623 16.0419 -7.57958 -14.0128 9.86934 -0.880015 6.4855 -2.34023 14.4202 -3.01615 7.82779 2.33397 5.99736 11.0976 -2.68717 -10.7136 5.49395 5.79067 -16.9506 -2.222 -2.36339 3.09044 5.37435 -12.9812 5.95623 -5.06856 -13.6488 -0.87705 -5.52981 -2.7546 11.7889 1.12272 1.36535 -14.7137 1.67928 -6.73343 -7.30491 1.16554 5.60998 -0.295571 -5.10509 -11.5007 -18.2209 -2.7267 -19.5646 -6.62082 1.35068 9.53163 -4.75187 12.1226 -7.20931 7.23806 -4.98818 20.3836 -13.8351 0.629596 7.57027 0.511719 -10.4924 -10.9917 -8.64134 13.075 -5.59477 0.77773 -2.59347 0.058621 4.79573 8.82353 9.92916 0.161786 2.57696 6.76486 9.77348 16.2443 13.5981 -11.169 -4.45455 16.4104 7.91393 11.1554 16.5811 0.267288 7.10088 8.85759 15.2648 -22.1897 -6.0551 -6.08153 11.9587 2.47052 18.6309 -0.392699 2.52587 -22.1003 3.49885 6.97781 -22.3409 -18.8118 15.2792 -1.43907 3.04764 -3.52645 9.36657 14.3793 -2.01579 0.96604 3.22347 -6.41403 3.07927 -1.81389 -19.6288 -4.4183 -2.85023 -9.38014 3.63547 -7.05628 8.98519 3.63549 -9.77837 4.76244 6.66646 13.8618 1.98321 5.3266 -5.24981 18.9826 4.96757 10.9306 -5.58977 -17.1901 7.60131 -11.6218 -7.42597 -8.95936 -7.68156 -0.457975 -5.28321 -3.68499 -18.8081 5.21294 -0.846589 9.58988 -7.46311 -6.278 -5.79883 -1.47801 13.9268 4.15199 11.8016 1.229 -7.09913 4.5361 6.44485 -19.3353 5.45726 -15.5018 10.6415 16.2078 -2.79581 25.049 -10.8604 18.2601 -13.6124 -1.53936 10.0385 6.28885 10.4306 -1.27961 14.3956 -5.51691 -13.0456 14.7141 0.415053 -4.60585 4.70583 6.1747 2.02273 10.851 2.41854 13.1397 3.23573 -23.769 3.04503 -3.5225 -10.8145 15.2196 -15.4333 -7.50478 9.39688 -5.91948 -14.8308 -15.7561 7.48063 22.9101 -2.60271 6.58947 -8.89799 5.5754 -15.7239 0.580094 -16.0586 2.16958 -10.3788 -2.66029 -1.97422 4.97372 -6.3757 -2.9772 16.4635 -12.2381 13.1493 -32.7261 6.62787 14.5674 15.5784 9.58833 12.3836 2.39618 10.5699 12.1903 6.62444 -11.0169 -7.07212 -18.0862 10.6454 9.91109 0.561826 -8.0813 -3.92466 6.75809 5.48865 -2.7086 3.54893 1.52392 7.63576 -5.26288 7.21614 12.7913 -0.242614 -11.8295 -13.9975 -3.66513 -12.6808 -8.51004 -5.3356 30.1976 2.93126 -12.4553 23.4495 -5.80684 -0.33229 6.64162 -3.21554 -3.85887 1.09924 7.46454 -2.27829 1.49543 -20.4805 -5.60549 -7.22504 3.47595 15.3468 -3.49121 -25.0662 21.0378 -3.73248 9.81807 -4.23429 -25.5648 23.3276 1.0112 -8.26524 1.71746 18.6142 14.6577 -0.866468 -5.10841 -2.41229 12.1252 -1.92946 -1.14795 10.4803 -9.73934 -16.5714 -9.07791 -2.90332 -2.33988 -3.68204 4.41747 4.52127 -8.35802 20.1286 -5.1253 -10.6119 2.77678 9.41182 -4.8945 -3.01741 -2.07139 22.4524 16.2131 9.58842 -4.73263 -10.9727 -16.482 5.31928 -11.9354 -5.30797 2.73422 -9.27762 -16.7902 -9.02152 6.64376 6.41537 -16.0291 10.5222 -7.20178 -22.1321 -5.55025 -20.1151 16.1008 6.60207 9.18068 -4.14192 9.24078 -2.20243 -18.6966 -6.96757 -4.85715 -15.5228 -2.64255 -3.78848 17.9366 -0.661615 0.280285 -10.0329 11.8295 18.8689 0.529805 -8.51099 1.84596 -14.5028 -14.852 -14.9494 3.95172 -11.5011 -29.4048 -26.9454 1.95334 -10.4363 -24.0042 9.04538 -2.99525 10.3133 1.15173 -13.3186 0.123772 -13.5809 0.0258963 -4.29553 16.306 16.2318 9.28805 11.6168 6.77203 -5.72432 -9.09493 6.56885 0.815302 0.558424 2.40187 10.1191 4.1467 22.2018 -7.68212 5.81884 12.0922 -8.15453 -24.9722 15.3236 -4.37156 -1.43442 -8.98778 2.03559 -15.894 -1.24381 15.7671 10.1753 -5.17801 -7.24286 -18.7058 -15.4356 18.0515 2.24302 4.06533 1.35758 3.80793 2.14719 -0.624644 23.3333 4.11496 8.15446 10.0234 -2.70598 -14.4134 -2.94535 0.37692 5.6674 -4.77695 -10.768 13.7143 10.3384 11.1297 -1.04131 -1.02637 -0.482604 18.1372 -18.8333 -3.56622 -8.41695 9.74002 9.40893 -18.9208 -6.02992 -7.3437 -6.23242 -16.2929 -9.86607 19.7742 -12.3831 8.845 10.2129 12.5193 -3.64406 0.39173 -17.1056 1.42655 -3.32192 -13.4293 6.64151 -14.5653 10.1481 -9.18158 4.33577 5.66889 11.5481 12.8692 19.9995 3.5916 -1.52814 -16.5208 6.74016 -2.98605 6.07932 -7.90435 5.18859 15.6831 -0.3273 3.1198 -3.65359 -9.44084 -2.233 5.49036 -6.12005 -0.301632 -2.23638 -5.89369 -8.51105 -0.753823 1.1811 2.05236 -2.22753 4.14848 5.4189 3.42023 -7.39198 -14.412 12.1508 3.3601 1.42625 -11.5616 -0.500595 19.2485 -5.3356 -14.4425 -3.22287 -5.48961 20.2522 12.359 -4.62178 8.41416 -3.94011 -4.38093 -4.06703 5.42861 -1.91523 -0.909161 11.5446 2.91248 5.07598 -3.90245 -0.397585 -2.09456 -14.032 -10.0492 0.683789 4.37787 7.44828 -3.95382 -3.729 -13.3098 -21.6242 -18.456 -1.92741 -0.69824 -22.3459 -0.887445 -0.0113657 26.6139 17.7094 -2.19315 0.00131248 13.2733 1.98488 10.9469 10.8998 0.207552 -4.37997 -5.8779 -8.60135 6.36556 -18.7309 -2.76195 -1.05574 4.2079 2.88815 -1.90575 12.2432 -13.6672 -5.95639 -15.4531 6.34994 7.72521 19.6564 -4.74903 -1.21759 5.98537 -0.741431 -6.2606 -1.58144 6.38771 16.3309 -4.82653 -24.3406 -5.86519 2.31782 5.31393 -6.79195 -7.19596 -7.292 19.223 -14.4908 -1.088 0.63408 8.17919 0.131037 10.3597 -4.03459 -1.43034 9.30615 -17.2418 -12.3648 8.01003 12.101 5.56332 -10.2734 -1.1703 -6.1239 2.67761 11.5474 8.61113 6.21213 -8.5188 30.0494 16.2187 -14.6422 25.8802 -8.37466 18.2117 -3.757 1.73087 0.571746 -1.88491 20.8549 -2.67961 -6.41379 13.1407 4.96169 18.3228 27.3348 1.02011 -8.41846 7.62927 -13.9642 -14.2713 -6.31213 5.27756 -10.6104 -2.33101 2.10042 -14.0954 2.63182 0.0505723 12.0802 -7.93284 7.62318 -4.36529 -16.2396 0.665287 -5.94725 -4.70193 -14.8225 -18.7065 -12.3163 3.54274 -3.851 -0.449016 11.0777 -16.4143 -5.69115 -4.63633 -9.37737 -17.6801 -13.1725 -15.1596 10.3256 16.1846 8.57198 21.9412 9.94003 16.7924 27.2547 -7.35364 -6.79745 17.6683 -1.47444
[05/26/2023-16:04:48] [I] 
[05/26/2023-16:04:48] [I] === Profile (2147 iterations ) ===
[05/26/2023-16:04:48] [I]                                                                                                                           Layer   Time (ms)   Avg. Time (ms)   Median Time (ms)   Time %
[05/26/2023-16:04:48] [I]                                                                                                        /conv1/Conv + /relu/Relu       59.71           0.0278             0.0276      1.9
[05/26/2023-16:04:48] [I]                                                                                                                /maxpool/MaxPool       12.67           0.0059             0.0061      0.4
[05/26/2023-16:04:48] [I]                                                                        /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu       22.54           0.0105             0.0102      0.7
[05/26/2023-16:04:48] [I]                                                                      /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/relu_1/Relu       36.55           0.0170             0.0174      1.2
[05/26/2023-16:04:48] [I]                                                                                   /layer1/layer1.0/downsample/downsample.0/Conv       33.81           0.0157             0.0154      1.1
[05/26/2023-16:04:48] [I]                                               /layer1/layer1.0/conv3/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_2/Relu       36.77           0.0171             0.0174      1.2
[05/26/2023-16:04:48] [I]                                                                        /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu       38.00           0.0177             0.0174      1.2
[05/26/2023-16:04:48] [I]                                                                      /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/relu_1/Relu       36.32           0.0169             0.0171      1.2
[05/26/2023-16:04:48] [I]                                               /layer1/layer1.1/conv3/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/relu_2/Relu       37.05           0.0173             0.0174      1.2
[05/26/2023-16:04:48] [I]                                                                        /layer1/layer1.2/conv1/Conv + /layer1/layer1.2/relu/Relu       37.84           0.0176             0.0174      1.2
[05/26/2023-16:04:48] [I]                                                                      /layer1/layer1.2/conv2/Conv + /layer1/layer1.2/relu_1/Relu       36.37           0.0169             0.0172      1.2
[05/26/2023-16:04:48] [I]                                               /layer1/layer1.2/conv3/Conv + /layer1/layer1.2/Add + /layer1/layer1.2/relu_2/Relu       36.81           0.0171             0.0174      1.2
[05/26/2023-16:04:48] [I]                                                                        /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu       49.59           0.0231             0.0235      1.6
[05/26/2023-16:04:48] [I]                                                                      /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/relu_1/Relu       84.47           0.0393             0.0389      2.7
[05/26/2023-16:04:48] [I]                                                                                   /layer2/layer2.0/downsample/downsample.0/Conv       55.49           0.0258             0.0256      1.8
[05/26/2023-16:04:48] [I]                                               /layer2/layer2.0/conv3/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_2/Relu       37.42           0.0174             0.0174      1.2
[05/26/2023-16:04:48] [I]                                                                        /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu       47.81           0.0223             0.0225      1.5
[05/26/2023-16:04:48] [I]                                                                      /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/relu_1/Relu       47.89           0.0223             0.0225      1.5
[05/26/2023-16:04:48] [I]                                               /layer2/layer2.1/conv3/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/relu_2/Relu       35.05           0.0163             0.0164      1.1
[05/26/2023-16:04:48] [I]                                                                        /layer2/layer2.2/conv1/Conv + /layer2/layer2.2/relu/Relu       47.38           0.0221             0.0224      1.5
[05/26/2023-16:04:48] [I]                                                                      /layer2/layer2.2/conv2/Conv + /layer2/layer2.2/relu_1/Relu       47.77           0.0223             0.0225      1.5
[05/26/2023-16:04:48] [I]                                               /layer2/layer2.2/conv3/Conv + /layer2/layer2.2/Add + /layer2/layer2.2/relu_2/Relu       35.09           0.0163             0.0164      1.1
[05/26/2023-16:04:48] [I]                                                                        /layer2/layer2.3/conv1/Conv + /layer2/layer2.3/relu/Relu       47.57           0.0222             0.0225      1.5
[05/26/2023-16:04:48] [I]                                                                      /layer2/layer2.3/conv2/Conv + /layer2/layer2.3/relu_1/Relu       47.70           0.0222             0.0225      1.5
[05/26/2023-16:04:48] [I]                                               /layer2/layer2.3/conv3/Conv + /layer2/layer2.3/Add + /layer2/layer2.3/relu_2/Relu       35.14           0.0164             0.0164      1.1
[05/26/2023-16:04:48] [I]                                                                        /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu       44.68           0.0208             0.0205      1.4
[05/26/2023-16:04:48] [I]                          Reformatting CopyNode for Input Tensor 0 to /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu       19.27           0.0090             0.0092      0.6
[05/26/2023-16:04:48] [I]                                                                      /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/relu_1/Relu      116.44           0.0542             0.0543      3.8
[05/26/2023-16:04:48] [I]                                                                                   /layer3/layer3.0/downsample/downsample.0/Conv       63.50           0.0296             0.0297      2.0
[05/26/2023-16:04:48] [I]   Reformatting CopyNode for Input Tensor 0 to /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu        9.87           0.0046             0.0044      0.3
[05/26/2023-16:04:48] [I]                                               /layer3/layer3.0/conv3/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_2/Relu       37.82           0.0176             0.0174      1.2
[05/26/2023-16:04:48] [I]                                                                        /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu       50.62           0.0236             0.0236      1.6
[05/26/2023-16:04:48] [I]                                                                      /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/relu_1/Relu       76.90           0.0358             0.0358      2.5
[05/26/2023-16:04:48] [I]                                               /layer3/layer3.1/conv3/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/relu_2/Relu       38.40           0.0179             0.0176      1.2
[05/26/2023-16:04:48] [I]                                                                        /layer3/layer3.2/conv1/Conv + /layer3/layer3.2/relu/Relu       50.29           0.0234             0.0236      1.6
[05/26/2023-16:04:48] [I]                                                                      /layer3/layer3.2/conv2/Conv + /layer3/layer3.2/relu_1/Relu       76.46           0.0356             0.0358      2.5
[05/26/2023-16:04:48] [I]                                               /layer3/layer3.2/conv3/Conv + /layer3/layer3.2/Add + /layer3/layer3.2/relu_2/Relu       37.87           0.0176             0.0174      1.2
[05/26/2023-16:04:48] [I]                                                                        /layer3/layer3.3/conv1/Conv + /layer3/layer3.3/relu/Relu       50.33           0.0234             0.0236      1.6
[05/26/2023-16:04:48] [I]                                                                      /layer3/layer3.3/conv2/Conv + /layer3/layer3.3/relu_1/Relu       76.52           0.0356             0.0358      2.5
[05/26/2023-16:04:48] [I]                                               /layer3/layer3.3/conv3/Conv + /layer3/layer3.3/Add + /layer3/layer3.3/relu_2/Relu       38.38           0.0179             0.0175      1.2
[05/26/2023-16:04:48] [I]                                                                        /layer3/layer3.4/conv1/Conv + /layer3/layer3.4/relu/Relu       50.24           0.0234             0.0236      1.6
[05/26/2023-16:04:48] [I]                                                                      /layer3/layer3.4/conv2/Conv + /layer3/layer3.4/relu_1/Relu       76.50           0.0356             0.0358      2.5
[05/26/2023-16:04:48] [I]                                               /layer3/layer3.4/conv3/Conv + /layer3/layer3.4/Add + /layer3/layer3.4/relu_2/Relu       37.99           0.0177             0.0174      1.2
[05/26/2023-16:04:48] [I]                                                                        /layer3/layer3.5/conv1/Conv + /layer3/layer3.5/relu/Relu       50.37           0.0235             0.0236      1.6
[05/26/2023-16:04:48] [I]                                                                      /layer3/layer3.5/conv2/Conv + /layer3/layer3.5/relu_1/Relu       76.51           0.0356             0.0358      2.5
[05/26/2023-16:04:48] [I]                                               /layer3/layer3.5/conv3/Conv + /layer3/layer3.5/Add + /layer3/layer3.5/relu_2/Relu       38.23           0.0178             0.0174      1.2
[05/26/2023-16:04:48] [I]                                                                        /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu       52.45           0.0244             0.0246      1.7
[05/26/2023-16:04:48] [I]                          Reformatting CopyNode for Input Tensor 0 to /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu       11.55           0.0054             0.0051      0.4
[05/26/2023-16:04:48] [I]                                                                      /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/relu_1/Relu      226.64           0.1056             0.1055      7.3
[05/26/2023-16:04:48] [I]                                       Reformatting CopyNode for Input Tensor 0 to /layer4/layer4.0/downsample/downsample.0/Conv       15.34           0.0071             0.0072      0.5
[05/26/2023-16:04:48] [I]                                                                                   /layer4/layer4.0/downsample/downsample.0/Conv       76.41           0.0356             0.0358      2.5
[05/26/2023-16:04:48] [I]                                               /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu       44.13           0.0206             0.0205      1.4
[05/26/2023-16:04:48] [I]  Reformatting CopyNode for Output Tensor 0 to /layer4/layer4.0/conv3/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_2/Relu       10.23           0.0048             0.0051      0.3
[05/26/2023-16:04:48] [I]                                                                        /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu       47.74           0.0222             0.0225      1.5
[05/26/2023-16:04:48] [I]                                                                      /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/relu_1/Relu      131.76           0.0614             0.0614      4.2
[05/26/2023-16:04:48] [I]                                               /layer4/layer4.1/conv3/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/relu_2/Relu       50.12           0.0233             0.0236      1.6
[05/26/2023-16:04:48] [I]                                                                        /layer4/layer4.2/conv1/Conv + /layer4/layer4.2/relu/Relu       47.27           0.0220             0.0221      1.5
[05/26/2023-16:04:48] [I]                                                                      /layer4/layer4.2/conv2/Conv + /layer4/layer4.2/relu_1/Relu      131.81           0.0614             0.0614      4.3
[05/26/2023-16:04:48] [I]                                               /layer4/layer4.2/conv3/Conv + /layer4/layer4.2/Add + /layer4/layer4.2/relu_2/Relu       49.15           0.0229             0.0225      1.6
[05/26/2023-16:04:48] [I]                                                                                                      /avgpool/GlobalAveragePool       11.21           0.0052             0.0051      0.4
[05/26/2023-16:04:48] [I]                                                                                                                        /fc/Gemm       35.36           0.0165             0.0164      1.1
[05/26/2023-16:04:48] [I]                                                                                                                           Total     3101.18           1.4444             1.4449    100.0
[05/26/2023-16:04:48] [I] 
&&&& PASSED TensorRT.trtexec [TensorRT v8501] # trtexec --onnx=resnet50.onnx --memPoolSize=workspace:2048 --saveEngine=resnet50.engine --verbose --profilingVerbosity=detailed --dumpOutput --dumpProfile --dumpLayerInfo --exportOutput=log/resnet50/build/build_output.log --exportProfile=log/resnet50/build/build_profile.log --exportLayerInfo=log/resnet50/build/build_layer_info.log --warmUp=200 --iterations=50
